<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/2/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/page/2/"/>





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/03/07/一次海光物理机资源竞争压测的记录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/07/一次海光物理机资源竞争压测的记录/" itemprop="url">一次海光物理机资源竞争压测的记录</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-07T17:30:03+08:00">
                2021-03-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一次海光物理机资源竞争压测的记录"><a href="#一次海光物理机资源竞争压测的记录" class="headerlink" title="一次海光物理机资源竞争压测的记录"></a>一次海光物理机资源竞争压测的记录</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>问题描述如下</p>
<blockquote>
<p>sysbench 压200个服务节点(每个4c16G，总共800core), 发现qps不能线性增加（200节点比100节点好1.2倍而已)。</p>
<p>如果压单个服务节点节点QPS 2.4万，CPU跑到390%（每个服务节点独占4个核），如果压200个服务节点（分布在16台64核的海光物理机上）平均每个服务节点节点QPS才1.2万。但是每个服务节点的CPU也跑到了390%左右。 现在的疑问就是为什么CPU跑上去了QPS打了个5折。</p>
<p>机器集群为16*64core 为1024core，也就是每个服务节点独占4core还有冗余</p>
</blockquote>
<p>因为服务节点还需要通过LVS调用后端的多个MySQL集群，所以需要排除LVS、网络等链路瓶颈，然后找到根因是什么。</p>
<h3 id="海光物理机CPU相关信息"><a href="#海光物理机CPU相关信息" class="headerlink" title="海光物理机CPU相关信息"></a>海光物理机CPU相关信息</h3><p>总共有16台如下的海光服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">#lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                64</div><div class="line">On-line CPU(s) list:   0-63</div><div class="line">Thread(s) per core:    2      //每个物理core有两个超线程</div><div class="line">Core(s) per socket:    16     //每路16个物理core</div><div class="line">Socket(s):             2      //2路</div><div class="line">NUMA node(s):          4</div><div class="line">Vendor ID:             HygonGenuine</div><div class="line">CPU family:            24</div><div class="line">Model:                 1</div><div class="line">Model name:            Hygon C86 5280 16-core Processor</div><div class="line">Stepping:              1</div><div class="line">CPU MHz:               2455.552</div><div class="line">CPU max MHz:           2500.0000</div><div class="line">CPU min MHz:           1600.0000</div><div class="line">BogoMIPS:              4999.26</div><div class="line">Virtualization:        AMD-V</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             64K</div><div class="line">L2 cache:              512K</div><div class="line">L3 cache:              8192K</div><div class="line">NUMA node0 CPU(s):     0-7,32-39</div><div class="line">NUMA node1 CPU(s):     8-15,40-47</div><div class="line">NUMA node2 CPU(s):     16-23,48-55</div><div class="line">NUMA node3 CPU(s):     24-31,56-63</div><div class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd sev ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 MySQLeed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca</div><div class="line"></div><div class="line">#numactl -H</div><div class="line">available: 4 nodes (0-3)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 32 33 34 35 36 37 38 39</div><div class="line">node 0 size: 128854 MB</div><div class="line">node 0 free: 89350 MB</div><div class="line">node 1 cpus: 8 9 10 11 12 13 14 15 40 41 42 43 44 45 46 47</div><div class="line">node 1 size: 129019 MB</div><div class="line">node 1 free: 89326 MB</div><div class="line">node 2 cpus: 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55</div><div class="line">node 2 size: 128965 MB</div><div class="line">node 2 free: 86542 MB</div><div class="line">node 3 cpus: 24 25 26 27 28 29 30 31 56 57 58 59 60 61 62 63</div><div class="line">node 3 size: 129020 MB</div><div class="line">node 3 free: 98227 MB</div><div class="line">node distances:</div><div class="line">node   0   1   2   3</div><div class="line">  0:  10  16  28  22</div><div class="line">  1:  16  10  22  28</div><div class="line">  2:  28  22  10  16</div><div class="line">  3:  22  28  16  10</div></pre></td></tr></table></figure>
<p>AMD Zen 架构的CPU是胶水核，也就是把两个die拼一块封装成一块CPU，所以一块CPU内跨die之间延迟还是很高的。</p>
<h2 id="验证是否是上下游的瓶颈"><a href="#验证是否是上下游的瓶颈" class="headerlink" title="验证是否是上下游的瓶颈"></a>验证是否是上下游的瓶颈</h2><p>需要先分析问题是否在LVS调用后端的多个MySQL集群上。</p>
<p>先写一个简单的测试程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">#cat Test.java</div><div class="line">import java.sql.Connection;</div><div class="line">import java.sql.DriverManager;</div><div class="line">import java.sql.ResultSet;</div><div class="line">import java.sql.SQLException;</div><div class="line">import java.sql.Statement;</div><div class="line">/*</div><div class="line"> * 目录：/home/admin/jdbc</div><div class="line"> *</div><div class="line"> * 编译：</div><div class="line"> *  javac -cp /home/admin/lib/*:. Test.java</div><div class="line"> *</div><div class="line"> *  运行：</div><div class="line"> *   java -cp /home/admin/MySQL-server/lib/*:. Test &quot;jdbc:mysql://172.16.160.1:4261/qc_pay_0xwd_0002&quot; &quot;myhhzi0d&quot; &quot;jOXaC1Lbif-k&quot; &quot;select count(*) from pay_order where user_id=1169257092557639682 and order_no=&apos;201909292111250000102&apos;&quot; &quot;100&quot;</div><div class="line"> *   */</div><div class="line">public class Test &#123;</div><div class="line"></div><div class="line">    public static void main(String args[]) throws NumberFormatException, InterruptedException, ClassNotFoundException &#123;</div><div class="line">        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);</div><div class="line">        String url = args[0];</div><div class="line">        String user = args[1];</div><div class="line">        String pass = args[2];</div><div class="line">        String sql = args[3];</div><div class="line">        String interval = args[4];</div><div class="line">        try &#123;</div><div class="line">            Connection conn = DriverManager.getConnection(url, user, pass);</div><div class="line">            while (true) &#123;</div><div class="line">                long start = System.currentTimeMillis();</div><div class="line">                for(int i=0; i&lt;1000; ++i)&#123;</div><div class="line">                    Statement stmt = conn.createStatement();</div><div class="line">                    ResultSet rs = stmt.executeQuery(sql);</div><div class="line">                    while (rs.next()) &#123;</div><div class="line">                    &#125;</div><div class="line">                    rs.close();</div><div class="line">                    stmt.close();</div><div class="line">                    Thread.sleep(Long.valueOf(interval));</div><div class="line">                &#125;</div><div class="line">                long end = System.currentTimeMillis();</div><div class="line">                System.out.println(&quot;rt : &quot; + (end - start));</div><div class="line">            &#125;</div><div class="line">        &#125; catch (SQLException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后通过传入不同的jdbc参数跑2组测试:</p>
<ol>
<li>走服务节点执行指定id的点查； </li>
<li>直接从服务节点节点连MySQL指定id点查  </li>
</ol>
<p>上述2组测试同时跑在三组场景下：</p>
<ul>
<li>A) 服务节点和MySQL都没有压力； </li>
<li>B) 跑1、2测试的服务节点没有压力，但是sysbench 在压别的服务节点，这样后端的MySQL是有sysbench压侧压力，LVS也有流量压力的； </li>
<li>C) sysbench压所有服务节点, 包含运行 1、2测试程序节点） </li>
</ul>
<p>这样2组测试3个场景组合可以得到6组响应时间的测试数据</p>
<p>从最终得到6组数据来看可以排除链路以及MySQL的问题，瓶颈似乎还是在服务节点上</p>
<p><img src="/images/oss/595bc15fdd72860f2d1875c86384a14b.png" alt="image.png"></p>
<p>单独压一个服务节点节点并在上面跑测试，服务节点 CPU被压到 390%(每个服务节点 节点固定绑到4核), 这个时候整个宿主机压力不大，但是这四个核比较紧张了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#cat rt.log  | awk &apos;&#123; print $3 &#125;&apos;  | awk &apos;&#123;if(min==&quot;&quot;)&#123;min=max=$1&#125;; if($1&gt;max) &#123;max=$1&#125;; if($1&lt;min) &#123;min=$1&#125;; total+=$1; count+=1&#125; END &#123;print &quot;avg &quot; total/count,&quot; | max &quot;max,&quot; | min &quot; min, &quot;| count &quot;, count&#125;&apos; ; cat MySQL.log  | awk &apos;&#123; print $3 &#125;&apos;  | awk &apos;&#123;if(min==&quot;&quot;)&#123;min=max=$1&#125;; if($1&gt;max) &#123;max=$1&#125;; if($1&lt;min) &#123;min=$1&#125;; total+=$1; count+=1&#125; END &#123;print &quot;avg &quot; total/count,&quot; | max &quot;max,&quot; | min &quot; min, &quot;| count &quot;, count &#125;&apos;;</div><div class="line">avg 2589.13  | max 3385  | min 2502 | count  69</div><div class="line">avg 1271.07  | max 1405  | min 1254 | count  141</div><div class="line"></div><div class="line">[root@d42a01107.cloud.a02.am78 /root]</div><div class="line">#taskset -pc 48759</div><div class="line">pid 48759&apos;s current affinity list: 19,52-54</div></pre></td></tr></table></figure>
<p>通过这6组测试数据可以看到，只有在整个系统都有压力（服务节点所在物理机、LVS、MySQL）的时候rt飙升最明显（C组数据），如果只是LVS、MySQL有压力，服务节点没有压力的时候可以看到数据还是很好的（B组数据）</p>
<h2 id="分析宿主机资源竞争"><a href="#分析宿主机资源竞争" class="headerlink" title="分析宿主机资源竞争"></a>分析宿主机资源竞争</h2><h3 id="perf分析"><a href="#perf分析" class="headerlink" title="perf分析"></a>perf分析</h3><p>只压单个服务节点</p>
<p><img src="/images/oss/7aea9045e50794fadc0439ee806f2496.png" alt="image.png"></p>
<p><img src="/images/oss/86c3f14887345a1d5f08cae985816564.png" alt="image.png"></p>
<p><strong>从以上截图，可以看到关键的 insn per cycle 能到0.51和0.66（这个数值越大性能越好）</strong></p>
<p>如果同时压物理机上的所有服务节点</p>
<p><img src="/images/oss/02f47474c612c2bf6e612efea3ab5de3.png" alt="image.png"></p>
<p><img src="/images/oss/c3d90e077d00a7a3db54770d9eea2dbb.png" alt="image.png"></p>
<p><strong>从以上截图，可以看到关键的 insn per cycle 能降到了0.27和0.31（这个数值越大性能越好），基本相当于单压的5折</strong></p>
<p>通过 perf list 找出所有Hardware event，然后对他们进行perf:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo perf stat -e branch-instructions,branch-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-prefetches,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads -a -- `pidof java`</div></pre></td></tr></table></figure>
<h3 id="尝试不同的绑核后的一些数据"><a href="#尝试不同的绑核后的一些数据" class="headerlink" title="尝试不同的绑核后的一些数据"></a>尝试不同的绑核后的一些数据</h3><p>通过以上perf数据以及numa结构，尝试将不同服务进程绑定到指定的4个核上</p>
<p>试了以下三种绑核的办法：</p>
<p>1）docker swarm随机绑（<strong>以上测试都是用的这种默认方案</strong>）；</p>
<p>2）一个服务节点绑连续4个core,这4个core都在同一个node； </p>
<p>3）一个服务节点绑4个core，这个4个core都在在同一个node，同时尽量HT在一起，也就是0，1，32，33 ； 2，3，34，35 这种绑法 </p>
<p><strong>结果是绑法2性能略好</strong>. </p>
<p>如果是绑法2，压单个服务节点 QPS能到2.3万；绑法1和3，压单个服务节点性能差别不明显，都是2万左右。 </p>
<h2 id="尝试将Java进程开启HugePage"><a href="#尝试将Java进程开启HugePage" class="headerlink" title="尝试将Java进程开启HugePage"></a>尝试将Java进程开启HugePage</h2><p>从perf数据来看压满后tlab miss比较高，得想办法降低这个值</p>
<h3 id="修改JVM启动参数"><a href="#修改JVM启动参数" class="headerlink" title="修改JVM启动参数"></a>修改JVM启动参数</h3><p>JVM启动参数增加如下三个(-XX:LargePageSizeInBytes=2m, 这个一定要，有些资料没提这个，在我的JDK8.0环境必须要)：</p>
<blockquote>
<p>-XX:+UseLargePages -XX:LargePageSizeInBytes=2m -XX:+UseHugeTLBFS</p>
</blockquote>
<h3 id="修改机器系统配置"><a href="#修改机器系统配置" class="headerlink" title="修改机器系统配置"></a>修改机器系统配置</h3><p>设置HugePage的大小</p>
<blockquote>
<p>cat /proc/sys/vm/nr_hugepages</p>
</blockquote>
<p>nr_hugepages设置多大参考如下计算方法：</p>
<blockquote>
<p>If you are using the option <code>-XX:+UseSHM</code> or <code>-XX:+UseHugeTLBFS</code>, then specify the number of large pages. In the following example, 3 GB of a 4 GB system are reserved for large pages (assuming a large page size of 2048kB, then 3 GB = 3 <em> 1024 MB = 3072 MB = 3072 </em> 1024 kB = 3145728 kB and 3145728 kB / 2048 kB = 1536):</p>
<p>echo 1536 &gt; /proc/sys/vm/nr_hugepages </p>
</blockquote>
<p>透明大页是没有办法减少系统tlab，tlab是对应于进程的，系统分给进程的透明大页还是由物理上的4K page组成。</p>
<p>Java进程用上HugePages后iTLB-load-misses从80%下降到了14%左右, dTLB也从30%下降到了20%，但是ipc变化不明显，QPS有不到10%的增加(不能确定是不是抖动所致)</p>
<p><img src="/images/oss/f6882f4c671b4c4b46feb01aa5e272fd.png" alt="image.png"></p>
<p>在公有云ecs虚拟机上测试对性能没啥帮助，实际看到用掉的HuagPage不多，如果/proc/sys/vm/nr_hugepages 设置比较大的话JVM会因为内存不足起不来，两者内存似乎是互斥的</p>
<h2 id="用sysbench验证一下海光服务器的多core能力"><a href="#用sysbench验证一下海光服务器的多core能力" class="headerlink" title="用sysbench验证一下海光服务器的多core能力"></a>用sysbench验证一下海光服务器的多core能力</h2><p>Intel E5 2682 2.5G VS hygon 7280 2.0G（Zen1）</p>
<p><img src="/images/951413iMgBlog/image-20210813095409553.png" alt="image-20210813095409553"></p>
<p><img src="/images/951413iMgBlog/image-20210813095757299.png" alt="image-20210813095757299"></p>
<p>由以上两个测试结果可以看出单核能力hygon 7280 强于 Intel 2682，但是hygon超线程能力还是没有任何提升。Intel用超线程计算能将耗时从109秒降到74秒。但是hygon(Zen1) 只是从89秒降到了87秒，基本没有变化。</p>
<p>再补充一个Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz 对比数据 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">#taskset -c 1,53 /usr/bin/sysbench --num-threads=2 --test=cpu --cpu-max-prime=50000 run</div><div class="line">sysbench 0.5:  multi-threaded system evaluation benchmark</div><div class="line"></div><div class="line">Running the test with following options:</div><div class="line">Number of threads: 2</div><div class="line">Random number generator seed is 0 and will be ignored</div><div class="line"></div><div class="line"></div><div class="line">Primer numbers limit: 50000</div><div class="line"></div><div class="line">Threads started!</div><div class="line"></div><div class="line"></div><div class="line">General statistics:</div><div class="line">    total time:                          48.5571s</div><div class="line">    total number of events:              10000</div><div class="line">    total time taken by event execution: 97.0944s</div><div class="line">    response time:</div><div class="line">         min:                                  8.29ms</div><div class="line">         avg:                                  9.71ms</div><div class="line">         max:                                 20.88ms</div><div class="line">         approx.  95 percentile:               9.71ms</div><div class="line"></div><div class="line">Threads fairness:</div><div class="line">    events (avg/stddev):           5000.0000/2.00</div><div class="line">    execution time (avg/stddev):   48.5472/0.01</div><div class="line"></div><div class="line">#taskset -c 1 /usr/bin/sysbench --num-threads=1 --test=cpu --cpu-max-prime=50000 run</div><div class="line">sysbench 0.5:  multi-threaded system evaluation benchmark</div><div class="line"></div><div class="line">Running the test with following options:</div><div class="line">Number of threads: 1</div><div class="line">Random number generator seed is 0 and will be ignored</div><div class="line"></div><div class="line"></div><div class="line">Primer numbers limit: 50000</div><div class="line"></div><div class="line">Threads started!</div><div class="line"></div><div class="line"></div><div class="line">General statistics:</div><div class="line">    total time:                          83.2642s</div><div class="line">    total number of events:              10000</div><div class="line">    total time taken by event execution: 83.2625s</div><div class="line">    response time:</div><div class="line">         min:                                  8.27ms</div><div class="line">         avg:                                  8.33ms</div><div class="line">         max:                                 10.03ms</div><div class="line">         approx.  95 percentile:               8.36ms</div><div class="line"></div><div class="line">Threads fairness:</div><div class="line">    events (avg/stddev):           10000.0000/0.00</div><div class="line">    execution time (avg/stddev):   83.2625/0.00</div><div class="line"></div><div class="line">#lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                104</div><div class="line">On-line CPU(s) list:   0-103</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    26</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 85</div><div class="line">Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</div><div class="line">Stepping:              7</div><div class="line">CPU MHz:               3200.097</div><div class="line">CPU max MHz:           3800.0000</div><div class="line">CPU min MHz:           1200.0000</div><div class="line">BogoMIPS:              4998.89</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              1024K</div><div class="line">L3 cache:              36608K</div><div class="line">NUMA node0 CPU(s):     0-25,52-77</div><div class="line">NUMA node1 CPU(s):     26-51,78-103</div><div class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb invpcid_single pln pts dtherm spec_ctrl ibpb_support tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt avx512f avx512dq rdseed adx smap clflushopt avx512cdavx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local cat_l3 mba</div></pre></td></tr></table></figure>
<p>用sysbench 测试Hygon C86 5280 16-core Processor，分别1、8、16、24、32、40、48、56、64 个thread，32个thread前都是完美的线性增加，32core之后基本不增长了，这个应该能说明这个服务器就是32core的能力</p>
<blockquote>
<p>sysbench –threads=1 –cpu-max-prime=50000 cpu run</p>
</blockquote>
<p><img src="/images/oss/f86cd786f3a8297078579b547f78ec81.png" alt="image.png"></p>
<p>对比下intel的 Xeon 104core，也是物理52core，但是性能呈现完美线性</p>
<p><img src="/images/oss/8086f47b955d8d951e4dd4c7fe5e135e.png" alt="image.png"></p>
<h3 id="openssl场景多核能力验证"><a href="#openssl场景多核能力验证" class="headerlink" title="openssl场景多核能力验证"></a>openssl场景多核能力验证</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">openssl speed aes-256-ige -multi N</div></pre></td></tr></table></figure>
<p>intel 52 VS 26，可以看到52个线程的性能大概是26个的1.8倍</p>
<p><img src="/images/oss/4534d8e5901cc812aa54e610d1386445.png" alt="image.png"></p>
<p>intel 104 VS 52 线程，性能还能提升1.4倍</p>
<p><img src="/images/oss/6583f52e03b9753969e52d6a8b211725.png" alt="image.png"></p>
<p>海光32 VS 16, 性能能提升大概1.8倍，跟intel一致</p>
<p><img src="/images/oss/41e7f230ec27653c1b5ae5287971cd3a.png" alt="image.png"></p>
<p>海光64 VS 32, 性能能提升大概1.2倍</p>
<p><img src="/images/oss/2ad45a252392a06fa64d7475848e5601.png" alt="image.png"></p>
<p>总结下就是，在物理core数以内的线程数intel和海光性能基本增加一致；但如果超过物理core数开始使用HT后海光明显相比Intel差了很多。</p>
<p>intel超线程在openssl场景下性能能提升40%，海光就只能提升20%了。</p>
<h3 id="对比一下鲲鹏920-ARM架构的芯片"><a href="#对比一下鲲鹏920-ARM架构的芯片" class="headerlink" title="对比一下鲲鹏920 ARM架构的芯片"></a>对比一下鲲鹏920 ARM架构的芯片</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">#numactl -H</div><div class="line">available: 1 nodes (0)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</div><div class="line">node 0 size: 773421 MB</div><div class="line">node 0 free: 756092 MB</div><div class="line">node distances:</div><div class="line">node   0</div><div class="line">  0:  10</div></pre></td></tr></table></figure>
<p>96核一起跑openssl基本就是1核的96倍，完美线性，这是因为鲲鹏就没有超线程，都是物理核。如果并发增加到192个，性能和96个基本一样的。</p>
<p><img src="/images/oss/be30ab94eddc37f1d90c53204a0ed215.png" alt="image.png"></p>
<h3 id="用Sysbench直接压MySQL-oltp-read-only的场景"><a href="#用Sysbench直接压MySQL-oltp-read-only的场景" class="headerlink" title="用Sysbench直接压MySQL oltp_read_only的场景"></a>用Sysbench直接压MySQL oltp_read_only的场景</h3><p><img src="/images/oss/89c7a0228c45f79b688710206ba9d414.png" alt="image.png"></p>
<p>从1到10个thread的时候完美呈现线性，到20个thread就只比10个thread增加50%了，30thread比20增加40%，过了32个thread后增加10个core性能加不到10%了。</p>
<p>在32thread前，随着并发的增加 IPC也有所减少，这也是导致thread翻倍性能不能翻倍的一个主要原因。</p>
<p>基本也和openssl 场景一致，海光的HT基本可以忽略，做的不是很好。超过32个thread后（物理core数）性能增加及其微弱</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>海光的一个core只有一个fpu，所以超线程算浮点完全没用.</p>
<blockquote>
<p>FP处理所有矢量操作。简单的整数向量运算（例如shift，add）都可以在一个周期内完成，是AMD以前架构延迟的一半。<strong>基本浮点数学运算具有三个周期的延迟</strong>，其中包括乘法（用于双精度需要一个额外周期）。<strong>融合乘加是五个周期。</strong></p>
<p>FP具有用于128位加载操作的单个管道。实际上，整个FP端都针对128位操作进行了优化。 Zen支持所有最新指令，例如SSE和AVX1/2。 256位AVX的设计方式是可以将它们作为两个独立的128位操作来执行。 Zen通过将这些指令作为两个操作。也就是说，<strong>Zen将256位操作分为两个µOP</strong>。同样，存储也是在128位块上完成的，从而使256位加载的有效吞吐量为每两个周期一个存储。这些管道之间的平衡相当好，因此大多数操作将安排至少两个管道，以保持每个周期至少一个这样的指令的吞吐量。暗示着，<strong>256位操作将占用两倍的资源来完成操作（即2x寄存器，调度程序和端口）。这是AMD采取的一种折衷方案，有助于节省芯片空间和功耗。</strong>相比之下，英特尔的竞争产品Skylake确实具有专用的256位电路。还应注意的是，英特尔的现代服务器级型号进一步扩展了此功能，以纳入支持AVX-512的专用512位电路，而性能最高的型号则具有二个专用的AVX-512单元。</p>
<p>此外，Zen还支持SHA和AES（并实现了2个AES单元），以提高加密性能。这些单位可以在浮点调度程序的管道0和1上找到。</p>
<p>这个也是为什么浮点比Intel X86会弱的原因。</p>
</blockquote>
<h3 id="一些其他对比结论"><a href="#一些其他对比结论" class="headerlink" title="一些其他对比结论"></a>一些其他对比结论</h3><ul>
<li>对纯CPU 运算场景，并发不超过物理core时，比如Prime运算，比如DRDS(CPU bound，IO在网络，可以加并发弥补)<ul>
<li>海光的IPC能保持稳定；</li>
<li>intel的IPC有所下降，但是QPS在IPC下降后还能完美线性</li>
</ul>
</li>
<li>在openssl和MySQL oltp_read_only场景下<ul>
<li>如果并发没超过物理core数时，海光和Intel都能随着并发的翻倍性能能增加80%</li>
<li>如果并发超过物理core数后，Intel还能随着并发的翻倍性能增加50%，海光增加就只有20%了</li>
<li>简单理解在这两个场景下Intel的HT能发挥半个物理core的作用，海光的HT就只能发挥0.2个物理core的作用了</li>
</ul>
</li>
<li>海光5280/7280 是Zen1/Zen2的AMD 架构，每个core只有一个fpu，综上在多个场景下HT基本上都可以忽略</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU的制造和概念/">CPU的制造和概念</a></p>
<p><a href="/2021/05/16/CPU Cache Line 和性能/">CPU 性能和Cache Line</a></p>
<p><a href="/2021/05/16/Perf IPC以及CPU利用率/">Perf IPC以及CPU性能</a></p>
<p><a href="/2021/06/18/几款CPU性能对比/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/05/15/飞腾ARM芯片(FT2500">飞腾ARM芯片(FT2500)的性能测试</a>的性能测试/)</p>
<p><a href="/2021/05/14/十年后数据库还是不敢拥抱NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2021/03/07/一次海光物理机资源竞争压测的记录/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://dino.ciuffetti.info/2011/07/howto-java-huge-pages-linux/" target="_blank" rel="external">How to use Huge Pages with Java and Linux</a>这个资料中提到了Java使用HugePage的时候启动进程的用户权限问题，在我的docker容器中用的admin启动的进程，测试验证是不需要按资料中的设置。</p>
<p><a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></p>
<p><a href="https://bbs.huaweicloud.com/blogs/146367" target="_blank" rel="external">华为TaiShan服务器ARMNginx应用调优案例 大量绑核、中断、Numa等相关调优信息</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/03/05/MacOS下如何使用iTerm2访问水木社区/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/05/MacOS下如何使用iTerm2访问水木社区/" itemprop="url">MacOS下如何使用iTerm2访问水木社区</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-03-05T17:30:03+08:00">
                2021-03-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MacOS下如何使用iTerm2访问水木社区"><a href="#MacOS下如何使用iTerm2访问水木社区" class="headerlink" title="MacOS下如何使用iTerm2访问水木社区"></a>MacOS下如何使用iTerm2访问水木社区</h1><p>关键字： MacOS、iTerm 、Dracula、ssh、bbs.newsmth.net</p>
<p>windows下有各种Term软件来帮助我们通过ssh访问bbs.newsmth.net, 但是工作环境切换到MacOS后发现FTerm、CTerm这样的工具都没有对应的了。但是term下访问 bbs.newsmth.net 简直是太爽了，所以本文希望解决这个问题。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>ssh 访问 bbs.newsmth.net 是没问题的，但是<strong>要解决配色和字符编码问题</strong></p>
<h3 id="解决编码"><a href="#解决编码" class="headerlink" title="解决编码"></a>解决编码</h3><p>在iTerm2的配置中增加一个profile，如下图 smth，主要是改字符编码集为 GB 18030，然后修改配色方案，我喜欢的Dracula不适合SMTH，十大完全看不了。</p>
<p><img src="/images/951413iMgBlog/image-20210602133111201.png" alt="image-20210602133111201"></p>
<p>执行脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#cat /usr/local/bin/smth</div><div class="line">echo -e &quot;\033]50;SetProfile=smth\a&quot;          //切换到smth的profile,也就是切换到了 GB18030</div><div class="line">sshpass -p&apos;密码&apos; ssh -o ServerAliveInterval=60 水木id@bbs.newsmth.net</div><div class="line">echo -e &quot;\033]50;SetProfile=Default\a&quot;       //切换回UTF8</div></pre></td></tr></table></figure>
<p>SetProfile=smth用来解决profile切换，连smth term前切换成GB 18030，断开的时候恢复成UTF-8，要不然的话正常工作的命令行就乱码了。</p>
<h3 id="解决配色问题"><a href="#解决配色问题" class="headerlink" title="解决配色问题"></a>解决配色问题</h3><p>然后还是在profile里面把smth的配色方案改成：Tango Dark, 一切简直是完美，工作灌水两不误，别人还发现不了</p>
<h2 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h2><p>目录（右边是工作窗口）：</p>
<p><img src="/images/oss/0265ed7a728bfdd6be940d838fc1feaf.png" alt="image.png"></p>
<p>十大，这个十大颜色和右边工作模式的配色方案不一样</p>
<p><img src="/images/oss/252b9295375f6e6078278a6e64e1d68c.png" alt="image.png"></p>
<p>断开后恢复成 Dracula 配色和UTF-8编码，不影响工作，别的工作tab也还是正常使用utf8</p>
<p><img src="/images/oss/cf8912c0634182b44fa92eeb9f854362.png" alt="image.png"></p>
<p>别的term网站也是类似，比如小百合、byr、ptt等</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/02/14/TCP疑难问题案例汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/14/TCP疑难问题案例汇总/" itemprop="url">TCP疑难问题案例汇总</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-14T13:30:03+08:00">
                2021-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TCP疑难问题案例汇总"><a href="#TCP疑难问题案例汇总" class="headerlink" title="TCP疑难问题案例汇总"></a>TCP疑难问题案例汇总</h1><p>碰到各种奇葩的TCP相关问题，所以汇总记录一下。分析清楚这些问题的所有来龙去脉，就能帮你在TCP知识体系里建立几个坚固的抓手，让TCP知识慢慢在抓手之间生长和互通</p>
<h2 id="服务不响应的现象或者奇怪异常的原因分析"><a href="#服务不响应的现象或者奇怪异常的原因分析" class="headerlink" title="服务不响应的现象或者奇怪异常的原因分析"></a>服务不响应的现象或者奇怪异常的原因分析</h2><p> <a href="/2021/02/10/%E4%B8%80%E4%B8%AA%E9%BB%91%E7%9B%92%E7%A8%8B%E5%BA%8F%E5%A5%87%E6%80%AA%E7%9A%84%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/">一个黑盒程序奇怪行为的分析</a> listen端口上很快就全连接队列溢出了，导致整个程序不响应了</p>
<p><a href="/2020/11/02/%E4%B8%BE%E4%B8%89%E5%8F%8D%E4%B8%80--%E4%BB%8E%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%88%B0%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%A8%E5%AF%BC/">举三反一–从理论知识到实际问题的推导</a> 服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
<p><a href="/2020/11/18/TCP%E8%BF%9E%E6%8E%A5%E4%B8%BA%E5%95%A5%E4%BA%92%E4%B8%B2%E4%BA%86/">活久见，TCP连接互串了</a>  应用每过一段时间总是会抛出几个连接异常的错误，需要查明原因。排查后发现是TCP连接互串了，这个案例实在是很珍惜，所以记录一下。</p>
<p> <a href="/2020/07/01/如何创建一个自己连自己的TCP连接/">如何创建一个自己连自己的TCP连接</a></p>
<h2 id="传输速度分析"><a href="#传输速度分析" class="headerlink" title="传输速度分析"></a>传输速度分析</h2><p>案例：<a href="/2021/01/15/TCP%E4%BC%A0%E8%BE%93%E9%80%9F%E5%BA%A6%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/">TCP传输速度案例分析</a>（长肥网络、rt升高、delay ack的影响等）</p>
<p>原理：<a href="/2019/09/28/就是要你懂TCP--性能和发送接收Buffer的关系/">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></p>
<p><a href="/2018/06/14/就是要你懂TCP--最经典的TCP性能问题/">就是要你懂TCP–最经典的TCP性能问题 Nagle和Delay ack</a></p>
<p><a href="/2019/06/21/就是要你懂TCP--性能优化大全/">就是要你懂TCP–性能优化大全</a></p>
<h2 id="TCP队列问题以及连接数"><a href="#TCP队列问题以及连接数" class="headerlink" title="TCP队列问题以及连接数"></a>TCP队列问题以及连接数</h2><p> <a href="/2020/11/30/一台机器上最多能创建多少个TCP连接/">到底一台服务器上最多能创建多少个TCP连接</a></p>
<p> <a href="/2019/08/31/就是要你懂TCP队列--通过实战案例来展示问题/">就是要你懂TCP队列–通过实战案例来展示问题</a></p>
<p> <a href="/2017/06/07/就是要你懂TCP--半连接队列和全连接队列/">就是要你懂TCP–半连接队列和全连接队列</a></p>
<p> <a href="/2017/06/02/就是要你懂TCP--连接和握手/">就是要你懂TCP–握手和挥手</a></p>
<h2 id="防火墙和reset定位分析"><a href="#防火墙和reset定位分析" class="headerlink" title="防火墙和reset定位分析"></a>防火墙和reset定位分析</h2><p>对ttl、identification等的运用</p>
<p><a href="/2018/08/26/关于TCP连接的KeepAlive和reset/">关于TCP连接的Keepalive和reset</a></p>
<p><a href="/2019/11/06/谁动了我的TCP连接/">就是要你懂网络–谁动了我的TCP连接</a></p>
<h2 id="TCP相关参数"><a href="#TCP相关参数" class="headerlink" title="TCP相关参数"></a>TCP相关参数</h2><p> <a href="/2020/01/26/TCP相关参数解释/">TCP相关参数解释</a></p>
<p><a href="/2019/05/16/网络通不通是个大问题--半夜鸡叫/">网络通不通是个大问题–半夜鸡叫</a> </p>
<p><a href="/2018/12/26/网络丢包/">网络丢包</a></p>
<h2 id="工具技巧篇"><a href="#工具技巧篇" class="headerlink" title="工具技巧篇"></a>工具技巧篇</h2><p> <a href="/2019/04/21/netstat定位性能案例/">netstat定位性能案例</a></p>
<p> <a href="/2017/08/28/netstat --timer/">netstat timer keepalive explain</a></p>
<p><a href="/2016/10/12/ss用法大全/">就是要你懂网络监控–ss用法大全</a></p>
<p><a href="/2019/06/21/就是要你懂抓包--WireShark之命令行版tshark/">就是要你懂抓包–WireShark之命令行版tshark</a></p>
<p><a href="/2018/01/01/通过tcpdump对Unix Socket 进行抓包解析/">通过tcpdump对Unix Domain Socket 进行抓包解析</a></p>
<p><a href="/2017/12/07/如何追踪网络流量/">如何追踪网络流量</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/02/10/一个黑盒程序奇怪的行为分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/10/一个黑盒程序奇怪的行为分析/" itemprop="url">一个黑盒程序奇怪行为的分析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-02-10T10:30:03+08:00">
                2021-02-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一个黑盒程序奇怪行为的分析"><a href="#一个黑盒程序奇怪行为的分析" class="headerlink" title="一个黑盒程序奇怪行为的分析"></a>一个黑盒程序奇怪行为的分析</h1><h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><blockquote>
<p>从银行金主baba手里拿到一个区块链程序，监听4000，在我们的环境中4000端口上很快就全连接队列溢出了，导致整个程序不响应了。这个程序是黑盒子，没有源代码，但是在金主baba自己的环境运行正常（一样的OS）</p>
</blockquote>
<p>如下图所示：</p>
<p><img src="/images/oss/623ca2f46084958efa447882cbb58e72.png" alt="image.png"></p>
<p>ss -lnt 看到全连接队列增长到了39，但是netstat -ant找不到这39个连接，本来是想看看队列堆了这么多连接，都是哪些ip连过来的，实际看不到这就奇怪了</p>
<p>同时验证过程发现我们的环境4000端口上开了slb，也就是slb会不停滴探活4000端口，关掉slb探活后一切正常了。</p>
<p>所以总结下来问题就是：</p>
<ol>
<li><p>为什么全连接队列里面的连接netstat看不到这些连接，但是ss能看到总数 </p>
</li>
<li><p>为什么关掉slb就正常了 </p>
</li>
<li><p>为什么应用不accept连接,也不close（应用是个黑盒子） </p>
</li>
</ol>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="为什么全连接队列里面的连接netstat-ss都看不到-ss能看到总数"><a href="#为什么全连接队列里面的连接netstat-ss都看不到-ss能看到总数" class="headerlink" title="为什么全连接队列里面的连接netstat/ss都看不到(ss能看到总数)"></a>为什么全连接队列里面的连接netstat/ss都看不到(ss能看到总数)</h3><p>这是因为这些连接都是探活连接，三次握手后很快被slb reset了，在OS层面这个连接已经被释放，所以肯定看不见。反过来想要是netstat能看见这个连接，那么它的状态是什么？ reset吗？tcp连接状态里肯定是没有reset状态的。</p>
<p>ss看到的总数是指只要这个连接没有被accept，那么连接队列里就还有这个连接，通过ss也能看到连接队列数量。</p>
<h4 id="为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？"><a href="#为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？" class="headerlink" title="为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？"></a>为什么会产生这个错误理解–全连接队列里面的连接netstat一定要能看到？</h4><p>那是因为正常情况都是能看到的，从没有考虑过握手后很快reset的情况。也没反问过如果能看到这个连接该是什么状态呢？</p>
<h4 id="这个连接被reset后，kernel会将全连接队列数量减1吗？"><a href="#这个连接被reset后，kernel会将全连接队列数量减1吗？" class="headerlink" title="这个连接被reset后，kernel会将全连接队列数量减1吗？"></a>这个连接被reset后，kernel会将全连接队列数量减1吗？</h4><p>不会，按照我们的理解连接被reset释放后，那么kernel要释放全连接队列里面的这个连接，因为这些动作都是kernel负责，上层没法处理这个reset。实际上内核认为所有 listen 到的连接, 必须要 accept 走, 用户有权利知道存在过这么一个连接。</p>
<p>也就是reset后，连接在内核层面释放了，所以netstat/ss看不到，但是全连接队列里面的应用数不会减1，只有应用accept后队列才会减1，accept这个空连接后读写会报错。基本可以认为全连接队列溢出了，主要是应用accept太慢导致的。</p>
<p>当应用从accept队列读取到已经被reset了的连接的时候应用层会得到一个报错信息。</p>
<h4 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h4><p>三次握手成功就变成 ESTABLISHED 了，三次握手成功的第一是收到第三步的ack并且全连接队列没有满，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>下面是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</div><div class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </div><div class="line">LISTEN      73     1024            *:terabase                 *:*</div></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;  </div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</div><div class="line"># ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </div><div class="line">LISTEN      1025   1024             *:terabase              *:*</div></pre></td></tr></table></figure>
<h3 id="为什么关掉slb就正常了"><a href="#为什么关掉slb就正常了" class="headerlink" title="为什么关掉slb就正常了"></a>为什么关掉slb就正常了</h3><p>slb探活逻辑是向监听端口发起三次握手，握手成功后立即发送一个reset断开连接</p>
<p>这是一个完整的探活过程：</p>
<p><img src="/images/oss/b81dcbaea26a5130383d0bc8317fd3c5.png" alt="image.png"></p>
<p>关掉就正常后要结合第三个问题来讲</p>
<h3 id="为什么应用不accept连接-也不close（应用是个黑盒子）"><a href="#为什么应用不accept连接-也不close（应用是个黑盒子）" class="headerlink" title="为什么应用不accept连接,也不close（应用是个黑盒子）"></a>为什么应用不accept连接,也不close（应用是个黑盒子）</h3><p>因为应用是个黑盒子，看不到源代码，只能从行为来分析了</p>
<p>从行为来看，这个应用在三次握手后，会主动给client发送一个12字节的数据，但是这个逻辑写在了accept主逻辑内部，一旦主动给client发12字节数据失败（比如这个连接reset了）那么一直卡在这里导致应用不再accept也不再close。</p>
<p>正确的实现逻辑是，accept在一个单独的线程里，一旦accept到一个新连接，那么就开启一个新的线程来处理这个新连接的读写。accept线程专注accept。</p>
<p>关掉slb后应用有机会发出这12个字节，然后accept就能继续了，否则就卡死了。</p>
<h2 id="一些验证"><a href="#一些验证" class="headerlink" title="一些验证"></a>一些验证</h2><h3 id="nc测试连接4000端口"><a href="#nc测试连接4000端口" class="headerlink" title="nc测试连接4000端口"></a>nc测试连接4000端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># nc -p 12346 dcep-blockchain-1 4000</div><div class="line"> //握手后4000返回的内容</div><div class="line"></div><div class="line">抓包：</div><div class="line">11:03:16.762547 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [S], seq 397659761, win 43690, options [mss 65495,sackOK,TS val 2329725964 ecr 0,nop,wscale 7], length 0</div><div class="line">04:42:24.466211 IP dcep-blockchain-1.terabase &gt; dcep-blockchain-1.12346: Flags [S.], seq 4239354556, ack 397659762, win 43690, options [mss 65495,sackOK,TS val 2329725964 ecr 2329725964,nop,wscale 7], length 0</div><div class="line">11:03:16.762571 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [.], ack 1, win 342, options [nop,nop,TS val 2329725964 ecr 2329725964], length 0</div><div class="line"></div><div class="line">----到这三次握手完毕，下面是隔了大概1.5ms，4000发了12字节给nc</div><div class="line">11:03:16.763893 IP dcep-blockchain-1.terabase &gt; dcep-blockchain-1.12346: Flags [P.], seq 1:13, ack 1, win 342, options [nop,nop,TS val 2329725966 ecr 2329725964], length 12</div><div class="line">11:03:16.763904 IP dcep-blockchain-1.12346 &gt; dcep-blockchain-1.terabase: Flags [.], ack 13, win 342, options [nop,nop,TS val 2329725966 ecr 2329725966], length 0</div></pre></td></tr></table></figure>
<p>如果在上面的1.5ms之间nc  reset了这个连接，那么这12字节就发不出来了</p>
<h3 id="握手后Server主动发数据的行为非常像MySQL-Server"><a href="#握手后Server主动发数据的行为非常像MySQL-Server" class="headerlink" title="握手后Server主动发数据的行为非常像MySQL Server"></a>握手后Server主动发数据的行为非常像MySQL Server</h3><p>MySQL Server在收到mysql client连接后会主动发送 Server Greeting、版本号、认证方式等给client</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">#nc -p 12345 127.0.0.1 3306</div><div class="line">J</div><div class="line">5.6.29�CuaV9v0xo�!</div><div class="line">                  qCHRrGRIJqvzmysql_native_password  </div><div class="line">                  </div><div class="line">#tcpdump -i any port 12345 -ennt</div><div class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</div><div class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 76: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [S], seq 3186409724, win 43690, options [mss 65495,sackOK,TS val 3967896050 ecr 0,nop,wscale 7], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 76: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [S.], seq 4188709983, ack 3186409725, win 43690, options [mss 65495,sackOK,TS val 3967896051 ecr 3967896050,nop,wscale 7], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 1, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 0 // 握手完毕</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 146: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [P.], seq 1:79, ack 1, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 78 //Server 发出Greeting</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 79, win 342, options [nop,nop,TS val 3967896051 ecr 3967896051], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.3306 &gt; 127.0.0.1.12345: Flags [F.], seq 79, ack 1, win 342, options [nop,nop,TS val 3967913551 ecr 3967896051], length 0</div><div class="line"> In 00:00:00:00:00:00 ethertype IPv4 (0x0800), length 68: 127.0.0.1.12345 &gt; 127.0.0.1.3306: Flags [.], ack 80, win 342, options [nop,nop,TS val 3967913591 ecr 3967913551], length 0</div></pre></td></tr></table></figure>
<p>如下是Server发出的长度为 78 的 Server Greeting信息内容</p>
<p><img src="/images/oss/203c52d94018bbf72dfd4fc64d8a237b.png" alt="image.png"></p>
<p>理论上如果slb探活连接检查MySQL Server的状态的时候也是很快reset了，如果MySQL Server程序写得烂的话也会出现同样的情况。</p>
<p>但是比如我们有实验验证MySQL  Server 是否正常的时候会用 nc 去测试，一般以能看到</p>
<blockquote>
<p>5.6.29�CuaV9v0xo�!<br>                  qCHRrGRIJqvzmysql_native_password </p>
</blockquote>
<p>就认为MySQL Server是正常的。但是真的是这样吗？我们看看 nc 的如下案例</p>
<h4 id="nc-6-4-快速fin"><a href="#nc-6-4-快速fin" class="headerlink" title="nc 6.4 快速fin"></a>nc 6.4 快速fin</h4><blockquote>
<p>#nc –version<br>Ncat: Version 6.40 ( <a href="http://nmap.org/ncat" target="_blank" rel="external">http://nmap.org/ncat</a> )</p>
</blockquote>
<p>用 nc 测试发现有一定的概率没有出现上面的Server Greeting信息，那么这是因为MySQL Server服务不正常了吗？</p>
<p><img src="/images/oss/1607660605575-1305739f-1621-4a01-89ad-0f81eef94922.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<blockquote>
<p> nc -i 3 10.97.170.11 3306 -w 4 -p 1234</p>
</blockquote>
<p>-i 3 表示握手成功后 等三秒钟nc退出（发fin）</p>
<p>nc 6.4 握手后立即发fin断开连接，导致可能收不到Greeting，换成7.5或者mysql client就OK了</p>
<p>nc 7.5的抓包，明显可以看到nc在发fin前会先等4秒钟：</p>
<p><img src="/images/oss/1607660937618-d66c4074-9aa2-44cb-8054-f7d3680d1181.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>
<h3 id="tcpping-模拟slb-探活"><a href="#tcpping-模拟slb-探活" class="headerlink" title="tcpping 模拟slb 探活"></a>tcpping 模拟slb 探活</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tcpping.py -R -i 0.1 -t 1 dcep-blockchain-1 4000</div></pre></td></tr></table></figure>
<p>-i 间隔0.1秒 </p>
<p>-R reset断开连接</p>
<p>-t 超时时间1秒</p>
<p>执行如上代码，跟4000端口握手，然后立即发出reset断开连接（完全模拟slb探活行为），很快重现了问题</p>
<p>增加延时</p>
<p>-D 0.01表示握手成功后10ms后再发出reset（让应用有机会成功发出那12个字节），应用工作正常</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tcpping.py -R -i 0.1 -t 1 -D 0.01 dcep-blockchain-1 4000</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最大的错误认知就是 ss 看到的全连接队列数量，netstat也能看到。实际是不一定，而这个快速reset+应用不accept就导致了看不到这个现象</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/01/28/journald和rsyslog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/28/journald和rsyslog/" itemprop="url">journald和rsyslogd</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-28T17:30:03+08:00">
                2021-01-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="journald和rsyslogd"><a href="#journald和rsyslogd" class="headerlink" title="journald和rsyslogd"></a>journald和rsyslogd</h1><p>碰到rsyslog-8.24.0-34.1.al7.x86_64 的 rsyslogd 占用内存过高，于是分析了一下原因并学习了一下系统日志、rsyslog、journald之间的关系，流水账记录此文。</p>
<h2 id="rsyslogd-占用内存过高的分析"><a href="#rsyslogd-占用内存过高的分析" class="headerlink" title="rsyslogd 占用内存过高的分析"></a>rsyslogd 占用内存过高的分析</h2><p>rsyslogd使用了大概1.6-2G内存，不正常（正常情况下内存占用30-50M之间）</p>
<p>现象：</p>
<p><img src="/images/oss/12d137f9416d7935dbe6540c626ca8b4.png" alt="image.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">KiB Mem :  7971268 total,   131436 free,  7712020 used,   127812 buff/cache</div><div class="line">KiB Swap:        0 total,        0 free,        0 used.    43484 avail Mem</div><div class="line"></div><div class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</div><div class="line">24850 admin     20   0 8743896   5.1g      0 S   2.0 66.9   1413:55 java</div><div class="line"> 1318 root      20   0 2380404   1.6g    536 S   0.0 21.6 199:09.36 rsyslogd</div><div class="line"> </div><div class="line"># systemctl status rsyslog</div><div class="line">● rsyslog.service - System Logging Service</div><div class="line">   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Tue 2020-10-20 16:01:01 CST; 3 months 8 days ago</div><div class="line">     Docs: man:rsyslogd(8)</div><div class="line">           http://www.rsyslog.com/doc/</div><div class="line"> Main PID: 1318 (rsyslogd)</div><div class="line">   CGroup: /system.slice/rsyslog.service</div><div class="line">           └─1318 /usr/sbin/rsyslogd -n</div><div class="line"></div><div class="line">Jan 28 09:10:07 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 09:10:07 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 10:27:48 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 10:27:49 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 11:45:23 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 11:45:24 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 13:03:00 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 13:03:01 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 14:20:42 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 14:20:42 iZwz95gaul6x9167sqdqz4Z rsyslogd[1318]: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ] </div><div class="line"></div><div class="line"></div><div class="line"># grep HUPed /var/log/messages</div><div class="line">Jan 24 03:39:15 iZwz95gaul6x9167sqdqz4Z rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;8.24.0-34.1.al7&quot; x-pid=&quot;1318&quot; x-info=&quot;http://www.rsyslog.com&quot;] rsyslogd was HUPed</div><div class="line"></div><div class="line"># journalctl --verify</div><div class="line">PASS: /var/log/journal/20190829214900434421844640356160/system@efef6fd56e2e4c9f861d0be25c8c0781-0000000001567546-0005b9e2e02a0a4f.journal</div><div class="line">PASS: /var/log/journal/20190829214900434421844640356160/system@efef6fd56e2e4c9f861d0be25c8c0781-00000000015ae56b-0005b9ea76e922e9.journal</div><div class="line">1be1e0: Data object references invalid entry at 1d03018</div><div class="line">File corruption detected at /var/log/journal/20190829214900434421844640356160/system.journal:1d02d80 (of 33554432 bytes, 90%).</div><div class="line">FAIL: /var/log/journal/20190829214900434421844640356160/system.journal (Bad message)</div></pre></td></tr></table></figure>
<p><code>journalctl --verify</code>命令检查发现系统日志卷文件损坏</p>
<h3 id="问题根因"><a href="#问题根因" class="headerlink" title="问题根因"></a>问题根因</h3><p><a href="https://access.redhat.com/solutions/3705051" target="_blank" rel="external">来自redhat官网的描述</a></p>
<p><img src="/images/oss/e1a1cd75553b5cbe2a64e835ba9f99a7.png" alt="image.png"></p>
<p>以下是现场收集到的日志：</p>
<p><img src="/images/oss/cdfe3fb8d50ee148b816a82a432f1b88.png" alt="image.png"></p>
<p>主要是rsyslogd的sd_journal_get_cursor报错，然后导致内存泄露。</p>
<p>journald 报Bad message, 跟rsyslogd内存泄露完全没关系，实际上升级rsyslogd后也有journald bad message,但是rsyslogd的内存一直稳定在30M以内</p>
<p><a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="external">这个CSDN的文章中有完全一样的症状</a> 但是作者的结论是：这是systemd的bug，在journald需要压缩的时候就会发生这个问题。实际上我用的是 systemd-219-62.6.al7.9.x86_64 比他描述的已经修复的版本还要要新，也还是有这个问题，所以这个结论是不对的</p>
<h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>1、重启rsyslog <code>systemctl restart rsyslog</code> 可以释放内存</p>
<p>2、升级rsyslog到rsyslog-8.24.0-38.1.al7.x86_64或更新的版本才能彻底修复这个问题</p>
<h3 id="一些配置方法"><a href="#一些配置方法" class="headerlink" title="一些配置方法"></a>一些配置方法</h3><p>修改配置/etc/rsyslog.conf，增加如下两行，然后重启<code>systemctl restart rsyslog</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$imjournalRatelimitInterval 0</div><div class="line">$imjournalRatelimitBurst 0</div><div class="line">12</div></pre></td></tr></table></figure>
<p>1、关掉journal压缩配置</p>
<p>vi /etc/systemd/journald.conf，把#Compress=yes改成Compress=no，之后systemctl restart systemd-journald即可</p>
<p>2、限制rsyslogd 内存大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">cat /etc/systemd/system/multi-user.target.wants/rsyslog.service</div><div class="line"></div><div class="line">在Service配置中添加MemoryAccounting=yes，MemoryMax=80M，MemoryHigh=8M三项如下所示。</div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line">EnvironmentFile=-/etc/sysconfig/rsyslog</div><div class="line">ExecStart=/usr/sbin/rsyslogd -n $SYSLOGD_OPTIONS</div><div class="line">Restart=on-failure</div><div class="line">UMask=0066</div><div class="line">StandardOutput=null</div><div class="line">Restart=on-failure</div><div class="line">MemoryAccounting=yes</div><div class="line">MemoryMax=80M</div><div class="line">MemoryHigh=8M</div></pre></td></tr></table></figure>
<h2 id="OOM-kill"><a href="#OOM-kill" class="headerlink" title="OOM kill"></a>OOM kill</h2><p>rsyslogd内存消耗过高后导致了OOM Kill</p>
<p><img src="/images/oss/c7332f5b48506ea1faa015cfc6ae1709.png" alt="image.png"></p>
<p><strong>RSS对应物理内存，单位是4K（page大小）</strong>，红框两个进程用了5G+2G，总内存是8G，所以触发OOM killer了</p>
<p>每次OOM Kill日志前后总带着systemd-journald的重启</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z journal: Permanent journal is using 520.0M (max allowed 500.0M, trying to leave 4.0G free of 83.7G available → current limit 520.0M).</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z journal: Journal started</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: AliYunDun invoked oom-killer: gfp_mask=0x6200ca(GFP_HIGHUSER_MOVABLE), nodemask=(null), order=0, oom_score_adj=0</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: AliYunDun cpuset=/ mems_allowed=0</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: CPU: 3 PID: 13296 Comm: AliYunDun Tainted: G           OE     4.19.57-15.1.al7.x86_64 #1</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Hardware name: Alibaba Cloud Alibaba Cloud ECS, BIOS 8c24b4c 04/01/2014</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Call Trace:</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: dump_stack+0x5c/0x7b</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: dump_header+0x77/0x29f</div><div class="line">***</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: [  18118]     0 18118    28218      255   245760        0             0 sshd</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Out of memory: Kill process 18665 (java) score 617 or sacrifice child</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: Killed process 18665 (java) total-vm:8446992kB, anon-rss:4905856kB, file-rss:0kB, shmem-rss:0kB</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z kernel: oom_reaper: reaped process 18665 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z systemd: systemd-journald.service watchdog timeout (limit 3min)!</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z rsyslogd: sd_journal_get_cursor() failed: &apos;Cannot assign requested address&apos;  [v8.24.0-34.1.al7]</div><div class="line">Jan 28 19:03:04 iZwz95gaul6x9167sqdqz5Z rsyslogd: imjournal: journal reloaded... [v8.24.0-34.1.al7 try http://www.rsyslog.com/e/0 ]</div><div class="line">Jan 28 20:14:38 iZwz95gaul6x9167sqdqz5Z rsyslogd: imjournal: journal reloaded... [v8.24.0-57.1.al7 try http://www.rsyslog.com/e/0 ]</div></pre></td></tr></table></figure>
<p><img src="/images/oss/45008a8323742fb7f145211a6281afbc.png" alt="image.png"></p>
<p>OOM kill前大概率伴随着systemd-journald 重启是因为watch dog timeout(limit 3min)，造成timeout的原因是journald定期要把日志刷到磁盘上，然后要么是内存不够，要么是io负载太重，导致刷磁盘这个过程非常慢，于是就timeout了。</p>
<p>当然systemd-journald 重启也不一定意味着OOM Killer，只是肯定是内存比较紧张了。</p>
<h2 id="What-is-the-difference-between-syslog-rsyslog-and-syslog-ng"><a href="#What-is-the-difference-between-syslog-rsyslog-and-syslog-ng" class="headerlink" title="What is the difference between syslog, rsyslog and syslog-ng? "></a><a href="https://serverfault.com/questions/692309/what-is-the-difference-between-syslog-rsyslog-and-syslog-ng" target="_blank" rel="external">What is the difference between syslog, rsyslog and syslog-ng? </a></h2><p>Basically, they are all the same, in the way they all permit the logging of data from different types of systems in a central repository.</p>
<p>But they are three different project, each project trying to improve the previous one with more reliability and functionalities.</p>
<p>The <code>Syslog</code> project was the very first project. It started in 1980. It is the root project to <code>Syslog</code> protocol. At this time Syslog is a very simple protocol. At the beginning it only supports UDP for transport, so that it does not guarantee the delivery of the messages.</p>
<p>Next came <code>syslog-ng</code> in 1998. It extends basic <code>syslog</code> protocol with new features like:</p>
<ul>
<li>content-based filtering</li>
<li>Logging directly into a database</li>
<li>TCP for transport</li>
<li>TLS encryption</li>
</ul>
<p>Next came <code>Rsyslog</code> in 2004. It extends <code>syslog</code> protocol with new features like:</p>
<ul>
<li>RELP Protocol support</li>
<li>Buffered operation support</li>
</ul>
<h2 id="rsyslog和journald的基础知识"><a href="#rsyslog和journald的基础知识" class="headerlink" title="rsyslog和journald的基础知识"></a>rsyslog和journald的基础知识</h2><p><code>systemd-journald</code>是用来协助<code>rsyslog</code>记录系统启动服务和服务启动失败的情况等等. <code>systemd-journald</code>使用内存保存记录, 系统重启记录会丢失. 所有还要用<code>rsyslog</code>来记录分类信息, 如上面<code>/etc/rsyslog.d/listen.conf</code>中的<code>syslog</code>分类.</p>
<p><code>systemd-journald</code>跟随systemd开机就启动，能及时记录所有日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># systemd-analyze critical-chain systemd-journald.service</div><div class="line">The time after the unit is active or started is printed after the &quot;@&quot; character.</div><div class="line">The time the unit takes to start is printed after the &quot;+&quot; character.</div><div class="line"></div><div class="line">systemd-journald.service +13ms</div><div class="line">└─system.slice</div><div class="line">  └─-.slice</div></pre></td></tr></table></figure>
<p>systemd-journald 由于是使用于内存的登录文件记录方式，因此重新开机过后，开机前的登录文件信息当然就不会被记载了。 为此，我们还是建议启动 rsyslogd 来协助分类记录！也就是说， systemd-journald 用来管理与查询这次开机后的登录信息，而 rsyslogd 可以用来记录以前及现在的所以数据到磁盘文件中，方便未来进行查询喔！</p>
<p><strong>Tips</strong> 虽然 systemd-journald 所记录的数据其实是在内存中，但是系统还是利用文件的型态将它记录到 /run/log/ 下面！ 不过我们从前面几章也知道， /run 在 CentOS 7 其实是内存内的数据，所以重新开机过后，这个 /run/log 下面的数据当然就被刷新，旧的当然就不再存在了！</p>
<blockquote>
<p>其实鸟哥是这样想的，既然我们还有 rsyslog.service 以及 logrotate 的存在，因此这个 systemd-journald.service 产生的登录文件， 个人建议最好还是放置到 /run/log 的内存当中，以加快存取的速度！而既然 rsyslog.service 可以存放我们的登录文件， 似乎也没有必要再保存一份 journal 登录文件到系统当中就是了。单纯的建议！如何处理，依照您的需求即可喔！</p>
</blockquote>
<p><strong><code>system-journal</code>服务监听 <code>/dev/log</code> socket获取日志, 保存在内存中, 并间歇性的写入<code>/var/log/journal</code>目录中.</strong></p>
<p><code>rsyslog</code>服务启动后监听<code>/run/systemd/journal/socket</code> 获取syslog类型日志, 并写入<code>/var/log/messages</code>文件中. </p>
<p>获取日志时需要记录日志条目的<code>position</code>到<code>/var/lib/rsyslog/imjournal.state</code>文件中.</p>
<p>比如haproxy日志配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># cat /etc/haproxy/haproxy.cfg</div><div class="line">global</div><div class="line"># log发给journald(journald监听 /dev/log)</div><div class="line">        log /dev/log    local1 warning</div></pre></td></tr></table></figure>
<p>以下是drds 的iptables日志配置，将tcp reset包记录下来，默认iptable日志输出到/varlog/messages中（dmesg也能看到），然后可以通过rsyslog.d 配置将这部分日志输出到单独的文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># 配置iptables 日志，增加 [drds] 标识</div><div class="line"># cat /home/admin/drds-worker/install/drds_filter.conf</div><div class="line"># Generated by iptables-save v1.4.21 on Wed Apr  1 11:39:31 2020</div><div class="line">*filter</div><div class="line">:INPUT ACCEPT [557:88127]</div><div class="line">:FORWARD ACCEPT [0:0]</div><div class="line">:OUTPUT ACCEPT [527:171711]</div><div class="line">-A INPUT -p tcp -m tcp ! --sport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</div><div class="line"># -A INPUT -p tcp -m tcp ! --dport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level7 --log-tcp-sequence --log-tcp-options --log-ip-options</div><div class="line">-A OUTPUT -p tcp -m tcp ! --sport 3406 --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</div><div class="line">COMMIT</div><div class="line"># Completed on Wed Apr  1 11:39:31 2020</div><div class="line"></div><div class="line">#通过rsyslogd将日志写出到指定位置(不配置的话默认输出到 dmesg)</div><div class="line"># cat /etc/rsyslog.d/drds_filter_log.conf</div><div class="line">:msg, startswith, &quot;[drds]&quot; -/home/admin/logs/tcp-rt/drds-tcp.log</div></pre></td></tr></table></figure>
<h3 id="journald-log持久化"><a href="#journald-log持久化" class="headerlink" title="journald log持久化"></a>journald log持久化</h3><p>创建 /var/log/journal 文件夹后默认会持久化，设置持久化后 /run/log 里面就没有日志了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"># cat /etc/systemd/journald.conf</div><div class="line">#  This file is part of systemd.</div><div class="line">#</div><div class="line">#  systemd is free software; you can redistribute it and/or modify it</div><div class="line">#  under the terms of the GNU Lesser General Public License as published by</div><div class="line">#  the Free Software Foundation; either version 2.1 of the License, or</div><div class="line">#  (at your option) any later version.</div><div class="line">#</div><div class="line"># Entries in this file show the compile time defaults.</div><div class="line"># You can change settings by editing this file.</div><div class="line"># Defaults can be restored by simply deleting this file.</div><div class="line">#</div><div class="line"># See journald.conf(5) for details.</div><div class="line"></div><div class="line">[Journal]</div><div class="line">#Storage=auto  //默认如果有 /var/log/journal 目录就会持久化到这里</div><div class="line">Compress=no</div><div class="line">#Seal=yes</div><div class="line">#SplitMode=uid</div><div class="line">#SyncIntervalSec=5m</div><div class="line">#RateLimitInterval=30s</div><div class="line">#RateLimitBurst=1000</div><div class="line">SystemMaxUse=500M   //最多保留500M日志文件，免得撑爆磁盘</div><div class="line">#SystemKeepFree=</div><div class="line">#SystemMaxFileSize=</div><div class="line">#RuntimeMaxUse=</div><div class="line">#RuntimeKeepFree=</div><div class="line">#RuntimeMaxFileSize=</div><div class="line">#MaxRetentionSec=</div><div class="line">#MaxFileSec=1month</div><div class="line">#ForwardToSyslog=yes</div><div class="line">#ForwardToKMsg=no</div><div class="line">#ForwardToConsole=no</div><div class="line">#ForwardToWall=yes</div><div class="line">#TTYPath=/dev/console</div><div class="line">#MaxLevelStore=debug</div><div class="line">#MaxLevelSyslog=debug</div><div class="line">#MaxLevelKMsg=notice</div><div class="line">#MaxLevelConsole=info</div><div class="line">#MaxLevelWall=emerg</div><div class="line">#LineMax=48K</div></pre></td></tr></table></figure>
<p>清理日志保留1M：journalctl –vacuum-size=1M </p>
<p>设置最大保留500M日志： journalctl –vacuum-size=500</p>
<h3 id="rsyslogd"><a href="#rsyslogd" class="headerlink" title="rsyslogd"></a>rsyslogd</h3><p>以下内容来自鸟哥的书：</p>
<p>CentOS 7 除了保有既有的 rsyslog.service 之外，其实最上游还使用了 systemd 自己的登录文件日志管理功能喔！他使用的是 systemd-journald.service 这个服务来支持的。基本上，系统由 systemd 所管理，那所有经由 systemd 启动的服务，如果再启动或结束的过程中发生一些问题或者是正常的讯息， 就会将该讯息由 systemd-journald.service 以二进制的方式记录下来，之后再将这个讯息发送给 rsyslog.service 作进一步的记载。</p>
<p>基本上， rsyslogd 针对各种服务与讯息记录在某些文件的配置文件就是 /etc/rsyslog.conf， 这个文件规定了“（1）什么服务 （2）的什么等级讯息 （3）需要被记录在哪里（设备或文件）” 这三个咚咚，所以设置的语法会是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$cat /etc/rsyslog.conf</div><div class="line">服务名称[.=!]讯息等级        讯息记录的文件名或设备或主机</div><div class="line"># 下面以 mail 这个服务产生的 info 等级为例：</div><div class="line">mail.info            /var/log/maillog_info</div><div class="line"># 这一行说明：mail 服务产生的大于等于 info 等级的讯息，都记录到</div><div class="line"># /var/log/maillog_info 文件中的意思。</div></pre></td></tr></table></figure>
<p><img src="/images/oss/1cce7612a84cf1a1addceeff6032cb5c.png" alt="syslog 所制订的服务名称与软件调用的方式"></p>
<p> CentOS 7.x 默认的 rsyslogd 本身就已经具有远程日志服务器的功能了， 只是默认并没有启动该功能而已。你可以通过 man rsyslogd 去查询一下相关的选项就能够知道啦！ 既然是远程日志服务器，那么我们的 Linux 主机当然会启动一个端口来监听了，那个默认的端口就是 UDP 或 TCP 的 port 514 </p>
<p><img src="/images/oss/40740cd5cfc8896c07c15b959420646f.png" alt="image.png"></p>
<p>Server配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">$ cat /etc/rsyslog.conf</div><div class="line"># 找到下面这几行：</div><div class="line"># Provides UDP syslog reception</div><div class="line">#$ModLoad imudp</div><div class="line">#$UDPServerRun 514</div><div class="line"></div><div class="line"># Provides TCP syslog reception</div><div class="line">#$ModLoad imtcp</div><div class="line">#$InputTCPServerRun 514</div><div class="line"># 上面的是 UDP 端口，下面的是 TCP 端口！如果你的网络状态很稳定，就用 UDP 即可。</div><div class="line"># 不过，如果你想要让数据比较稳定传输，那么建议使用 TCP 啰！所以修改下面两行即可！</div><div class="line">$ModLoad imtcp</div><div class="line">$InputTCPServerRun 514</div><div class="line"></div><div class="line"># 2\. 重新启动与观察 rsyslogd 喔！</div><div class="line">[root@study ~]# systemctl restart rsyslog.service</div><div class="line">[root@study ~]# netstat -ltnp &amp;#124; grep syslog</div><div class="line">Proto Recv-Q Send-Q Local Address  Foreign Address   State    PID/Program name</div><div class="line">tcp        0      0 0.0.0.0:514    0.0.0.0:*         LISTEN   2145/rsyslogd</div><div class="line">tcp6       0      0 :::514         :::*              LISTEN   2145/rsyslogd</div><div class="line"># 嘿嘿！你的登录文件主机已经设置妥当啰！很简单吧！</div></pre></td></tr></table></figure>
<p>client配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cat /etc/rsyslog.conf</div><div class="line">*.*       @@192.168.1.100</div><div class="line">#*.*       @192.168.1.100  # 若用 UDP 传输，设置要变这样！</div></pre></td></tr></table></figure>
<p>常见的几个系统日志有哪些呢？一般而言，有下面几个：</p>
<ul>
<li>/var/log/boot.log： 开机的时候系统核心会去侦测与启动硬件，接下来开始各种核心支持的功能启动等。这些流程都会记录在 /var/log/boot.log 里面哩！ 不过这个文件只会存在这次开机启动的信息，前次开机的信息并不会被保留下来！</li>
<li>/var/log/cron： 还记得<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html" target="_blank" rel="external">第十五章例行性工作调度</a>吧？你的 crontab 调度有没有实际被进行？ 进行过程有没有发生错误？你的 /etc/crontab 是否撰写正确？在这个登录文件内查询看看。</li>
<li>/var/log/dmesg： 记录系统在开机的时候核心侦测过程所产生的各项信息。由于 CentOS 默认将开机时核心的硬件侦测过程取消显示， 因此额外将数据记录一份在这个文件中；</li>
<li>/var/log/lastlog： 可以记录系统上面所有的帐号最近一次登陆系统时的相关信息。<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#uselinux_find" target="_blank" rel="external">第十三章讲到的 lastlog</a> 指令就是利用这个文件的记录信息来显示的。</li>
<li>/var/log/maillog 或 /var/log/mail/*： 记录邮件的往来信息，其实主要是记录 postfix （SMTP 协定提供者） 与 dovecot （POP3 协定提供者） 所产生的讯息啦。 SMTP 是发信所使用的通讯协定， POP3 则是收信使用的通讯协定。 postfix 与 dovecot 则分别是两套达成通讯协定的软件。</li>
<li>/var/log/messages： 这个文件相当的重要，几乎系统发生的错误讯息 （或者是重要的信息） 都会记录在这个文件中； 如果系统发生莫名的错误时，这个文件是一定要查阅的登录文件之一。</li>
<li>/var/log/secure： 基本上，只要牵涉到“需要输入帐号密码”的软件，那么当登陆时 （不管登陆正确或错误） 都会被记录在此文件中。 包括系统的 login 程序、图形接口登陆所使用的 gdm 程序、 su, sudo 等程序、还有网络连线的 ssh, telnet 等程序， 登陆信息都会被记载在这里；</li>
<li>/var/log/wtmp, /var/log/faillog： 这两个文件可以记录正确登陆系统者的帐号信息 （wtmp） 与错误登陆时所使用的帐号信息 （faillog） ！ 我们在<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/Text/index.html#last" target="_blank" rel="external">第十章谈到的 last</a> 就是读取 wtmp 来显示的， 这对于追踪一般帐号者的使用行为很有帮助！</li>
<li>/var/log/httpd/<em>, /var/log/samba/</em>： 不同的网络服务会使用它们自己的登录文件来记载它们自己产生的各项讯息！上述的目录内则是个别服务所制订的登录文件。</li>
</ul>
<h2 id="journalctl-常用参数"><a href="#journalctl-常用参数" class="headerlink" title="journalctl 常用参数"></a><a href="https://linuxhint.com/journalctl-tail-and-cheatsheet/" target="_blank" rel="external">journalctl 常用参数</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">-n or –lines= Show the most recent **n** number of log lines</div><div class="line"></div><div class="line">-f or –follow Like a tail operation for viewing live updates</div><div class="line"></div><div class="line">-S, –since=, -U, –until= Search based on a date. “2019-07-04 13:19:17”, “00:00:00”, “yesterday”, “today”, “tomorrow”, “now” are valid formats. For complete time and date specification, see systemd.time(7)</div><div class="line"></div><div class="line">-u service unit</div></pre></td></tr></table></figure>
<p>清理journald日志</p>
<blockquote>
<p> journalctl –vacuum-size=1M &amp;&amp; journalctl –vacuum-size=500</p>
</blockquote>
<h2 id="logrotate"><a href="#logrotate" class="headerlink" title="logrotate"></a>logrotate</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">/var/log/cron</div><div class="line">&#123;</div><div class="line">    sharedscripts</div><div class="line">    postrotate</div><div class="line">        /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</div><div class="line">    endscript</div><div class="line">&#125;</div><div class="line"></div><div class="line">执行：</div><div class="line">sudo /usr/sbin/logrotate --force --verbose /etc/logrotate.d/drds</div><div class="line">debug：</div><div class="line">sudo /usr/sbin/logrotate -d --verbose /etc/logrotate.d/drds</div><div class="line">查看日志：</div><div class="line">cat /var/lib/logrotate/logrotate.status</div></pre></td></tr></table></figure>
<h3 id="kill-HUP"><a href="#kill-HUP" class="headerlink" title="kill -HUP"></a><a href="https://unix.stackexchange.com/questions/440004/why-is-kill-hup-used-in-logrotate-in-rhel-is-it-necessary-in-all-cases" target="_blank" rel="external">kill -HUP</a></h3><p>Generally services keep the log files opened while they are running. This mean that they do not care if the log files are renamed/moved or deleted they will continue to write to the open file handled.</p>
<p>When logrotate move the files, the services keep writing to the same file.</p>
<p>Example: syslogd will write to /var/log/cron.log. Then logrotate will rename the file to /var/log/cron.log.1, so syslogd will keep writing to the open file /var/log/cron.log.1.</p>
<p>Sending the HUP signal to syslogd will force him to close existing file handle and open new file handle to the original path /var/log/cron.log which will create a new file.</p>
<p>The use of the HUP signal instead of another one is at the discretion of the program. Some services like php-fpm will listen to the USR1 signal to reopen it’s file handle without terminating itself.</p>
<p>不过还得看应用是否屏蔽了 HUP 信号</p>
<h2 id="systemd"><a href="#systemd" class="headerlink" title="systemd"></a>systemd</h2><p>sudo systemctl list-unit-files –type=service | grep enabled //列出启动项</p>
<p> journalctl -b -1 //复审前一次启动， -2 复审倒数第 2 次启动. 重演你的系统启动的所有消息</p>
<p>sudo systemd-analyze blame   <strong>sudo systemd-analyze critical-chain</strong></p>
<p>systemd-analyze critical-chain –fuzz 1h</p>
<p>sudo systemd-analyze blame networkd</p>
<p>systemd-analyze critical-chain network.target local-fs.target</p>
<p><img src="/images/oss/bb21293e-9b52-40f9-9ab2-7c5aeb7beca1.png" alt="img"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>一模一样的症状，但是根因找错了：<a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="external">rsyslog占用内存高</a> </p>
<p><a href="https://access.redhat.com/solutions/3705051" target="_blank" rel="external">https://access.redhat.com/solutions/3705051</a></p>
<p><a href="https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html" target="_blank" rel="external">https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html</a></p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html" target="_blank" rel="external">鸟哥 journald 介绍</a></p>
<p><a href="https://linuxhint.com/journalctl-tail-and-cheatsheet/" target="_blank" rel="external">journalctl tail and cheatsheet</a></p>
<p><a href="https://lp007819.wordpress.com/2015/01/17/systemd-journal-介绍/" target="_blank" rel="external">Journal的由来</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/01/15/TCP传输速度案例分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/15/TCP传输速度案例分析/" itemprop="url">TCP传输速度案例分析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-15T17:30:03+08:00">
                2021-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TCP传输速度案例分析"><a href="#TCP传输速度案例分析" class="headerlink" title="TCP传输速度案例分析"></a>TCP传输速度案例分析</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>TCP传输速度受网络带宽和传输窗口的影响（接收、发送、拥塞窗口），带宽我们没办法改变，以下案例主要是讨论rt、窗口如何影响速度。</p>
<p>详细的buffer、rt对TCP传输速度的影响请看这篇：</p>
<p> <a href="/2019/09/28/就是要你懂TCP--性能和发送接收Buffer的关系/">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></p>
<p>以及 <a href="/2018/06/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%9C%80%E7%BB%8F%E5%85%B8%E7%9A%84TCP%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/">就是要你懂TCP–最经典的TCP性能问题 Nagle和Delay ack</a></p>
<p>上面两篇以及下面几个案例读完，应该所有TCP传输速度问题都能解决了。</p>
<h2 id="前后端rtt差异大-vip下载慢的案例"><a href="#前后端rtt差异大-vip下载慢的案例" class="headerlink" title="前后端rtt差异大+vip下载慢的案例"></a>前后端rtt差异大+vip下载慢的案例</h2><p>来源：<a href="https://mp.weixin.qq.com/s/er8vTKZUcahA6-Pf8DZBng" target="_blank" rel="external">https://mp.weixin.qq.com/s/er8vTKZUcahA6-Pf8DZBng</a> 文章中的trace-cmd工具也不错</p>
<p>如下三个链路，有一个不正常了</p>
<p><img src="/images/oss/2422ae219d3b27cfe8c799642662d5b2.png" alt="image.png"></p>
<p>首先通过 ss -it dst “ip:port” 来分析cwnd、ssthresh、buffer，到底是什么导致了传输慢</p>
<h3 id="原因TCPLossProbe："><a href="#原因TCPLossProbe：" class="headerlink" title="原因TCPLossProbe："></a>原因TCPLossProbe：</h3><p>如果尾包发生了丢包，没有新包可发送触发多余的dup ack来实现快速重传，完全依赖RTO超时来重传，代价太大，那如何能优化解决这种尾丢包的情况。也就是在某些情况下一个可以的重传包就能触发ssthresh减半，从而导致传输速度上不来。</p>
<p>本案例中，因为client到TGW跨了地域，导致rtt增大，但是TGW和STGW之间的rtt很小，导致握手完毕后STGW认为和client的rtt很小，所以很快就触发了丢包重传，实际没有丢包，只是rtt变大了，所以触发了如上的TLP( PTO=max(2rtt, 10ms) ， 因为只有一次重传并收到了 dup，还是不应该触发TLP，但是因为老版本kernel bug导致，4.0的kernel修复了这个问题， 函数 is_tlp_dupack)</p>
<p>握手完毕后第七号包很快重传了</p>
<p><img src="/images/oss/2867daa600363af61f8f971479246858.png" alt="image.png"></p>
<h3 id="观察："><a href="#观察：" class="headerlink" title="观察："></a>观察：</h3><p>netstat -s |grep TCPLossProbes</p>
<h3 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h3><p>tcp_early_retrans可用于开启和关闭ER和TLP，默认是3（enable TLP and delayed ER），sysctl -w net.ipv4.tcp_early_retrans=2 关掉TLP</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>kernel版本小于4.0+TLP开启+VIP代理导致RS认为rtt很小，实际比较大，这两个条件下就会出现如上问题。</p>
<p>这个问题一看就是跟client和VIP代理之间的rtt扩大有关系，不过不是因为扩大后发送窗口不够之类导致的。</p>
<h2 id="长肥网络（高rtt）场景下tcp-metrics记录的ssthresh太小导致传输慢的案例"><a href="#长肥网络（高rtt）场景下tcp-metrics记录的ssthresh太小导致传输慢的案例" class="headerlink" title="长肥网络（高rtt）场景下tcp_metrics记录的ssthresh太小导致传输慢的案例"></a>长肥网络（高rtt）场景下tcp_metrics记录的ssthresh太小导致传输慢的案例</h2><p><a href="https://www.atatech.org/articles/109967" target="_blank" rel="external">https://www.atatech.org/articles/109967</a></p>
<blockquote>
<p>tcp_metrics会记录下之前已关闭tcp 连接的状态，包括发送端拥塞窗口和拥塞控制门限，如果之前网络有一段时间比较差或者丢包比较严重，就会导致tcp 的拥塞控制门限ssthresh降低到一个很低的值，这个值在连接结束后会被tcp_metrics cache 住，在新连接建立时，即使网络状况已经恢复，依然会继承 tcp_metrics 中cache 的一个很低的ssthresh 值，在长肥管道情况下，新连接经历短暂的“慢启动”后，随即进入缓慢的拥塞控制阶段, 导致连接速度很难在短时间内上去。而后面的连接，需要很特殊的场景之下才能将ssthresh 再次推到一个比较高的值缓存下来，因此很有很能在接下来的很长一段时间，连接的速度都会处于一个很低的水平</p>
</blockquote>
<p>因为 tcp_metrics记录的ssthresh非常小，导致后面新的tcp连接传输数据时很快进入拥塞控制阶段，如果传输的文件不大的话就没有机会将ssthresh撑大。除非传输一个特别大的文件，忍受拥塞控制阶段的慢慢增长，最后tcp_metrics记录下撑大后的ssthresh，整个网络才会恢复正常。</p>
<p>所以关闭 tcp_metrics其实是个不错的选择： net.ipv4.tcp_no_metrics_save = 1 </p>
<p>或者清除： sudo ip tcp_metrics flush all</p>
<h3 id="从系统cache中查看-tcp-metrics-item"><a href="#从系统cache中查看-tcp-metrics-item" class="headerlink" title="从系统cache中查看 tcp_metrics item"></a>从系统cache中查看 tcp_metrics item</h3><pre><code>$sudo ip tcp_metrics show | grep  100.118.58.7
100.118.58.7 age 1457674.290sec tw_ts 3195267888/5752641sec ago rtt 1000us rttvar 1000us ssthresh 361 cwnd 40 ----这两个值对传输性能很重要

192.168.1.100 age 1051050.859sec ssthresh 4 cwnd 2 rtt 4805us rttvar 4805us source 192.168.0.174 ---这条记录有问题，缓存的ssthresh 4 cwnd 2都太小，传输速度一定慢 

清除 tcp_metrics, sudo ip tcp_metrics flush all 
关闭 tcp_metrics 功能，net.ipv4.tcp_no_metrics_save = 1
sudo ip tcp_metrics delete 100.118.58.7
</code></pre><p>每个连接的ssthresh默认是个无穷大的值，但是内核会cache对端ip上次的ssthresh（大部分时候两个ip之间的拥塞窗口大小不会变），这样大概率到达ssthresh之后就基本拥塞了，然后进入cwnd的慢增长阶段。</p>
<h2 id="长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响"><a href="#长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响" class="headerlink" title="长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响"></a>长肥网络（rt很高、带宽也高）下接收窗口对传输性能的影响</h2><p>最后通过一个实际碰到的案例，涉及到了接收窗口、发送Buffer以及高延时情况下的性能问题</p>
<p>案例描述：从中国访问美国的服务器下载图片，只能跑到220K，远远没有达到带宽能力，其中中美之间的网络延时时150ms，这个150ms已经不能再优化了。业务结构是：</p>
<p>client ——150ms—–&gt;&gt;&gt;LVS—1ms–&gt;&gt;&gt;美国的统一接入server—–1ms—–&gt;&gt;&gt;nginx</p>
<p>通过下载一个4M的文件大概需要20秒，分别在client和nginx上抓包来分析这个问题（统一接入server没权限上去）</p>
<h3 id="Nginx上抓包"><a href="#Nginx上抓包" class="headerlink" title="Nginx上抓包"></a>Nginx上抓包</h3><p><img src="/images/oss/259767fb17f7dbffe7f77ab059c47dbd.png" alt="image.png"></p>
<p>从这里可以看到Nginx大概在60ms内就将4M的数据都发完了</p>
<h3 id="client上抓包"><a href="#client上抓包" class="headerlink" title="client上抓包"></a>client上抓包</h3><p><img src="/images/oss/466fba92829f6a922ccd2d57a7e3fdac.png" alt="image.png"></p>
<p>从这个图上可以清楚看到大概每传输大概30K数据就有一个150ms的等待平台，这个150ms基本是client到美国的rt。</p>
<p>从我们前面的阐述可以清楚了解到因为rt比较高，统一接入server每发送30K数据后要等150ms才能收到client的ack，然后继续发送，猜是因为上面设置的发送buffer大概是30K。</p>
<p>检查统一接入server的配置，可以看到接入server的配置里面果然有个32K buffer设置</p>
<h3 id="将buffer改大"><a href="#将buffer改大" class="headerlink" title="将buffer改大"></a>将buffer改大</h3><p>速度可以到420K，但是还没有跑满带宽：</p>
<p><img src="/images/oss/93e254c5154ce2e065bec9fb34f3db2b.png" alt="image.png"></p>
<p><img src="/images/oss/0a8c68a58da6f169573b57cde0ffba93.png" alt="image.png"></p>
<p>接着看一下client上的抓包</p>
<p><img src="/images/oss/822737a4ed6ffe6b920d4b225a1be5bf.png" alt="image.png"></p>
<p>可以清楚看到 client的接收窗口是64K， 64K*1000/150=426K 这个64K很明显是16位的最大值，应该是TCP握手有一方不支持window scaling factor</p>
<p>那么继续分析一下握手包，syn：</p>
<p><img src="/images/oss/004886698ddbaa1cbc8342a9cd667c76.png" alt="image.png"></p>
<p>说明client是支持的，再看 syn+ack：</p>
<p><img src="/images/oss/70155e021390cb1ee07091c306c375f4.png" alt="image.png"></p>
<p>可以看到服务端不支持，那就最大只能用到64K。需要修改服务端代理程序，这主要是LVS或者代理的锅。</p>
<p>如果内网之间rt很小这个锅不会爆发，一旦网络慢一点就把问题恶化了</p>
<p>比如这是这个应用的开发人员的反馈：</p>
<p><img src="/images/oss/a08a204ec7ad4bba7867dacea1668322.png" alt="image.png"></p>
<p><a href="https://datatracker.ietf.org/doc/html/rfc1072" target="_blank" rel="external">长肥网络</a>就像是很长很宽的高速公路，上面可以同时跑很多车，而如果发车能力不够，就容易跑不满高速公路。<br>在rt很短的时候可以理解为高速公路很短，所以即使发车慢也还好，因为车很快就到了，到了后就又能发新车了。rt很长的话就要求更大的仓库了。</p>
<p>整个这个问题，我最初拿到的问题描述结构是这样的（不要笑程序员连自己的业务结构都描述不清）：</p>
<p>client ——150ms—–&gt;&gt;&gt;nginx</p>
<p>实际开发人员也不能完全描述清楚结构，从抓包中慢慢分析反推他们的结构，到最后问题的解决。</p>
<p>这个案例综合了发送窗口（32K）、接收窗口（64K，因为握手LVS不支持window scale）、rt很大将问题暴露出来（跨国网络，rt没法优化）。</p>
<p>nginx buffer 分析参考案例：<a href="https://juejin.cn/post/6875223721615818765" target="_blank" rel="external">https://juejin.cn/post/6875223721615818765</a> nginx上下游收发包速率不一致导致nginx buffer打爆, 关闭nginx proxy_buffering 可解 （作者：挖坑的张师傅）</p>
<p><img src="/images/951413iMgBlog/433762.png" alt="image.png"></p>
<h2 id="应用层发包逻辑影响了BDP不能跑满"><a href="#应用层发包逻辑影响了BDP不能跑满" class="headerlink" title="应用层发包逻辑影响了BDP不能跑满"></a>应用层发包逻辑影响了BDP不能跑满</h2><p>来自 dog250: <a href="https://zhuanlan.zhihu.com/p/413732839" target="_blank" rel="external">一行代码解决scp在Internet传输慢的问题（RT高的网络环境）</a> </p>
<blockquote>
<p>用scp在长链路上传输文件竟然慢到无法忍受！100～200毫秒往返时延的链路，wget下载文件吞吐可达40MBps，scp却只有9MBps。</p>
<p>这次不是因为buffer导致BDP跑不满，而是也scp业务层有自己流控的逻辑导致发包慢了</p>
<p><strong>SSH允许在一个TCP连接上复用多个channel，需要对每一个channel做流控以保证公平，所以每个channel必须自己做而不是使用TCP的流控，OpenSSH的实现有问题。</strong></p>
</blockquote>
<h2 id="delay-ack拉高实际rt的案例"><a href="#delay-ack拉高实际rt的案例" class="headerlink" title="delay ack拉高实际rt的案例"></a>delay ack拉高实际rt的案例</h2><p><strong>这个案例跟速度没有关系，只是解析监控图表上的rt为什么不符合逻辑地偏高了。</strong></p>
<p>如下业务监控图：实际处理时间（逻辑服务时间1ms，rtt2.4ms，加起来3.5ms），但是系统监控到的rt（蓝线）是6ms，如果一个请求分很多响应包串行发给client，这个6ms是正常的（1+2.4*N），但实际上如果send buffer足够的话，按我们前面的理解多个响应包会并发发出去，所以如果整个rt是3.5ms才是正常的。</p>
<p><img src="/images/oss/d56f87a19a10b0ac9a3b7009641247a0.png" alt="image.png"></p>
<p>抓包来分析原因：</p>
<p><img src="/images/oss/d5e2e358dd1a24e104f54815c84875c9.png" alt="image.png"></p>
<p>实际看到大量的response都是3.5ms左右，符合我们的预期，但是有少量rt被delay ack严重影响了</p>
<p>从下图也可以看到有很多rtt超过3ms的，这些超长时间的rtt会最终影响到整个服务rt</p>
<p><img src="/images/oss/48eae3dcd7c78a68b0afd5c66f783f23.png" alt="image.png"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.allanjude.com/bsd/AsiaBSDCon2017_-_SSH_Performance.pdf" target="_blank" rel="external">SSH Performance</a></p>
<p><a href="https://stackoverflow.com/questions/8849240/why-when-i-transfer-a-file-through-sftp-it-takes-longer-than-ftp" target="_blank" rel="external">Why when I transfer a file through SFTP, it takes longer than FTP?</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/01/03/mac路由和DSN相关知识/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/03/mac路由和DSN相关知识/" itemprop="url">mac 路由和DSN相关知识</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-03T17:30:03+08:00">
                2021-01-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/其它/" itemprop="url" rel="index">
                    <span itemprop="name">其它</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="mac-路由和DSN相关知识"><a href="#mac-路由和DSN相关知识" class="headerlink" title="mac 路由和DSN相关知识"></a>mac 路由和DSN相关知识</h1><p>Mac 下上网,尤其是在双网卡一起使用的时候, 一个网卡连内网，一个网卡连外网，经常会碰到ip不通(路由问题,比较好解决)或者dns解析不了问题. 或者是在通过VPN连公司网络会插入一些内网route,导致部分网络访问不了.</p>
<p>即使对Linux下的DNS解析无比熟悉了，但是在Mac下还是花了一些时间来折腾，配置不好路由和DNS是不配使用Mac的，所以记录下。</p>
<h2 id="route"><a href="#route" class="headerlink" title="route"></a>route</h2><p>如果ip不通就看路由表, 根据内外网IP增加/删除相应的路由信息,常用命令如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">sudo route -n add 10.176/16 192.168.3.1</div><div class="line">sudo route -n add -net 10.176.0.0/16 192.168.3.1 //添加路由, 访问10.176.0.0/16 走192.168.3.1 </div><div class="line">sudo route -n delete -net 10.176.0.0/16 192.168.3.1</div><div class="line">sudo route -n delete 0.0.0.0 192.168.184.1</div><div class="line">sudo route -n add 0.0.0.0 192.168.184.1  //添加默认路由访问外网 </div><div class="line"></div><div class="line">sudo route -n delete 0.0.0.0 192.168.3.1</div><div class="line">sudo route -n add 10.176/16 192.168.3.1</div><div class="line">sudo route -n delete 0.0.0.0 192.168.184.1 -ifscope en0</div><div class="line">sudo route -n add 0.0.0.0 192.168.184.1 </div><div class="line">sudo networksetup -setdnsservers 'Apple USB Ethernet Adapter' 202.106.196.115 202.106.0.20 114.114.114.114</div><div class="line"></div><div class="line">sudo networksetup -setdnsservers 'USB 10/100/1000 LAN' 223.5.5.5 30.30.30.30 114.114.114.114</div><div class="line"></div><div class="line">ip route get 8.8.8.8 //linux</div><div class="line">route get 8.8.8.8    //macos</div><div class="line">netstat -rn          //查看路由  </div><div class="line">netstat -nr -f inet  //只看ipv4相关路由</div></pre></td></tr></table></figure>
<p>如果本来IP能通,连上VPN后就通不了,那一定是VPN加入了一些更精细的路由导致原来的路由不通了,那么很简单停掉VPN就能恢复或者增加一条更精确的路有记录进去,或者删掉VPN增加的某条路由.</p>
<h2 id="DNS-解析"><a href="#DNS-解析" class="headerlink" title="DNS 解析"></a>DNS 解析</h2><p>mac下DNS解析问题搞起来比较费劲,相应的资料也不多, 经过上面的操作后如果IP能通,域名解析有问题,一般都是DNS解析出了问题</p>
<p><a href="https://shockerli.net/post/macos-hostname-scutil/" target="_blank" rel="external">mac下 /etc/resolv.conf 不再用来解析域名, 只有nslookup能用到resolv.conf</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">cat /etc/resolv.conf                                                </div><div class="line"><span class="meta">#</span><span class="bash"></span></div><div class="line"><span class="meta">#</span><span class="bash"> macOS Notice</span></div><div class="line"><span class="meta">#</span><span class="bash"></span></div><div class="line"><span class="meta">#</span><span class="bash"> This file is not consulted <span class="keyword">for</span> DNS hostname resolution, address</span></div><div class="line"><span class="meta">#</span><span class="bash"> resolution, or the DNS query routing mechanism used by most</span></div><div class="line"><span class="meta">#</span><span class="bash"> processes on this system.</span></div><div class="line"><span class="meta">#</span><span class="bash"></span></div><div class="line"><span class="meta">#</span><span class="bash"> To view the DNS configuration used by this system, use:</span></div><div class="line"><span class="meta">#</span><span class="bash">   scutil --dns</span></div><div class="line"></div><div class="line">scutil --dns //查看DNS 解析器</div><div class="line">scutil --nwi //查看网络</div></pre></td></tr></table></figure>
<p>解析出了问题先检查nameserver</p>
<p>scutil –dns 一般会展示一大堆的resolver, 每个resolver又可以有多个nameserver</p>
<blockquote>
<p>A scoped DNS query can use only specified network interfaces (e.g. Ethernet or WiFi), while non-scoped can use any available interface.</p>
<p>More verbosely, an application that wants to resolve a name, sends a <em>request</em> (either scoped or non-scoped) to a resolver (usually a DNS client application), if the resolver does not have the answer cached, it sends a DNS <em>query</em> to a particular nameserver (and this goes through one interface, so it is always “scoped”).</p>
<p>In your example resolver #1 “for scoped queries” can use only en0 interface (Ethernet).</p>
</blockquote>
<h3 id="修改-nameserver"><a href="#修改-nameserver" class="headerlink" title="修改 nameserver"></a>修改 nameserver</h3><p>默认用第一个resolver, 如果第一个resolver没有nameserver那么域名没法解析, 可以修改dns resolver的nameserver: </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash">networksetup -listallnetworkservices  //列出网卡service, 比如 wifi ,以下是我的 macos 输出</span></div><div class="line">An asterisk (*) denotes that a network service is disabled.</div><div class="line">USB 10/100/1000 LAN</div><div class="line">Apple USB Ethernet Adapter</div><div class="line">Wi-Fi</div><div class="line">Bluetooth PAN</div><div class="line">Thunderbolt Bridge</div><div class="line"><span class="meta">$</span><span class="bash">sudo networksetup -setdnsservers <span class="string">'Wi-Fi'</span> 202.106.196.115 202.106.0.20 114.114.114.114 //修改nameserver</span></div><div class="line"><span class="meta">$</span><span class="bash">networksetup -getdnsservers Wi-Fi //查看对应的nameserver, 跟 scutil --dns 类似</span></div></pre></td></tr></table></figure>
<p>如上, 只要是你的nameserver工作正常那么DNS就肯定回复了</p>
<p>删掉所有DNS nameserver:</p>
<blockquote>
<p>One note to anyone wanting to remove the DNS, just write “empty” (without the quotes) instead of the DNS: <code>sudo networksetup -setdnsservers &lt;networkservice&gt; empty</code></p>
</blockquote>
<h2 id="networksetup用法"><a href="#networksetup用法" class="headerlink" title="networksetup用法"></a><a href="https://www.jianshu.com/p/c84e0f972353" target="_blank" rel="external">networksetup用法</a></h2><h3 id="查看设备和配置"><a href="#查看设备和配置" class="headerlink" title="查看设备和配置"></a>查看设备和配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash">networksetup -listallnetworkservices</span></div><div class="line">An asterisk (*) denotes that a network service is disabled.</div><div class="line">USB 10/100/1000 LAN</div><div class="line">Apple USB Ethernet Adapter</div><div class="line">Wi-Fi</div><div class="line">Bluetooth PAN</div><div class="line">Thunderbolt Bridge</div><div class="line">Thunderbolt Bridge 2</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">查看网卡配置</span></div><div class="line"><span class="meta">$</span><span class="bash">networksetup -getinfo <span class="string">"USB 10/100/1000 LAN"</span>                                   </span></div><div class="line">DHCP Configuration</div><div class="line">IP address: 30.25.25.195</div><div class="line">Subnet mask: 255.255.255.128</div><div class="line">Router: 30.25.25.254</div><div class="line">Client ID:</div><div class="line">IPv6 IP address: none</div><div class="line">IPv6 Router: none</div><div class="line">Ethernet Address: 44:67:52:02:16:d4</div><div class="line"><span class="meta"></span></div><div class="line">$<span class="bash">networksetup -listallhardwareports</span></div><div class="line">Hardware Port: USB 10/100/1000 LAN</div><div class="line">Device: en7</div><div class="line">Ethernet Address: 44:67:52:02:16:d4</div><div class="line"></div><div class="line">Hardware Port: Wi-Fi</div><div class="line">Device: en0</div><div class="line">Ethernet Address: 88:66:5a:10:e4:2b</div><div class="line"></div><div class="line">Hardware Port: Thunderbolt Bridge</div><div class="line">Device: bridge0</div><div class="line">Ethernet Address: 82:0a:d5:01:b4:00</div><div class="line"></div><div class="line">VLAN Configurations</div><div class="line">===================</div><div class="line"><span class="meta">$</span><span class="bash">networksetup -getinfo <span class="string">"Thunderbolt Bridge"</span></span></div><div class="line">DHCP Configuration</div><div class="line">Client ID:</div><div class="line">IPv6: Automatic</div><div class="line">IPv6 IP address: none</div><div class="line">IPv6 Router: none</div><div class="line"></div><div class="line">//查看wifi和热点</div><div class="line">networksetup -listpreferredwirelessnetworks en0 </div><div class="line">networksetup -getairportnetwork "en0"</div></pre></td></tr></table></figure>
<h3 id="dhcp、route、domain配置"><a href="#dhcp、route、domain配置" class="headerlink" title="dhcp、route、domain配置"></a>dhcp、route、domain配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">[-setmanual networkservice ip subnet router]</div><div class="line"></div><div class="line">[-setdhcp networkservice [clientid]]</div><div class="line"></div><div class="line">[-setbootp networkservice]</div><div class="line"></div><div class="line">[-setmanualwithdhcprouter networkservice ip]</div><div class="line"></div><div class="line">[-getadditionalroutes networkservice]</div><div class="line"></div><div class="line">[-setadditionalroutes networkservice [dest1 mask1 gate1] [dest2 mask2 gate2] ..</div><div class="line"></div><div class="line">. [destN maskN gateN]]</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">给网卡配置ip、网关</span></div><div class="line"><span class="meta">$</span><span class="bash"> networksetup -getinfo <span class="string">"Apple USB Ethernet Adapter"</span>                                DHCP Configuration</span></div><div class="line">Client ID:</div><div class="line">IPv6: Automatic</div><div class="line">IPv6 IP address: none</div><div class="line">IPv6 Router: none</div><div class="line">Ethernet Address: (null)</div><div class="line"><span class="meta">$</span><span class="bash">networksetup -setmanual <span class="string">"Apple USB Ethernet Adapter"</span> 192.168.100.100 255.255.255.0 192.168.100.1</span></div><div class="line"><span class="meta">$</span><span class="bash">networksetup -getinfo <span class="string">"Apple USB Ethernet Adapter"</span></span></div><div class="line">Manual Configuration</div><div class="line">IP address: 192.168.100.100</div><div class="line">Subnet mask: 255.255.255.0</div><div class="line">Router: 192.168.100.1</div><div class="line">IPv6: Automatic</div><div class="line">IPv6 IP address: none</div><div class="line">IPv6 Router: none</div><div class="line">Ethernet Address: (null)</div></pre></td></tr></table></figure>
<h3 id="代理配置"><a href="#代理配置" class="headerlink" title="代理配置"></a>代理配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">//ftp</div><div class="line">[-getftpproxy networkservice]</div><div class="line"></div><div class="line">[-setftpproxy networkservice domain portnumber authenticated username password]</div><div class="line"></div><div class="line">[-setftpproxystate networkservice on | off]</div></pre></td></tr></table></figure>
<p>网页</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[-getwebproxy networkservice]</div><div class="line">[-setwebproxy networkservice domain portnumber authenticated username password]</div><div class="line">[-setwebproxystate networkservice on | off]</div><div class="line"></div><div class="line">$networksetup -setwebproxy &quot;Built-in Ethernet&quot; proxy.company.com 80</div><div class="line">$networksetup -setwebproxy &quot;Built-In Ethernet&quot; proxy.company.com 80 On authusername authpassword</div></pre></td></tr></table></figure>
<p>Socks5 代理</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash">networksetup -setsocksfirewallproxy <span class="string">"USB 10/100/1000 LAN"</span> 127.0.0.1 13659</span></div><div class="line"><span class="meta">$</span><span class="bash">networksetup -getsocksfirewallproxy <span class="string">"USB 10/100/1000 LAN"</span></span></div><div class="line">Enabled: Yes</div><div class="line">Server: 127.0.0.1</div><div class="line">Port: 13659</div><div class="line">Authenticated Proxy Enabled: 0</div></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>mac同时连wifi(外网或者vpn)和有线(内网), 如果内网干扰了访问外部ip, 就检查路由表,调整顺序. 如果内网干扰了dns,可以通过scutil –dns查看dns顺序到系统配置里去掉不必要的resolver</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://gowa.club/macOS/macOS%E7%9A%84networksetup%E5%91%BD%E4%BB%A4%E6%9D%A5%E7%AE%A1%E7%90%86%E7%BD%91%E7%BB%9C.html" target="_blank" rel="external">macOS的networksetup命令来管理网络</a></p>
<p><a href="https://www.diamondtin.com/2009/reloading-pac-script-in-mac/" target="_blank" rel="external">在Mac下使用脚本重载proxy自动配置脚本（pac）</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/01/01/网络相关知识/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/01/网络相关知识/" itemprop="url">网络硬件相关知识</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-01T17:30:03+08:00">
                2021-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="网络硬件相关知识"><a href="#网络硬件相关知识" class="headerlink" title="网络硬件相关知识"></a>网络硬件相关知识</h1><p>程序员很难有机会接触到底层的一些东西,尤其是偏硬件部分,所以记录下</p>
<h2 id="光纤和普通网线的性能差异"><a href="#光纤和普通网线的性能差异" class="headerlink" title="光纤和普通网线的性能差异"></a>光纤和普通网线的性能差异</h2><p>以下都是在4.19内核的UOS，光纤交换机为锐捷，服务器是华为鲲鹏920的环境测试所得数据：</p>
<p><img src="/images/oss/553e1c5fff2dd04a668434f0da4f9d90.png" alt="image.png"></p>
<p>光纤稳定性好很多，平均rt是网线的三分之一，最大值则是网线的十分之一. 上述场景下光纤的带宽大约是网线的1.5倍. 实际光纤理论带宽一般都是万M, 网线是千M.</p>
<p>光纤接口：</p>
<p><img src="/images/oss/b67715de1b8e143f6fc17ba574bcf0c4.png" alt="image.png" style="zoom:60%;"></p>
<h3 id="单模光纤和多模光纤"><a href="#单模光纤和多模光纤" class="headerlink" title="单模光纤和多模光纤"></a>单模光纤和多模光纤</h3><p>下图绿色是多模光纤(Multi Mode Fiber),黄色是单模光纤(Single Mode Fiber), 因为光纤最好能和光模块匹配, 我们测试用的光模块都是多模的, 单模光纤线便宜,但是对应的光模块贵多了。</p>
<p>多模光模块工作波长为850nm，单模光模块工作波长为1310nm或1550nm, 从成本上来看，单模光模块所使用的设备多出多模光模块两倍，总体成本远高于多模光模块，但单模光模块的传输距离也要长于多模光模块，单模光模块最远传输距离为100km，多模光模块最远传输距离为2km。因单模光纤的传输原理为使光纤直射到中心，所以主要用作远距离数据传输，而多模光纤则为多通路传播模式，所以主要用于短距离数据传输。单模光模块适用于对距离和传输速率要求较高的大型网络中，多模光模块主要用于短途网路。</p>
<p><img src="/images/951413iMgBlog/image-20210831211315077.png" alt="image-20210831211315077"></p>
<p>ping结果比较:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">[aliyun@uos15 11:00 /home/aliyun]  以下88都是光口、89都是电口。</div><div class="line"><span class="meta">$</span><span class="bash">ping -c 10 10.88.88.16 //光纤</span></div><div class="line">PING 10.88.88.16 (10.88.88.16) 56(84) bytes of data.</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=1 ttl=64 time=0.058 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=2 ttl=64 time=0.049 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=3 ttl=64 time=0.053 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=4 ttl=64 time=0.040 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=5 ttl=64 time=0.053 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=6 ttl=64 time=0.043 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=7 ttl=64 time=0.038 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=8 ttl=64 time=0.050 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=9 ttl=64 time=0.043 ms</div><div class="line">64 bytes from 10.88.88.16: icmp_seq=10 ttl=64 time=0.064 ms</div><div class="line"></div><div class="line">--- 10.88.88.16 ping statistics ---</div><div class="line">10 packets transmitted, 10 received, 0% packet loss, time 159ms</div><div class="line">rtt min/avg/max/mdev = 0.038/0.049/0.064/0.008 ms</div><div class="line"></div><div class="line">[aliyun@uos15 11:01 /home/aliyun]</div><div class="line"><span class="meta">$</span><span class="bash">ping -c 10 10.88.89.16 //电口</span></div><div class="line">PING 10.88.89.16 (10.88.89.16) 56(84) bytes of data.</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=1 ttl=64 time=0.087 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=2 ttl=64 time=0.053 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=3 ttl=64 time=0.095 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=4 ttl=64 time=0.391 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=5 ttl=64 time=0.051 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=6 ttl=64 time=0.343 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=7 ttl=64 time=0.045 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=8 ttl=64 time=0.341 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=9 ttl=64 time=0.054 ms</div><div class="line">64 bytes from 10.88.89.16: icmp_seq=10 ttl=64 time=0.066 ms</div><div class="line"></div><div class="line">--- 10.88.89.16 ping statistics ---</div><div class="line">10 packets transmitted, 10 received, 0% packet loss, time 149ms</div><div class="line">rtt min/avg/max/mdev = 0.045/0.152/0.391/0.136 ms</div><div class="line"></div><div class="line">[aliyun@uos15 11:02 /u01]</div><div class="line"><span class="meta">$</span><span class="bash">scp uos.tar aliyun@10.88.89.16:/tmp/</span></div><div class="line">uos.tar                                  100% 3743MB 111.8MB/s   00:33    </div><div class="line"></div><div class="line">[aliyun@uos15 11:03 /u01]</div><div class="line"><span class="meta">$</span><span class="bash">scp uos.tar aliyun@10.88.88.16:/tmp/</span></div><div class="line">uos.tar                                   100% 3743MB 178.7MB/s   00:20    </div><div class="line"></div><div class="line">[aliyun@uos15 11:07 /u01]</div><div class="line"><span class="meta">$</span><span class="bash">sudo ping -f 10.88.89.16</span></div><div class="line">PING 10.88.89.16 (10.88.89.16) 56(84) bytes of data.</div><div class="line">--- 10.88.89.16 ping statistics ---</div><div class="line">284504 packets transmitted, 284504 received, 0% packet loss, time 702ms</div><div class="line">rtt min/avg/max/mdev = 0.019/0.040/1.014/0.013 ms, ipg/ewma 0.048/0.042 ms</div><div class="line"></div><div class="line">[aliyun@uos15 11:07 /u01]</div><div class="line"><span class="meta">$</span><span class="bash">sudo ping -f 10.88.88.16</span></div><div class="line">PING 10.88.88.16 (10.88.88.16) 56(84) bytes of data.</div><div class="line">--- 10.88.88.16 ping statistics ---</div><div class="line">299748 packets transmitted, 299748 received, 0% packet loss, time 242ms</div><div class="line">rtt min/avg/max/mdev = 0.012/0.016/0.406/0.006 ms, pipe 2, ipg/ewma 0.034/0.014 ms</div></pre></td></tr></table></figure>
<h2 id="多网卡bonding"><a href="#多网卡bonding" class="headerlink" title="多网卡bonding"></a>多网卡bonding</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">cat ifcfg-bond0</span></div><div class="line">DEVICE=bond0</div><div class="line">TYPE=Bond</div><div class="line">ONBOOT=yes</div><div class="line">BOOTPROTO=static</div><div class="line">IPADDR=10.176.7.11</div><div class="line">NETMASK=255.255.255.0</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">cat /etc/sysconfig/network-scripts/ifcfg-eth0</span></div><div class="line">DEVICE=eth0</div><div class="line">TYPE=Ethernet</div><div class="line">ONBOOT=yes</div><div class="line">BOOTPROTO=none</div><div class="line">MASTER=bond0</div><div class="line">SLAVE=yes</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">cat /etc/sysconfig/network-scripts/ifcfg-eth1</span></div><div class="line">DEVICE=eth1</div><div class="line">TYPE=Ethernet</div><div class="line">ONBOOT=yes</div><div class="line">BOOTPROTO=none</div><div class="line">MASTER=bond0</div><div class="line">SLAVE=yes</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash">cat /proc/net/bonding/bond0</span></div><div class="line"></div><div class="line">----加载内核bonding模块, mode=0 是RR负载均衡模式</div><div class="line"><span class="meta">#</span><span class="bash">cat /etc/modprobe.d/bonding.conf</span></div><div class="line"><span class="meta">#</span><span class="bash"> modprobe bonding</span></div><div class="line">alias bond0 bonding</div><div class="line">options bond0 mode=0 miimon=100  //这一行也可以放到bond0配置文件中,比如:BONDING_OPTS="miimon=100 mode=4 xmit_hash_policy=layer3+4" 用iperf 多连接测试bonding后的带宽发现，发送端能用上两张网卡，但是接收队列只能使用一张物理网卡</div></pre></td></tr></table></figure>
<p>网卡绑定mode共有七种(0~6) bond0、bond1、bond2、bond3、bond4、bond5、bond6</p>
<p>常用的有三种</p>
<ul>
<li><p>mode=0：平衡负载模式 <strong>(balance-rr)</strong>，有自动备援，两块物理网卡和bond网卡使用同一个mac地址，但需要”Switch”支援及设定。</p>
</li>
<li><p>mode=1：自动备援模式 <strong>(balance-backup)</strong>，其中一条线若断线，其他线路将会自动备援。</p>
</li>
<li><p>mode=6：平衡负载模式<strong>(balance-alb)</strong>，有自动备援，不必”Switch”支援及设定，两块网卡是使用不同的MAC地址</p>
</li>
<li><strong>Mode 4 (802.3ad)</strong>: This mode creates aggregation groups that share the same speed and duplex settings, and it requires a switch that supports an IEEE 802.3ad dynamic link. Mode 4 uses all interfaces in the active aggregation group. For example, you can aggregate three 1 GB per second (GBPS) ports into a 3 GBPS trunk port. This is equivalent to having one interface with 3 GBPS speed. It provides fault tolerance and load balancing.</li>
</ul>
<p>需要说明的是如果想做成mode 0的负载均衡,仅仅设置这里options bond0 miimon=100 mode=0是不够的,与网卡相连的交换机必须做特殊配置（这两个端口应该采取聚合方式），因为做bonding的这两块网卡是使用同一个MAC地址.从原理分析一下（bond运行在mode 0下）：</p>
<p>mode 0下bond所绑定的网卡的IP都被修改成相同的mac地址，如果这些网卡都被接在同一个交换机，那么交换机的arp表里这个mac地址对应的端口就有多 个，那么交换机接受到发往这个mac地址的包应该往哪个端口转发呢？正常情况下mac地址是全球唯一的，一个mac地址对应多个端口肯定使交换机迷惑了。所以 mode0下的bond如果连接到交换机，交换机这几个端口应该采取聚合方式（cisco称为 ethernetchannel，foundry称为portgroup），因为交换机做了聚合后，聚合下的几个端口也被捆绑成一个mac地址.我们的解决办法是，两个网卡接入不同的交换机即可。</p>
<p>mode6模式下无需配置交换机，因为做bonding的这两块网卡是使用不同的MAC地址。</p>
<p>mod=5，即：(balance-tlb) Adaptive transmit load balancing（适配器传输负载均衡）</p>
<p>特点：不需要任何特别的switch(交换机)支持的通道bonding。在每个slave上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的slave出故障了，另一个slave接管失败的slave的MAC地址。</p>
<p>该模式的必要条件：ethtool支持获取每个slave的速率.</p>
<p>案例，两块万兆bonding后带宽翻倍</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">#ethtool bond0</div><div class="line">Settings for bond0:</div><div class="line">	Supported ports: [ ]</div><div class="line">	Supported link modes:   Not reported</div><div class="line">	Supported pause frame use: No</div><div class="line">	Supports auto-negotiation: No</div><div class="line">	Advertised link modes:  Not reported</div><div class="line">	Advertised pause frame use: No</div><div class="line">	Advertised auto-negotiation: No</div><div class="line">	Speed: 20000Mb/s</div><div class="line">	Duplex: Full</div><div class="line">	Port: Other</div><div class="line">	PHYAD: 0</div><div class="line">	Transceiver: internal</div><div class="line">	Auto-negotiation: off</div><div class="line">	Link detected: yes</div><div class="line"></div><div class="line">[root@phy 16:55 /root]</div><div class="line">#cat /etc/sysconfig/network-scripts/ifcfg-bond0</div><div class="line">DEVICE=bond0</div><div class="line">BOOTPROTO=static</div><div class="line">TYPE=&quot;ethernet&quot;</div><div class="line">IPADDR=100.1.1.2</div><div class="line">NETMASK=255.255.255.192</div><div class="line">ONBOOT=yes</div><div class="line">USERCTL=no</div><div class="line">PEERDNS=no</div><div class="line">BONDING_OPTS=&quot;miimon=100 mode=4 xmit_hash_policy=layer3+4&quot;</div><div class="line"></div><div class="line">#cat /etc/modprobe.d/bonding.conf</div><div class="line">alias netdev-bond0 bonding</div><div class="line"></div><div class="line">#lsmod |grep bond</div><div class="line">bonding               137339  0</div><div class="line"></div><div class="line">#cat ifcfg-bond0</div><div class="line">DEVICE=bond0</div><div class="line">BOOTPROTO=static</div><div class="line">TYPE=&quot;ethernet&quot;</div><div class="line">IPADDR=100.81.131.221</div><div class="line">NETMASK=255.255.255.192</div><div class="line">ONBOOT=yes</div><div class="line">USERCTL=no</div><div class="line">PEERDNS=no</div><div class="line">BONDING_OPTS=&quot;miimon=100 mode=4 xmit_hash_policy=layer3+4&quot;</div><div class="line"></div><div class="line">#cat ifcfg-eth1</div><div class="line">DEVICE=eth1</div><div class="line">TYPE=&quot;Ethernet&quot;</div><div class="line">HWADDR=7C:D3:0A:E0:F7:81</div><div class="line">BOOTPROTO=none</div><div class="line">ONBOOT=yes</div><div class="line">MASTER=bond0</div><div class="line">SLAVE=yes</div><div class="line">PEERDNS=no</div><div class="line">RX_MAX=`ethtool -g &quot;$DEVICE&quot; | grep &apos;Pre-set&apos; -A1 | awk &apos;/RX/&#123;print $2&#125;&apos;`</div><div class="line">RX_CURRENT=`ethtool -g &quot;$DEVICE&quot; | grep &quot;Current&quot; -A1 | awk &apos;/RX/&#123;print $2&#125;&apos;`</div><div class="line">[[ &quot;$RX_CURRENT&quot; -lt &quot;$RX_MAX&quot; ]] &amp;&amp; ethtool -G &quot;$DEVICE&quot; rx &quot;$RX_MAX&quot;</div></pre></td></tr></table></figure>
<h2 id="网络中断和绑核"><a href="#网络中断和绑核" class="headerlink" title="网络中断和绑核"></a>网络中断和绑核</h2><p>网络包的描述符的内存（RingBuffer）跟着设备走（设备在哪个Die/Node上，就近分配内存）， 数据缓冲区（Data Buffer–存放网络包）内存跟着队列(中断)走， 如果队列绑定到DIE0， 而设备在die1上，这样在做DMA通信时， <a href="https://ata.alibaba-inc.com/articles/230545" target="_blank" rel="external">会产生跨die的交织访问</a>。</p>
<p>不管设备插在哪一个die上， 只要描述符申请的内存和数据缓冲区的内存都在同一个die上（需要修改驱动源代码–非常规），就能避免跨die内存交织， 性能能保持一致。</p>
<p><strong>irqbalance服务不会将中断进行跨node迁移，只会在同一numa node中进行优化。</strong></p>
<h3 id="ethtool"><a href="#ethtool" class="headerlink" title="ethtool"></a>ethtool</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">ethtool -i p1p1   //查询网卡bus-info</span></div><div class="line">driver: mlx5_core</div><div class="line">version: 5.0-0</div><div class="line">firmware-version: 14.27.1016 (MT_2420110004)</div><div class="line">expansion-rom-version:</div><div class="line">bus-info: 0000:21:00.0</div><div class="line">supports-statistics: yes</div><div class="line">supports-test: yes</div><div class="line">supports-eeprom-access: no</div><div class="line">supports-register-dump: no</div><div class="line">supports-priv-flags: yes</div><div class="line"></div><div class="line">//根据bus-info找到中断id</div><div class="line"><span class="meta">#</span><span class="bash">cat /proc/interrupts | grep 0000:21:00.0 | awk -F: <span class="string">'&#123;print $1&#125;'</span> | wc -l</span></div><div class="line"></div><div class="line">//修改网卡队列数</div><div class="line">sudo ethtool -L eth0  combined 2 （不能超过网卡最大队列数）</div><div class="line"></div><div class="line">然后检查是否生效了(不需要重启应用和机器，实时生效)：</div><div class="line">sudo ethtool -l eth0</div></pre></td></tr></table></figure>
<p>根据网卡bus-info可以找到对应的irq id</p>
<p>手工绑核脚本:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line"><span class="meta">#</span><span class="bash">irq_list=(`cat /proc/interrupts | grep enp131s0 | awk -F: <span class="string">'&#123;print $1&#125;'</span>`)</span></div><div class="line">intf=$1</div><div class="line">irq_list=(cat /proc/interrupts | grep `ethtool -i $intf |grep bus-info | awk  '&#123; print $2 &#125;'` | awk -F: '&#123;print $1&#125;')</div><div class="line">cpunum=48  # 修改为所在node的第一个Core</div><div class="line">for irq in $&#123;irq_list[@]&#125;</div><div class="line">do</div><div class="line">echo $cpunum &gt; /proc/irq/$irq/smp_affinity_list</div><div class="line">echo `cat /proc/irq/$irq/smp_affinity_list`</div><div class="line">(( cpunum+=1 ))</div><div class="line">done</div></pre></td></tr></table></figure>
<p>检查绑定结果: sh irqCheck.sh enp131s0</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 网卡名</span></div><div class="line">intf=$1</div><div class="line">irqID=`ethtool -i $intf |grep bus-info | awk  '&#123; print $2 &#125;'`</div><div class="line">log=irqSet-`date "+%Y%m%d-%H%M%S"`.log</div><div class="line"><span class="meta">#</span><span class="bash"> 可用的CPU数</span></div><div class="line">cpuNum=$(cat /proc/cpuinfo |grep processor -c)</div><div class="line"><span class="meta">#</span><span class="bash"> RX TX中断列表</span></div><div class="line">irqListRx=$(cat /proc/interrupts | grep $&#123;irqID&#125; | awk -F':' '&#123;print $1&#125;')</div><div class="line">irqListTx=$(cat /proc/interrupts | grep $&#123;irqID&#125; | awk -F':' '&#123;print $1&#125;')</div><div class="line"><span class="meta">#</span><span class="bash"> 绑定接收中断rx irq</span></div><div class="line">for irqRX in $&#123;irqListRx[@]&#125;</div><div class="line">do</div><div class="line">cat /proc/irq/$&#123;irqRX&#125;/smp_affinity_list</div><div class="line">done</div><div class="line"><span class="meta">#</span><span class="bash"> 绑定发送中断tx irq</span></div><div class="line">for irqTX in $&#123;irqListTx[@]&#125;</div><div class="line">do</div><div class="line">cat /proc/irq/$&#123;irqTX&#125;/smp_affinity_list</div><div class="line">done</div></pre></td></tr></table></figure>
<h3 id="中断联合（Coalescing）"><a href="#中断联合（Coalescing）" class="headerlink" title="中断联合（Coalescing）"></a>中断联合（Coalescing）</h3><p>中断联合可供我们推迟向内核通告新事件的操作，将多个事件汇总在一个中断中通知内核。该功能的当前设置可通过<code>ethtool -c</code>查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ ethtool -c eth0</div><div class="line">Coalesce parameters for eth0:</div><div class="line">...</div><div class="line">rx-usecs: 50</div><div class="line">tx-usecs: 50</div></pre></td></tr></table></figure>
<p>此处可以设置固定上限，对每内核每秒处理中断数量的最大值进行硬性限制，或针对特定硬件根据吞吐率<a href="https://community.mellanox.com/docs/DOC-2511" target="_blank" rel="external">自动调整中断速率</a>。</p>
<p>启用联合（使用<code>-C</code>）会增大延迟并可能导致丢包，因此对延迟敏感的工作可能需要避免这样做。另外，彻底禁用该功能可能导致中断受到节流限制，进而影响性能。</p>
<p>多次在nginx场景下测试未发现这个值对TPS有什么明显的改善</p>
<p><a href="https://blog.cloudflare.com/how-to-achieve-low-latency/" target="_blank" rel="external">How to achieve low latency with 10Gbps Ethernet</a> 中有提到 Linux 3.11 added support for the <a href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/open-source-kernel-enhancements-paper.pdf" target="_blank" rel="external"><code>SO_BUSY_POLL</code> socket option</a>.  也有类似的作用</p>
<h3 id="irqbalance"><a href="#irqbalance" class="headerlink" title="irqbalance"></a><a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/performance_tuning_guide/appe-red_hat_enterprise_linux-performance_tuning_guide-tool_reference" target="_blank" rel="external">irqbalance</a></h3><p><strong>irqbalance</strong> 是一个命令行工具，在处理器中分配硬件中断以提高系统性能。默认设置下在后台程序运行，但只可通过 <code>--oneshot</code> 选项运行一次。</p>
<p>以下参数可用于提高性能。</p>
<ul>
<li><p>–powerthresh</p>
<p>CPU 进入节能模式之前，设定可空闲的 CPU 数量。如果有大于阀值数量的 CPU 是大于一个标准的偏差，该差值低于平均软中断工作负载，以及没有 CPU 是大于一个标准偏差，且该偏差高出平均，并有多于一个的 irq 分配给它们，一个 CPU 将处于节能模式。在节能模式中，CPU 不是 irqbalance 的一部分，所以它在有必要时才会被唤醒。</p>
</li>
<li><p>–hintpolicy</p>
<p>决定如何解决 irq 内核关联提示。有效值为 <code>exact</code>（总是应用 irq 关联提示）、<code>subset</code> （irq 是平衡的，但分配的对象是关联提示的子集）、或者 <code>ignore</code>（irq 完全被忽略）。</p>
</li>
<li><p>–policyscript</p>
<p>通过设备路径、当作参数的irq号码以及 <strong>irqbalance</strong> 预期的零退出代码，定义脚本位置以执行每个中断请求。定义的脚本能指定零或多键值对来指导管理传递的 irq 中 <strong>irqbalance</strong>。下列是为效键值对：ban有效值为 <code>true</code>（从平衡中排除传递的 irq）或 <code>false</code>（该 irq 表现平衡）。balance_level允许用户重写传递的 irq 平衡度。默认设置下，平衡度基于拥有 irq 设备的 PCI 设备种类。有效值为 <code>none</code>、<code>package</code>、<code>cache</code>、或 <code>core</code>。numa_node允许用户重写视作为本地传送 irq 的 NUMA 节点。如果本地节点的信息没有限定于 ACPI ，则设备被视作与所有节点距离相等。有效值为识别特定 NUMA 节点的整数（从0开始）和 <code>-1</code>，规定 irq 应被视作与所有节点距离相等。</p>
</li>
<li><p>–banirq</p>
<p>将带有指定中断请求号码的中断添加至禁止中断的列表。</p>
</li>
</ul>
<p>也可以使用 <em><code>IRQBALANCE_BANNED_CPUS</code></em> 环境变量来指定被 <strong>irqbalance</strong> 忽略的 CPU 掩码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">//默认irqbalance绑定一个numa, -1指定多个numa</div><div class="line">echo -1 &gt;/sys/bus/pci/devices/`ethtool -i p1p1 |grep bus-info | awk  '&#123; print $2 &#125;'`/numa_node ; </div><div class="line">// 目录 /sys/class/net/p1p1/ link到了 /sys/bus/pci/devices/`ethtool -i p1p1 |grep bus-info | awk  '&#123; print $2 &#125;'` </div><div class="line"></div><div class="line">执行 irqbalance --debug 进行调试</div></pre></td></tr></table></figure>
<h4 id="irqbalance指定core"><a href="#irqbalance指定core" class="headerlink" title="irqbalance指定core"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/interrupt_and_process_binding" target="_blank" rel="external">irqbalance指定core</a></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">cat /etc/sysconfig/irqbalance</div><div class="line"># IRQBALANCE_BANNED_CPUS</div><div class="line"># 64 bit bitmask which allows you to indicate which cpu&apos;s should</div><div class="line"># be skipped when reblancing irqs. Cpu numbers which have their</div><div class="line"># corresponding bits set to one in this mask will not have any</div><div class="line"># irq&apos;s assigned to them on rebalance</div><div class="line">#绑定软中断到8-15core, 每位表示4core</div><div class="line">#IRQBALANCE_BANNED_CPUS=ffffffff,ffff00ff</div><div class="line">#绑定软中断到8-15core和第65core</div><div class="line">IRQBALANCE_BANNED_CPUS=ffffffff,fffffdff,ffffffff,ffff00ff</div><div class="line"></div><div class="line">#96core 鲲鹏920下绑前16core</div><div class="line">IRQBALANCE_BANNED_CPUS=ffffffff,ffffffff,ffff0000</div></pre></td></tr></table></figure>
<h4 id="irqbalance的流程"><a href="#irqbalance的流程" class="headerlink" title="irqbalance的流程"></a><a href="https://blog.csdn.net/whrszzc/article/details/50533866" target="_blank" rel="external">irqbalance的流程</a></h4><p>初始化的过程只是建立链表的过程，暂不描述，只考虑正常运行状态时的流程<br>-处理间隔是10s<br>-清除所有中断的负载值<br>-/proc/interrupts读取中断，并记录中断数<br>-/proc/stat读取每个cpu的负载，并依次计算每个层次每个节点的负载以及每个中断的负载<br>-通过平衡算法找出需要重新分配的中断<br>-把需要重新分配的中断加入到新的节点中<br>-配置smp_affinity使处理生效</p>
<p><strong>irqbalance服务不会将中断进行跨node迁移，只会在同一numa node中进行优化。</strong></p>
<h3 id="网卡软中断以及内存远近的测试结论"><a href="#网卡软中断以及内存远近的测试结论" class="headerlink" title="网卡软中断以及内存远近的测试结论"></a><a href="https://ata.alibaba-inc.com/articles/230545" target="_blank" rel="external">网卡软中断以及内存远近</a>的测试结论</h3><p>一般网卡中断会占用一些CPU，如果把网卡中断挪到其它node的core上，在鲲鹏920上测试（网卡插在node0上），业务跑在node3，网卡中断分别在node0和node3，QPS分别是：179000 VS 175000</p>
<p>如果将业务跑在node0上，网卡中断分别在node0和node1上得到的QPS分别是：204000 VS 212000 </p>
<p>以上测试的时候业务进程分配的内存全限制在node0上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#/root/numa-maps-summary.pl &lt;/proc/123853/numa_maps</div><div class="line">N0        :      5085548 ( 19.40 GB)</div><div class="line">N1        :         4479 (  0.02 GB)</div><div class="line">N2        :            1 (  0.00 GB)</div><div class="line">active    :            0 (  0.00 GB)</div><div class="line">anon      :      5085455 ( 19.40 GB)</div><div class="line">dirty     :      5085455 ( 19.40 GB)</div><div class="line">kernelpagesize_kB:         2176 (  0.01 GB)</div><div class="line">mapmax    :          348 (  0.00 GB)</div><div class="line">mapped    :         4626 (  0.02 GB)</div></pre></td></tr></table></figure>
<p>从以上测试数据可以看到在这个内存分布场景下，如果就近访问内存性能有20%以上的提升</p>
<p>一般默认申请的data buffer也都在设备所在的numa节点上<strong>， 如果将队列的中断绑定到其他cpu上， 那么</strong>队列申请的data buffer的节点也会跟着中断迁移。</p>
<h3 id="阿里云绑核脚本"><a href="#阿里云绑核脚本" class="headerlink" title="阿里云绑核脚本"></a>阿里云绑核脚本</h3><p>通常情况下，Linux的网卡中断是由一个CPU核心来处理的，当承担高流量的场景下，会出现一些诡异的情况（网卡尚未达到瓶颈，但是却出现丢包的情况）</p>
<p>这种时候，我们最好看下网卡中断是不是缺少调优。</p>
<p>优化3要点：网卡多队列+irq affinity亲缘性设置+关闭irqbalance (systemctl stop irqbalance)</p>
<p>目前阿里云官方提供的centos和ubuntu镜像里面，已经自带了优化脚本，内容如下:</p>
<p><strong>centos7的脚本路径在 /usr/sbin/ecs_mq_rps_rfs 具体内容如下：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line"><span class="meta">#</span><span class="bash"> This is the default setting of networking multiqueue and irq affinity</span></div><div class="line"><span class="meta">#</span><span class="bash"> 1. <span class="built_in">enable</span> multiqueue <span class="keyword">if</span> available</span></div><div class="line"><span class="meta">#</span><span class="bash"> 2. irq affinity optimization</span></div><div class="line"><span class="meta">#</span><span class="bash"> 3. stop irqbalance service</span></div><div class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> and check multiqueue</span></div><div class="line"></div><div class="line">function set_check_multiqueue()</div><div class="line">&#123;</div><div class="line">    eth=$1</div><div class="line">    log_file=$2</div><div class="line">    queue_num=$(ethtool -l $eth | grep -ia5 'pre-set' | grep -i combined | awk &#123;'print $2'&#125;)</div><div class="line">    if [ $queue_num -gt 1 ]; then</div><div class="line">        # set multiqueue</div><div class="line">        ethtool -L $eth combined $queue_num</div><div class="line">        # check multiqueue setting</div><div class="line">        cur_q_num=$(ethtool -l $eth | grep -iA5 current | grep -i combined | awk &#123;'print $2'&#125;)</div><div class="line">        if [ "X$queue_num" != "X$cur_q_num" ]; then</div><div class="line">            echo "Failed to set $eth queue size to $queue_num" &gt;&gt; $log_file</div><div class="line">            echo "after setting, pre-set queue num: $queue_num , current: $cur_q_num" &gt;&gt; $log_file</div><div class="line">            return 1</div><div class="line">        else</div><div class="line">            echo "OK. set $eth queue size to $queue_num" &gt;&gt; $log_file</div><div class="line">        fi</div><div class="line">    else</div><div class="line">        echo "only support $queue_num queue; no need to enable multiqueue on $eth" &gt;&gt; $log_file</div><div class="line">    fi</div><div class="line">&#125;</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"><span class="built_in">set</span> irq affinity</span></div><div class="line">function set_irq_smpaffinity()</div><div class="line">&#123;</div><div class="line">    log_file=$1</div><div class="line">    node_dir=/sys/devices/system/node</div><div class="line">    for i in $(ls -d $node_dir/node*); do</div><div class="line">        i=$&#123;i/*node/&#125;</div><div class="line">    done</div><div class="line">    </div><div class="line">    echo "max node :$i" &gt;&gt; $log_file</div><div class="line">    node_cpumax=$(cat /sys/devices/system/node/node$&#123;i&#125;/cpulist |awk -F- '&#123;print $NF&#125;')</div><div class="line">    irqs=($(cat /proc/interrupts |grep virtio |grep put | awk -F: '&#123;print $1&#125;'))</div><div class="line">    core=0</div><div class="line">    for irq in $&#123;irqs[@]&#125;;do</div><div class="line">        VEC=$core</div><div class="line">        if [ $VEC -ge 32 ];then</div><div class="line">            let "IDX = $VEC / 32"</div><div class="line">            MASK_FILL=""</div><div class="line">            MASK_ZERO="00000000"</div><div class="line">            for ((i=1; i&lt;=$IDX;i++))</div><div class="line">                do</div><div class="line">                    MASK_FILL="$&#123;MASK_FILL&#125;,$&#123;MASK_ZERO&#125;"</div><div class="line">                done</div><div class="line">            let "VEC -= 32 * $IDX"</div><div class="line">            MASK_TMP=$((1&lt;&lt;$VEC))</div><div class="line">            MASK=$(printf "%X%s" $MASK_TMP $MASK_FILL)</div><div class="line">        else</div><div class="line">            MASK_TMP=$((1&lt;&lt;$VEC))</div><div class="line">            MASK=$(printf "%X" $MASK_TMP)</div><div class="line">        fi</div><div class="line">        echo $MASK &gt; /proc/irq/$irq/smp_affinity</div><div class="line">        echo "mask:$MASK, irq:$irq" &gt;&gt; $log_file</div><div class="line">        core=$(((core+1)%(node_cpumax+1)))</div><div class="line">    done</div><div class="line">&#125;</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> stop irqbalance service</span></div><div class="line">function stop_irqblance()</div><div class="line">&#123;</div><div class="line">    log_file=$1</div><div class="line">    ret=0</div><div class="line">    if [ "X" != "X$(ps -ef | grep irqbalance | grep -v grep)" ]; then</div><div class="line">        if which systemctl;then</div><div class="line">            systemctl stop irqbalance</div><div class="line">        else</div><div class="line">            service irqbalance stop</div><div class="line">        fi</div><div class="line">        if [ $? -ne 0 ]; then</div><div class="line">            echo "Failed to stop irqbalance" &gt;&gt; $log_file</div><div class="line">            ret=1</div><div class="line">        fi</div><div class="line">    else</div><div class="line">       echo "OK. irqbalance stoped." &gt;&gt; $log_file</div><div class="line">    fi</div><div class="line">    return $ret</div><div class="line">&#125;</div><div class="line"><span class="meta">#</span><span class="bash"> main logic</span></div><div class="line">function main()</div><div class="line">&#123;</div><div class="line">    ecs_network_log=/var/log/ecs_network_optimization.log</div><div class="line">    ret_value=0</div><div class="line">    echo "running $0" &gt; $ecs_network_log</div><div class="line">    echo "========  ECS network setting starts $(date +'%Y-%m-%d %H:%M:%S') ========" &gt;&gt; $ecs_network_log</div><div class="line">    # we assume your NIC interface(s) is/are like eth*</div><div class="line">    eth_dirs=$(ls -d /sys/class/net/eth*)</div><div class="line">    if [ "X$eth_dirs" = "X" ]; then</div><div class="line">        echo "ERROR! can not find any ethX in /sys/class/net/ dir." &gt;&gt; $ecs_network_log</div><div class="line">        ret_value=1</div><div class="line">    fi</div><div class="line">    for i in $eth_dirs</div><div class="line">    do</div><div class="line">        cur_eth=$(basename $i)</div><div class="line">        echo "optimize network performance: current device $cur_eth" &gt;&gt; $ecs_network_log</div><div class="line">        # only optimize virtio_net device</div><div class="line">        driver=$(basename $(readlink $i/device/driver))</div><div class="line">        if ! echo $driver | grep -q virtio; then</div><div class="line">            echo "ignore device $cur_eth with driver $driver" &gt;&gt; $ecs_network_log</div><div class="line">            continue</div><div class="line">        fi</div><div class="line">        echo "set and check multiqueue on $cur_eth" &gt;&gt; $ecs_network_log</div><div class="line">        set_check_multiqueue $cur_eth $ecs_network_log</div><div class="line">        if [ $? -ne 0 ]; then</div><div class="line">            echo "Failed to set multiqueue on $cur_eth" &gt;&gt; $ecs_network_log</div><div class="line">            ret_value=1</div><div class="line">        fi</div><div class="line">    done</div><div class="line">    stop_irqblance  $ecs_network_log</div><div class="line">    set_irq_smpaffinity $ecs_network_log</div><div class="line">    echo "========  ECS network setting END $(date +'%Y-%m-%d %H:%M:%S')  ========" &gt;&gt; $ecs_network_log</div><div class="line">    return $ret_value</div><div class="line">&#125;</div><div class="line"><span class="meta"></span></div><div class="line"></div><div class="line">#<span class="bash"> program starts here</span></div><div class="line">main</div><div class="line">exit $?</div></pre></td></tr></table></figure>
<p>查询的rps绑定情况的脚本 get_rps.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></div><div class="line"><span class="meta">#</span><span class="bash"> 获取当前rps情况</span></div><div class="line">for i in $(ls /sys/class/net/eth0/queues/rx-*/rps_cpus); do </div><div class="line">  echo $i</div><div class="line">  cat $i</div><div class="line">done</div></pre></td></tr></table></figure>
<h2 id="RSS-和-RPS"><a href="#RSS-和-RPS" class="headerlink" title="RSS 和 RPS"></a>RSS 和 RPS</h2><ul>
<li>RSS：即receive side steering,利用网卡的多队列特性，将每个核分别跟网卡的一个首发队列绑定，以达到网卡硬中断和软中断均衡的负载在各个CPU上。他要求网卡必须要支持多队列特性。</li>
<li>RPS：receive packet steering，他把收到的packet依据一定的hash规则给hash到不同的CPU上去，以达到各个CPU负载均衡的目的。他只是把软中断做负载均衡，不去改变硬中断。因而对网卡没有任何要求。</li>
<li>RFS：receive flow steering，RFS需要依赖于RPS，他跟RPS不同的是不再简单的依据packet来做hash，而是根据flow的特性，即application在哪个核上来运行去做hash，从而使得有更好的数据局部性。</li>
</ul>
<p>RSS</p>
<p><img src="/images/951413iMgBlog/image-20221125190002856.png" alt="image-20221125190002856"></p>
<p>设置 RPS，首先内核要开启<strong>CONFIG_RPS</strong>编译选项，然后设置需要将中断分配到哪些CPU：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#cat /sys/class/net/eth3/queues/rx-[0-7]/rps_cpus</div><div class="line">#cat /sys/class/net/eth3/queues/tx-[0-7]/xps_cpus</div></pre></td></tr></table></figure>
<p>我们可以看到很多案例，使用这些特性后提醒了网络包的处理能力，从而提升QPS，降低RT。</p>
<p><img src="/images/951413iMgBlog/640-20221114141014713.png" alt="Image"></p>
<p>如上图所示，数据包在进入内核IP/TCP协议栈之前，经历了这些步骤：</p>
<ol>
<li>网口（NIC）收到packets</li>
<li>网口通过DMA（Direct memeory access）将数据写入到内存（RAM）中。</li>
<li>网口通过RSS（网卡多队列）将收到的数据包分发给某个rx队列，并触发该队列所绑定核上的CPU中断。</li>
<li>收到中断的核，调用该核所在的内核软中断线程（softirqd）进行后续处理。</li>
<li>softirqd负责将数据包从RAM中取到内核中。</li>
<li>如果开启了RPS，RPS会选择一个目标cpu核来处理该包，如果目标核非当前正在运行的核，则会触发目标核的IPI（处理器之间中断），并将数据包放在目标核的backlog队列中。</li>
<li>软中断线程将数据包（数据包可能来源于第5步、或第6步），通过gro(generic receive offload，如果开启的话)等处理后，送往IP协议栈，及之后的TCP/UDP等协议栈。</li>
</ol>
<h2 id="查看网卡和numa的关系"><a href="#查看网卡和numa的关系" class="headerlink" title="查看网卡和numa的关系"></a>查看网卡和numa的关系</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#yum install lshw -y</div><div class="line">#lshw -C network -short</div><div class="line">H/W path               Device          Class      Description</div><div class="line">=============================================================</div><div class="line">/0/100/0/9/0           eth0            network    MT27710 Family [ConnectX-4 Lx]</div><div class="line">/0/100/0/9/0.1         eth1            network    MT27710 Family [ConnectX-4 Lx]</div><div class="line">/1                     e41358fae4ee_h  network    Ethernet interface</div><div class="line">/2                     86b0637ef1e1_h  network    Ethernet interface</div><div class="line">/3                     a6706e785f53_h  network    Ethernet interface</div><div class="line">/4                     d351290e50a0_h  network    Ethernet interface</div><div class="line">/5                     1a9e5df98dd1_h  network    Ethernet interface</div><div class="line">/6                     766ec0dab599_h  network    Ethernet interface</div><div class="line">/7                     bond0.11        network    Ethernet interface</div><div class="line">/8                     ea004888c217_h  network    Ethernet interface</div></pre></td></tr></table></figure>
<p>以及：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">lscpu | grep -i numa</div><div class="line">numactl --hardware</div><div class="line">cat /proc/interrupts | egrep -i &quot;CPU|rx&quot;</div></pre></td></tr></table></figure>
<p><a href="https://ixnfo.com/en/how-to-find-out-on-which-numa-node-network-interfaces.html" target="_blank" rel="external">Check if the network interfaces are tied to Numa</a> (if -1 means not tied, if 0, then to numa0):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat /sys/class/net/eth0/device/numa_node</div></pre></td></tr></table></figure>
<p>You can see which NAMA the network card belongs to, for example, using lstopo:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div></pre></td><td class="code"><pre><div class="line">yum install hwloc -y</div><div class="line">lstopo</div><div class="line">lstopo --logical</div><div class="line">lstopo --logical --output-format png &gt; lstopo.png</div><div class="line"></div><div class="line">--</div><div class="line">[root@hygon3 10:58 /root]  //hygon 7280 CPU</div><div class="line">#lstopo --logical</div><div class="line">Machine (503GB total)               //总内存大小</div><div class="line">  NUMANode L#0 (P#0 252GB)          //socket0、numa0 的内存大小</div><div class="line">    Package L#0</div><div class="line">      L3 L#0 (8192KB)               //L3 cache，对应4个物理core，8个HT</div><div class="line">        L2 L#0 (512KB) + L1d L#0 (32KB) + L1i L#0 (64KB) + Core L#0 // L1/L2</div><div class="line">          PU L#0 (P#0)</div><div class="line">          PU L#1 (P#64)</div><div class="line">        L2 L#1 (512KB) + L1d L#1 (32KB) + L1i L#1 (64KB) + Core L#1</div><div class="line">          PU L#2 (P#1)</div><div class="line">          PU L#3 (P#65)</div><div class="line">        L2 L#2 (512KB) + L1d L#2 (32KB) + L1i L#2 (64KB) + Core L#2</div><div class="line">          PU L#4 (P#2)</div><div class="line">          PU L#5 (P#66)</div><div class="line">        L2 L#3 (512KB) + L1d L#3 (32KB) + L1i L#3 (64KB) + Core L#3</div><div class="line">          PU L#6 (P#3)</div><div class="line">          PU L#7 (P#67)</div><div class="line">      L3 L#1 (8192KB)</div><div class="line">      L3 L#2 (8192KB)</div><div class="line">      L3 L#3 (8192KB)</div><div class="line">      L3 L#4 (8192KB)</div><div class="line">      L3 L#5 (8192KB)</div><div class="line">      L3 L#6 (8192KB)</div><div class="line">      L3 L#7 (8192KB)</div><div class="line">    HostBridge L#0</div><div class="line">      PCIBridge</div><div class="line">        PCIBridge</div><div class="line">          PCI 1a03:2000</div><div class="line">            GPU L#0 &quot;controlD64&quot;</div><div class="line">            GPU L#1 &quot;card0&quot;</div><div class="line">      PCIBridge</div><div class="line">        PCI 1d94:7901</div><div class="line">          Block(Disk) L#2 &quot;sdm&quot;   //ssd系统盘，接在Node0上，绑核有优势</div><div class="line">    HostBridge L#4</div><div class="line">      PCIBridge</div><div class="line">        PCI 1000:0097</div><div class="line">      PCIBridge</div><div class="line">        PCI 1c5f:000d</div><div class="line">      PCIBridge</div><div class="line">        PCI 1c5f:000d</div><div class="line">    HostBridge L#8</div><div class="line">      PCIBridge</div><div class="line">        PCI 15b3:1015</div><div class="line">          Net L#3 &quot;p1p1&quot;      //万兆网卡接在Node0上</div><div class="line">        PCI 15b3:1015</div><div class="line">          Net L#4 &quot;p1p2&quot;</div><div class="line">    HostBridge L#10</div><div class="line">      PCIBridge</div><div class="line">        PCI 8086:1521</div><div class="line">          Net L#5 &quot;em1&quot;       //千兆网卡接在Node0上</div><div class="line">        PCI 8086:1521</div><div class="line">          Net L#6 &quot;em2&quot;</div><div class="line">  NUMANode L#1 (P#1 251GB)    //另外一个socket</div><div class="line">    Package L#1</div><div class="line">      L3 L#8 (8192KB)</div><div class="line">        L2 L#32 (512KB) + L1d L#32 (32KB) + L1i L#32 (64KB) + Core L#32</div><div class="line">        </div><div class="line">----------- FT2500 两路共128core</div><div class="line">#lstopo-no-graphics --logical</div><div class="line">Machine (503GB total)</div><div class="line">  Package L#0 + L3 L#0 (64MB)</div><div class="line">    NUMANode L#0 (P#0 31GB)</div><div class="line">      L2 L#0 (2048KB)         //4个物理core共享2M </div><div class="line">        L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)</div><div class="line">        L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#1)</div><div class="line">        L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#2)</div><div class="line">        L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#3)</div><div class="line">      L2 L#1 (2048KB)</div><div class="line">        L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#4)</div><div class="line">        L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#5)</div><div class="line">        L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#6)</div><div class="line">        L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#7)</div><div class="line">      HostBridge L#0</div><div class="line">        PCIBridge</div><div class="line">          PCIBridge</div><div class="line">            PCIBridge</div><div class="line">              PCI 1000:00ac</div><div class="line">                Block(Disk) L#0 &quot;sdh&quot;</div><div class="line">                Block(Disk) L#1 &quot;sdf&quot;  // 磁盘挂在Node0上</div><div class="line">            PCIBridge</div><div class="line">              PCI 8086:1521</div><div class="line">                Net L#13 &quot;eth0&quot;</div><div class="line">              PCI 8086:1521</div><div class="line">                Net L#14 &quot;eth1&quot;       //网卡挂在node0上</div><div class="line">        PCIBridge</div><div class="line">          PCIBridge</div><div class="line">            PCI 1a03:2000</div><div class="line">              GPU L#15 &quot;controlD64&quot;</div><div class="line">              GPU L#16 &quot;card0&quot;</div><div class="line">    NUMANode L#1 (P#1 31GB)</div><div class="line">    NUMANode L#2 (P#2 31GB)</div><div class="line">    NUMANode L#3 (P#3 31GB)</div><div class="line">    NUMANode L#4 (P#4 31GB)</div><div class="line">    NUMANode L#5 (P#5 31GB)</div><div class="line">    NUMANode L#6 (P#6 31GB)</div><div class="line">    NUMANode L#7 (P#7 31GB)</div><div class="line">      L2 L#14 (2048KB)</div><div class="line">        L1d L#56 (32KB) + L1i L#56 (32KB) + Core L#56 + PU L#56 (P#56)</div><div class="line">        L1d L#57 (32KB) + L1i L#57 (32KB) + Core L#57 + PU L#57 (P#57)</div><div class="line">        L1d L#58 (32KB) + L1i L#58 (32KB) + Core L#58 + PU L#58 (P#58)</div><div class="line">        L1d L#59 (32KB) + L1i L#59 (32KB) + Core L#59 + PU L#59 (P#59)</div><div class="line">      L2 L#15 (2048KB)</div><div class="line">        L1d L#60 (32KB) + L1i L#60 (32KB) + Core L#60 + PU L#60 (P#60)</div><div class="line">        L1d L#61 (32KB) + L1i L#61 (32KB) + Core L#61 + PU L#61 (P#61)</div><div class="line">        L1d L#62 (32KB) + L1i L#62 (32KB) + Core L#62 + PU L#62 (P#62)</div><div class="line">        L1d L#63 (32KB) + L1i L#63 (32KB) + Core L#63 + PU L#63 (P#63)</div><div class="line">  Package L#1 + L3 L#1 (64MB)   //socket2</div><div class="line">    NUMANode L#8 (P#8 31GB)</div><div class="line">      L2 L#16 (2048KB)</div><div class="line">        L1d L#64 (32KB) + L1i L#64 (32KB) + Core L#64 + PU L#64 (P#64)</div><div class="line">        L1d L#65 (32KB) + L1i L#65 (32KB) + Core L#65 + PU L#65 (P#65)</div><div class="line">        L1d L#66 (32KB) + L1i L#66 (32KB) + Core L#66 + PU L#66 (P#66)</div><div class="line">        L1d L#67 (32KB) + L1i L#67 (32KB) + Core L#67 + PU L#67 (P#67)</div><div class="line">      L2 L#17 (2048KB)</div><div class="line">        L1d L#68 (32KB) + L1i L#68 (32KB) + Core L#68 + PU L#68 (P#68)</div><div class="line">        L1d L#69 (32KB) + L1i L#69 (32KB) + Core L#69 + PU L#69 (P#69)</div><div class="line">        L1d L#70 (32KB) + L1i L#70 (32KB) + Core L#70 + PU L#70 (P#70)</div><div class="line">        L1d L#71 (32KB) + L1i L#71 (32KB) + Core L#71 + PU L#71 (P#71)</div><div class="line">      HostBridge L#7</div><div class="line">        PCIBridge</div><div class="line">          PCIBridge</div><div class="line">            PCIBridge</div><div class="line">              PCI 15b3:1015</div><div class="line">                Net L#17 &quot;eth2&quot;   //node8 上的网卡，eth2、eth3做了bonding</div><div class="line">              PCI 15b3:1015</div><div class="line">                Net L#18 &quot;eth3&quot;</div><div class="line">            PCIBridge</div><div class="line">              PCI 144d:a808</div><div class="line">            PCIBridge</div><div class="line">              PCI 144d:a808</div><div class="line">              </div><div class="line"> ---鲲鹏920 每路48core 2路共4node，网卡插在node0，磁盘插在node2</div><div class="line"> #lstopo-no-graphics</div><div class="line">Machine (755GB total)</div><div class="line">  Package L#0</div><div class="line">    NUMANode L#0 (P#0 188GB)</div><div class="line">      L3 L#0 (24MB)</div><div class="line">        L2 L#0 (512KB) + L1d L#0 (64KB) + L1i L#0 (64KB) + Core L#0 + PU L#0 (P#0)</div><div class="line">        L2 L#1 (512KB) + L1d L#1 (64KB) + L1i L#1 (64KB) + Core L#1 + PU L#1 (P#1)</div><div class="line">        L2 L#22 (512KB) + L1d L#22 (64KB) + L1i L#22 (64KB) + Core L#22 + PU L#22 (P#22)</div><div class="line">        L2 L#23 (512KB) + L1d L#23 (64KB) + L1i L#23 (64KB) + Core L#23 + PU L#23 (P#23)</div><div class="line">      HostBridge L#0</div><div class="line">        PCIBridge</div><div class="line">          PCI 15b3:1017</div><div class="line">            Net L#0 &quot;enp2s0f0&quot;</div><div class="line">          PCI 15b3:1017</div><div class="line">            Net L#1 &quot;eth1&quot;</div><div class="line">        PCIBridge</div><div class="line">          PCI 19e5:1711</div><div class="line">            GPU L#2 &quot;controlD64&quot;</div><div class="line">            GPU L#3 &quot;card0&quot;</div><div class="line">      HostBridge L#3</div><div class="line">        2 x &#123; PCI 19e5:a230 &#125;</div><div class="line">        PCI 19e5:a235</div><div class="line">          Block(Disk) L#4 &quot;sda&quot;</div><div class="line">      HostBridge L#4</div><div class="line">        PCIBridge</div><div class="line">          PCI 19e5:a222</div><div class="line">            Net L#5 &quot;enp125s0f0&quot;</div><div class="line">          PCI 19e5:a221</div><div class="line">            Net L#6 &quot;enp125s0f1&quot;</div><div class="line">          PCI 19e5:a222</div><div class="line">            Net L#7 &quot;enp125s0f2&quot;</div><div class="line">          PCI 19e5:a221</div><div class="line">            Net L#8 &quot;enp125s0f3&quot;</div><div class="line">    NUMANode L#1 (P#1 189GB) + L3 L#1 (24MB)</div><div class="line">      L2 L#24 (512KB) + L1d L#24 (64KB) + L1i L#24 (64KB) + Core L#24 + PU L#24 (P#24)</div><div class="line">  Package L#1</div><div class="line">    NUMANode L#2 (P#2 189GB)</div><div class="line">      L3 L#2 (24MB)</div><div class="line">        L2 L#48 (512KB) + L1d L#48 (64KB) + L1i L#48 (64KB) + Core L#48 + PU L#48 (P#48)</div><div class="line">      HostBridge L#6</div><div class="line">        PCIBridge</div><div class="line">          PCI 19e5:3714</div><div class="line">        PCIBridge</div><div class="line">          PCI 19e5:3714</div><div class="line">        PCIBridge</div><div class="line">          PCI 19e5:3714</div><div class="line">        PCIBridge</div><div class="line">          PCI 19e5:3714</div><div class="line">      HostBridge L#11</div><div class="line">        PCI 19e5:a230</div><div class="line">        PCI 19e5:a235</div><div class="line">        PCI 19e5:a230</div><div class="line">    NUMANode L#3 (P#3 189GB) + L3 L#3 (24MB)</div><div class="line">      L2 L#72 (512KB) + L1d L#72 (64KB) + L1i L#72 (64KB) + Core L#72 + PU L#72 (P#72)</div><div class="line">  Misc(MemoryModule)</div></pre></td></tr></table></figure>
<p>如果cpu core太多, interrupts 没法看的话，通过cut只看其中一部分core</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat /proc/interrupts | grep -i &apos;eth4\|CPU&apos; | cut -c -8,865-995,1425-</div></pre></td></tr></table></figure>
<h2 id="lspci"><a href="#lspci" class="headerlink" title="lspci"></a>lspci</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">lspci -s 21:00.0 -vvv</span></div><div class="line">21:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]</div><div class="line">	Subsystem: Mellanox Technologies ConnectX-4 Lx Stand-up dual-port 10GbE MCX4121A-XCAT</div><div class="line">	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- DisINTx+</div><div class="line">	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</div><div class="line">	Latency: 0, Cache Line Size: 64 bytes</div><div class="line">	Interrupt: pin A routed to IRQ 105</div><div class="line">	Region 0: Memory at 3249c000000 (64-bit, prefetchable) [size=32M]</div><div class="line">	Expansion ROM at db300000 [disabled] [size=1M]</div><div class="line">	Capabilities: [60] Express (v2) Endpoint, MSI 00</div><div class="line">		DevCap:	MaxPayload 512 bytes, PhantFunc 0, Latency L0s unlimited, L1 unlimited</div><div class="line">			ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset+ SlotPowerLimit 0.000W</div><div class="line">		DevCtl:	CorrErr+ NonFatalErr+ FatalErr+ UnsupReq-</div><div class="line">			RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+ FLReset-</div><div class="line">			MaxPayload 512 bytes, MaxReadReq 512 bytes</div><div class="line">		DevSta:	CorrErr+ NonFatalErr- FatalErr- UnsupReq+ AuxPwr- TransPend-</div><div class="line">		LnkCap:	Port #0, Speed 8GT/s, Width x8, ASPM not supported</div><div class="line">			ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+</div><div class="line">		LnkCtl:	ASPM Disabled; RCB 64 bytes Disabled- CommClk+</div><div class="line">			ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-</div><div class="line">		LnkSta:	Speed 8GT/s (ok), Width x8 (ok)</div><div class="line">			TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-</div><div class="line">		DevCap2: Completion Timeout: Range ABC, TimeoutDis+, LTR-, OBFF Not Supported</div><div class="line">			 AtomicOpsCap: 32bit- 64bit- 128bitCAS-</div><div class="line">		DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled</div><div class="line">			 AtomicOpsCtl: ReqEn-</div><div class="line">		LnkCtl2: Target Link Speed: 8GT/s, EnterCompliance- SpeedDis-</div><div class="line">			 Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-</div><div class="line">			 Compliance De-emphasis: -6dB</div><div class="line">		LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+, EqualizationPhase1+</div><div class="line">			 EqualizationPhase2+, EqualizationPhase3+, LinkEqualizationRequest-</div><div class="line">	Capabilities: [48] Vital Product Data</div><div class="line">		Product Name: CX4121A - ConnectX-4 LX SFP28</div><div class="line">		Read-only fields:</div><div class="line">			[PN] Part number: MCX4121A-XCAT</div><div class="line">			[EC] Engineering changes: AJ</div><div class="line">			[SN] Serial number: MT2031J09199</div><div class="line">			[V0] Vendor specific: PCIeGen3 x8</div><div class="line">			[RV] Reserved: checksum good, 0 byte(s) reserved</div><div class="line">		End</div><div class="line">	Capabilities: [9c] MSI-X: Enable+ Count=64 Masked-</div><div class="line">		Vector table: BAR=0 offset=00002000</div><div class="line">		PBA: BAR=0 offset=00003000</div><div class="line">	Capabilities: [c0] Vendor Specific Information: Len=18 &lt;?&gt;</div><div class="line">	Capabilities: [40] Power Management version 3</div><div class="line">		Flags: PMEClk- DSI- D1- D2- AuxCurrent=375mA PME(D0-,D1-,D2-,D3hot-,D3cold+)</div><div class="line">		Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-</div><div class="line">	Capabilities: [100 v1] Advanced Error Reporting</div><div class="line">		UESta:	DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-</div><div class="line">		UEMsk:	DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-</div><div class="line">		UESvrt:	DLP+ SDES- TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC+ UnsupReq- ACSViol-</div><div class="line">		CESta:	RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-</div><div class="line">		CEMsk:	RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+</div><div class="line">		AERCap:	First Error Pointer: 04, ECRCGenCap+ ECRCGenEn+ ECRCChkCap+ ECRCChkEn+</div><div class="line">			MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-</div><div class="line">		HeaderLog: 00000000 00000000 00000000 00000000</div><div class="line">	Capabilities: [150 v1] Alternative Routing-ID Interpretation (ARI)</div><div class="line">		ARICap:	MFVC- ACS-, Next Function: 1</div><div class="line">		ARICtl:	MFVC- ACS-, Function Group: 0</div><div class="line">	Capabilities: [180 v1] Single Root I/O Virtualization (SR-IOV)</div><div class="line">		IOVCap:	Migration-, Interrupt Message Number: 000</div><div class="line">		IOVCtl:	Enable- Migration- Interrupt- MSE- ARIHierarchy+</div><div class="line">		IOVSta:	Migration-</div><div class="line">		Initial VFs: 8, Total VFs: 8, Number of VFs: 0, Function Dependency Link: 00</div><div class="line">		VF offset: 2, stride: 1, Device ID: 1016</div><div class="line">		Supported Page Size: 000007ff, System Page Size: 00000001</div><div class="line">		Region 0: Memory at 000003249e800000 (64-bit, prefetchable)</div><div class="line">		VF Migration: offset: 00000000, BIR: 0</div><div class="line">	Capabilities: [1c0 v1] Secondary PCI Express &lt;?&gt;</div><div class="line">	Capabilities: [230 v1] Access Control Services</div><div class="line">		ACSCap:	SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-</div><div class="line">		ACSCtl:	SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-</div><div class="line">	Kernel driver in use: mlx5_core</div><div class="line">	Kernel modules: mlx5_core</div></pre></td></tr></table></figure>
<p>如果有多个高速设备争夺带宽（例如将高速网络连接到高速存储），那么 PCIe 也可能成为瓶颈，因此可能需要从物理上将 PCIe 设备划分给不同 CPU，以获得最高吞吐率。</p>
<p><img src="/images/951413iMgBlog/d718966f8f1fa1375e4437842fc759c2.png" alt="img"></p>
<p>数据来源：<a href="https://en.wikipedia.org/wiki/PCI_Express#History_and_revisions" target="_blank" rel="external"> https://en.wikipedia.org/wiki/PCI_Express#History_and_revisions</a></p>
<p>Intel 认为，有时候 PCIe 电源管理（ASPM）可能导致延迟提高，因进而导致丢包率增高。因此也可以为内核命令行参数添加<code>pcie_aspm=off</code>将其禁用。</p>
<h2 id="Default-路由持久化"><a href="#Default-路由持久化" class="headerlink" title="Default 路由持久化"></a>Default 路由持久化</h2><p>通过 ip route 可以添加默认路由，但是reboot就丢失了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">route add default dev bond0</div></pre></td></tr></table></figure>
<p>如果要持久化，在centos下可以创建 /etc/sysconfig/network-scripts/route-bond0 文件，内容如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">default dev bond0    ---默认路由，后面的可以省略</div><div class="line">10.0.0.0/8 via 11.158.239.247 dev bond0</div><div class="line">11.0.0.0/8 via 11.158.239.247 dev bond0</div><div class="line">30.0.0.0/8 via 11.158.239.247 dev bond0</div><div class="line">172.16.0.0/12 via 11.158.239.247 dev bond0</div><div class="line">192.168.0.0/16 via 11.158.239.247 dev bond0</div><div class="line">100.64.0.0/10 via 11.158.239.247 dev bond0</div><div class="line">33.0.0.0/8 via 11.158.239.247 dev bond0</div></pre></td></tr></table></figure>
<p>或者用sed在文件第一行添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sed -i &apos;/default /d&apos;  /etc/sysconfig/network-scripts/route-bond0   //先删除默认路由（如果有）</div><div class="line">sed -i &apos;1 i\default dev bond0&apos; /etc/sysconfig/network-scripts/route-bond0   //添加</div></pre></td></tr></table></figure>
<p>Centos 7的话需要在 /etc/sysconfig/network 中添加创建默认路由的命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># cat /etc/sysconfig/network</div><div class="line"># Created by anaconda</div><div class="line">ip route add default dev eth0</div></pre></td></tr></table></figure>
<h2 id="内核态启动并加载网卡的逻辑"><a href="#内核态启动并加载网卡的逻辑" class="headerlink" title="内核态启动并加载网卡的逻辑"></a>内核态启动并加载网卡的逻辑</h2><ol>
<li><p>运行Linux的机器在BIOS阶段之后，机器的boot loader根据我们预先定义好的配置文件，将intrd和linux kernel加载到内存。这个包含initrd和linux kernel的配置文件通常在/boot分区（从grub.conf中读取参数）</p>
</li>
<li><p>内核启动，运行当前根目录下面的init进程，init进程再运行其他必要的进程，其中跟网卡PCI设备相关的一个进程，就是udevd进程，udevd负责根据内核pci scan的pci设备，从initrd这个临时的根文件系统中加载内核模块，对于网卡来说，就是网卡驱动。(对应systemd-udevd 服务)</p>
</li>
<li><p>udevd，根据内核pci device scan出来的pci device，通过netlink消息机制通知udevd加载相应的内核驱动，其中，网卡驱动就是在这个阶段加载，如果initrd临时文件系统里面有这个网卡的驱动文件。通常upstream到linux内核的驱动，比如ixgbe，或者和内核一起编译的网卡驱动，会默认包含在initrd文件系统中。这些跟内核一起ship的网卡驱动会在这个阶段加载</p>
</li>
<li><p>udevd除了负责网卡驱动加载之外，还要负责为网卡命名。udevd在为网卡命名的时候，会首先check “/etc/udev/rules.d/“下的rule，如果hit到相应的rule，就会通过rule里面指定的binary为网卡命名。如果/etc/udev/rules.d/没有命名成功网卡，那么udevd会使用/usr/lib/udev/rule.d下面的rule，为网卡重命名。其中rule的文件经常以数字开头，数字越小，表示改rule的优先级越高。intrd init不会初始化network服务，所以/etc/sysconfig/network-scripts下面的诸如bond0，route的配置都不会生效。（内核启动先是 intrd init，然后执行一次真正的init）</p>
</li>
<li><p>在完成网卡driver load和name命名之后，initrd里面的init进程，会重启其他用户态进程，如udevd等，并且重新mount真正的根文件系统，启动network service。</p>
</li>
<li><p>重启udevd，会触发一次kernel的rescan device。这样第三方安装的网卡driver，由于其driver模块没有在initrd里面，会在这个阶段由udevd触发加载。同时，也会根据“/etc/udev/rules.d/”和“/usr/lib/udev/rule.d”的rule，重命名网卡设备。–用户态修改网卡名字的机会</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kernel: ixgbe 0000:3b:00.1 eth1: renamed from enp59s0f1</div><div class="line">kernel: i40e 0000:88:00.0 eth7: renamed from enp136s0</div></pre></td></tr></table></figure>
</li>
<li><p>同时network service 会启动，进而遍历etc/sysconfig/network-scripts下面的脚本，我们配置的bond0， 默认路由，通常会在这个阶段运行，创建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">kernel: bond0: Enslaving eth0 as a backup interface with a down link</div><div class="line">kernel: ixgbe 0000:3b:00.0 eth0: detected SFP+: 5</div><div class="line">kernel: power_meter ACPI000D:00: Found ACPI power meter.</div><div class="line">kernel: power_meter ACPI000D:00: Ignoring unsafe software power cap!</div><div class="line">kernel: ixgbe 0000:3b:00.1: registered PHC device on eth1</div><div class="line">kernel: ixgbe 0000:3b:00.0 eth0: NIC Link is Up 10 Gbps, Flow Control: RX/TX</div><div class="line">kernel: bond0: Enslaving eth1 as a backup interface with a down link</div><div class="line">kernel: bond0: Warning: No 802.3ad response from the link partner for any adapters in the bond</div><div class="line">kernel: bond0: link status definitely up for interface eth0, 10000 Mbps full duplex</div><div class="line">kernel: bond0: first active interface up!</div></pre></td></tr></table></figure>
</li>
</ol>
<p>由于我们系统的初始化有两个阶段，udevd会运行两次，所以内核态网卡driver的加载，网卡命名也有两次机会。</p>
<p>第一次网卡driver的加载和命名是在initrd运行阶段，这个阶段由于initrd文件系统比较小，只包括和kernel一起ship的内核module，所以这个阶段只能加载initrd里面有的内核模块。网卡的重命名也只能重命名加载了驱动的网卡。</p>
<p>第二个网卡driver的加载和命名，是在真正根文件系统加载后，内核再一次pci scan，这个时候，由于真的根文件系统包含了所有的driver，第一个阶段无法probe的网卡会在这个阶段probe，重命名也会在这个阶段进行。</p>
<blockquote>
<p>内核默认命名规则有一定的局限性，往往不一定准确对应网卡接口的物理顺序，而且每次启动只根据内核发现网卡的顺序进行命名，因此并不固定；所以目前一般情况下会在用户态启用其他的方式去更改网卡名称，原则就是在内核命名ethx后将其在根据用户态的规则rename为其他的名字，这种规则往往是根据网卡的Mac地址以及其他能够唯一代表一块网卡的参数去命名，因此会一一对应；</p>
</blockquote>
<p>内核自带的网卡驱动在initrd中的内核模块中。对于第三方网卡，我们通常通过rpm包的方式安装。这种第三方安装的rpm，通常不会在initrd里面，只存在disk上。这样这种内核模块就只会在第二次udevd启动的时候被加载。</p>
<p>不论第一次重命名还是第二次重命名，其都遵循一样的逻辑，也就是先check /etc/udev/rules.d/的rule，然后check /usr/lib/udev/rule.d中的rule，其中rule的优先级etc下最高，然后是usr下面。并且，rule的文件名中的数字表示该rule在同一文件夹中的优先级，数字越低，优先级越高。</p>
<p>network.service 根据network-script里面的脚本创建bond0，下发路由。这个过程和网卡重命名是同步进行，一般网卡重命名会超级快，单极端情况下重命名可能在network.service后会导致创建bond0失败（依赖网卡名来bonding），这里会依赖network.service retry机制来反复尝试确保network服务能启动成功</p>
<p>要想解决网卡加载慢的问题，可以考虑把安装后的网卡集成到initrd中。Linux系统提供的dracut可以做到这一点，我们只需要在安装完第三方网卡驱动后，执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dracut --forace</div><div class="line"></div><div class="line">查看</div><div class="line">udevadm info -q all -a /dev/nvme0</div></pre></td></tr></table></figure>
<p>就可以解决这个问题，该命令会根据最新的内存中的module，重新下刷initrd。</p>
<p>其实在多数第三方网卡的rpm spec或者makefile里面通常也会加入这种强制重刷的逻辑，确保内核驱动在initrd里面，从而加快网卡驱动的加载。</p>
<h3 id="用户态命名网卡流程"><a href="#用户态命名网卡流程" class="headerlink" title="用户态命名网卡流程"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-understanding_the_device_renaming_procedure" target="_blank" rel="external">用户态命名网卡流程</a></h3><p><a href="https://blog.csdn.net/biaotai/article/details/120710966" target="_blank" rel="external">CentOS 7提供了在网络接口中使用一致且可预期的网络设备命名方法， 目前默认使用的是net.ifnames规则</a>。The device name procedure in detail is as follows:</p>
<ol>
<li>A rule in <code>/usr/lib/udev/rules.d/60-net.rules</code> instructs the <strong>udev</strong> helper utility, <strong>/lib/udev/rename_device</strong>, to look into all <code>/etc/sysconfig/network-scripts/ifcfg-*suffix*</code> files. If it finds an <code>ifcfg</code> file with a <code>HWADDR</code> entry matching the MAC address of an interface it renames the interface to the name given in the <code>ifcfg</code> file by the <code>DEVICE</code> directive.（根据提前定义好的ifcfg-网卡名来命名网卡–依赖mac匹配，如果网卡的ifconfig文件中未加入HWADDR，则rename脚本并不会根据配置文件去重命名网卡）</li>
<li>A rule in <code>/usr/lib/udev/rules.d/71-biosdevname.rules</code> instructs <strong>biosdevname</strong> to rename the interface according to its naming policy, provided that it was not renamed in a previous step, <strong>biosdevname</strong> is installed, and <code>biosdevname=0</code> was not given as a kernel command on the boot command line.</li>
<li>A rule in <code>/lib/udev/rules.d/75-net-description.rules</code> instructs <strong>udev</strong> to fill in the internal <strong>udev</strong> device property values ID_NET_NAME_ONBOARD, ID_NET_NAME_SLOT, ID_NET_NAME_PATH, ID_NET_NAME_MAC by examining the network interface device. Note, that some device properties might be undefined.</li>
<li>A rule in <code>/usr/lib/udev/rules.d/80-net-name-slot.rules</code> instructs <strong>udev</strong> to rename the interface, provided that it was not renamed in step 1 or 2, and the kernel parameter <code>net.ifnames=0</code> was not given, according to the following priority: ID_NET_NAME_ONBOARD, ID_NET_NAME_SLOT, ID_NET_NAME_PATH. It falls through to the next in the list, if one is unset. If none of these are set, then the interface will not be renamed.</li>
</ol>
<p>Steps 3 and 4 are implementing the naming schemes 1, 2, 3, and optionally 4, described in <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/ch-Consistent_Network_Device_Naming#sec-Naming_Schemes_Hierarchy" target="_blank" rel="external">Section 11.1, “Naming Schemes Hierarchy”</a>. Step 2 is explained in more detail in <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-Consistent_Network_Device_Naming_Using_biosdevname" target="_blank" rel="external">Section 11.6, “Consistent Network Device Naming Using biosdevname”</a>.</p>
<p>以上重命名简要概述就是对于CentOS系统，一般有下面几个rule在/usr/lib/udev/rule.d来重命名网卡：</p>
<ol>
<li>/usr/lib/udev/rules.d/60-net.rules 文件中的规则会让 udev 帮助工具/lib/udev/rename_device 查看所有 /etc/sysconfig/network-scripts/ifcfg-* 文件。如果发现包含 HWADDR 条目的 ifcfg 文件与某个接口的 MAC 地址匹配，它会将该接口重命名为ifcfg 文件中由 DEVICE 指令给出的名称。rename条件：如果网卡的ifconfig文件中未加入HWADDR，则rename脚本并不会根据配置文件去重命名网卡；</li>
<li>/usr/lib/udev/rules.d/71-biosdevname.rules 中的规则让 biosdevname 根据其命名策略重命名该接口，即在上一步中没有重命名该接口、安装biosdevname、且在 boot 命令行中将biosdevname=0 作为内核命令给出。（bisodevname规则，从CentOS 7 开始默认不使用，所以该条规则在不配置的情况下失效，直接去执行3；默认在cmdline中bisodevname=0，如果需要启用，则需要设置bisodevname=1）</li>
<li>/lib/udev/rules.d/75-net-description.rules 中的规则让 udev 通过检查网络接口设备，填写内部 udev 设备属性值 ID_NET_NAME_ONBOARD、ID_NET_NAME_SLOT、ID_NET_NAME_PATH、ID_NET_NAME_MAC。注：有些设备属性可能处于未定义状态。 –没有修改网卡名，只是取到了命名需要的一些属性值。查看：udevadm info -p /sys/class/net/enp125s0f0</li>
<li>/usr/lib/udev/rules.d/80-net-name-slot.rules 中的规则让 udev 重命名该接口，优先顺序如下：ID_NET_NAME_ONBOARD、ID_NET_NAME_SLOT、ID_NET_NAME_PATH。并提供如下信息：没有在步骤 1 或 2 中重命名该接口，同时未给出内核参数 net.ifnames=0。如果一个参数未设定，则会按列表的顺序设定下一个。如果没有设定任何参数，则不会重命名该接口 —- 目前主流CentOS流都是这个命名方式</li>
<li>network service起来后会遍历/etc/sysconfig/network-scripts下的脚本，配置bond0、默认路由、其它网卡等</li>
</ol>
<p>其中60 rule会调用rename_device根据ifcfg-xxx脚本来命名，rule 71调用biosdevname来命名网卡。以上规则数字越小优先级越高，高优先级生效后跳过低优先级</p>
<p>总的来说网卡命名规则：grub启动参数 -&gt; /etc/udev/rules.d/的rule -&gt; /usr/lib/udev/rule.d</p>
<p><a href="https://opensource.com/article/22/8/network-configuration-files?continueFlag=0e1c90589c7c691ec44a1aaecdcba76e" target="_blank" rel="external">参考</a>：</p>
<p>The following is an excerpt from Chapter 11 of the RHEL 7 “Networking Guide”:</p>
<ul>
<li>Scheme 1: Names incorporating Firmware or BIOS provided index numbers for on-board devices (example: eno1), are applied if that information from the firmware or BIOS is applicable and available, else falling back to scheme 2.</li>
<li>Scheme 2: Names incorporating Firmware or BIOS provided PCI Express hotplug slot index numbers (example: ens1) are applied if that information from the firmware or BIOS is applicable and available, else falling back to scheme 3.</li>
<li>Scheme 3: Names incorporating physical location of the connector of the hardware (example: enp2s0), are applied if applicable, else falling directly back to scheme 5 in all other cases.</li>
<li>Scheme 4: Names incorporating interface’s MAC address (example: enx78e7d1ea46da), is not used by default, but is available if the user chooses.</li>
<li>Scheme 5: The traditional unpredictable kernel naming scheme, is used if all other methods fail (example: eth0).</li>
</ul>
<h3 id="网卡命名"><a href="#网卡命名" class="headerlink" title="网卡命名"></a>网卡命名</h3><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-consistent_network_device_naming_using_biosdevname" target="_blank" rel="external">默认安装网卡所在位置来命名（enp131s0 等）</a>，按位置命名实例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">//name example  ---默认方式，按照 /usr/lib/udev/rules.d/80-net-name-slot.rules 来命名</div><div class="line">enp4s10f1                        pci 0000:04:0a.1</div><div class="line">| | |  |                                |  |  | |</div><div class="line">| | |  |                   domain &lt;- 0000  |  | |</div><div class="line">| | |  |                                   |  | |</div><div class="line">en| |  |  --&gt; ethernet                     |  | |</div><div class="line">  | |  |                                   |  | |</div><div class="line">  p4|  |  --&gt; prefix/bus number (4)   &lt;-- 04  | |</div><div class="line">    |  |                                      | |</div><div class="line">    s10|  --&gt; slot/device number (10) &lt;--    10 |</div><div class="line">       |                                        |</div><div class="line">       f1 --&gt; function number (1)     &lt;--       1</div></pre></td></tr></table></figure>
<p>可以<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-disabling_consistent_network_device_naming" target="_blank" rel="external">关掉这种按位置命名的方式</a>，在grub参数中添加： net.ifnames=0 biosdevname=0，关闭后默认命名方式是eth**，开启biosdevname=1后，默认网卡命名方式是p1p1/p1p2(麒麟默认开启；alios默认关闭，然后以eth来命名)</p>
<blockquote>
<p>You have two options (<a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Disabling_Consistent_Network_Device_Naming.html" target="_blank" rel="external">as described in the new RHEL 7 Networking Guide</a>) to disable the <a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/ch-Consistent_Network_Device_Naming.html#sec-Understanding_the_Predictable_Network_Interface_Device_Names" target="_blank" rel="external">new naming scheme</a>:</p>
<ul>
<li>Run once: <code>ln -s /dev/null /etc/udev/rules.d/80-net-name-slot.rules</code></li>
</ul>
<p>or</p>
<ul>
<li>Run once: <code>echo &#39;GRUB_CMDLINE_LINUX=&quot;net.ifnames=0&quot;&#39; &gt;&gt;/etc/default/grub</code></li>
</ul>
<p>Note that the <strong>biosdevname</strong> package is not installed by default, so unless it gets installed, you don’t need to add <code>biosdevname=0</code> as a kernel argument.</p>
</blockquote>
<p>也可以添加命名规则在 /etc/udev/rules.d/ 下(这种优先级挺高），比如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cat /etc/udev/rules.d/70-persistent-net.rules</div><div class="line"># PCI device 21:00.0 (ixgbe)</div><div class="line">SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;address&#125;==&quot;d4:5d:64:bb:06:32&quot;, PROGRAM=&quot;/lib/udev/rename_device&quot;, ATTR&#123;type&#125;==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth0&quot;</div><div class="line"># PCI device 0x8086:0x105e (e1000e)</div><div class="line">SUBSYSTEM==&quot;net&quot;, ACTION==&quot;add&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;address&#125;==&quot;b8:59:9f:2d:48:2b&quot;, PROGRAM=&quot;/lib/udev/rename_device&quot;, ATTR&#123;type&#125;==&quot;1&quot;, KERNEL==&quot;eth*&quot;, NAME=&quot;eth1&quot;</div></pre></td></tr></table></figure>
<p>但是以上规则在麒麟下没有生效</p>
<p>网卡重命名方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/sbin/ip link set eth1 name eth123</div></pre></td></tr></table></figure>
<h2 id="校验"><a href="#校验" class="headerlink" title="校验"></a><a href="https://www.kernel.org/doc/html/latest/networking/checksum-offloads.html" target="_blank" rel="external">校验</a></h2><p>比如如下结构下因为通过xdp redirect来联通veth0、veth1，两边能ping通，但是TCP、UDP 都不通</p>
<p><img src="/images/951413iMgBlog/image-20220614171759153.png" alt="image-20220614171759153"></p>
<p>正常走bridge ping/tcp/udp是不会有问题的, 这也是docker下常见用法</p>
<p><img src="/images/951413iMgBlog/image-20220614173416775.png" alt="image-20220614173416775"></p>
<p>当前主流的网卡（包括虚拟网卡，如veth/tap）都支持一个叫做RX/TX Checksum Offload（RX和TX对应接收和发送两个方向）的特性，用于将传输层协议的校验和计算卸载到网卡硬件中（IP头的检验和会被操作系统用软件方式正确计算）。对于经过启用该功能的网卡的报文，操作系统不会对该报文进行校验和的计算，从而减少对系统CPU资源的占用。</p>
<p><img src="/images/951413iMgBlog/1613803162582-ce09e9cc-36e4-4805-b968-98d8dd601f52-5197629.png" alt="1613803162582-ce09e9cc-36e4-4805-b968-98d8dd601f52"></p>
<p>对于没有挂载XDP程序的且开启Checksum Offload功能的Veth设备，在接收到数据包时，会将<code>ip_summed</code>置为<code>CHECKSUM_UNNECESSARY</code>，因此上层L4协议栈在收到该数据包的时候不会再检查校验和，即使是数据包的校验和不正确也会正常被处理。但是若我们在veth设备上挂载了XDP程序，XDP程序运行时将网卡接收队列中的数据转换为结构<code>struct xdp_buff</code>时会丢失掉<code>ip_summed</code>信息，这就导致数据包被L4协议栈接收后由于校验和错误而被丢弃。</p>
<p>如上图因为veth挂载了XDP程序，导致包没有校验信息而丢掉，如果在同样环境下ping是可以通的，因为ping包提前计算好了正确的校验和</p>
<p><img src="/images/951413iMgBlog/1613982679299-d6832373-6a5f-4b54-9440-fd16606b8341.png" alt="img"></p>
<p>这种丢包可以通过 <code>/proc/net/snmp</code> 看到</p>
<p><img src="/images/951413iMgBlog/1613814089563-fb1de2e7-46d4-4bb7-9162-356e39c19a4c.png" alt="img"></p>
<p>通过命令<code>ethtool -K &lt;nic-name&gt; tx off</code>工具关闭Checksum Offload特性，强行让操作系统用软件方式计算校验和。</p>
<h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><p><a href="https://www.cyberciti.biz/faq/linux-log-suspicious-martian-packets-un-routable-source-addresses/" target="_blank" rel="external">网卡日志打开</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.ipv4.conf.all.log_martians=1 //所有网卡</div><div class="line">sysctl -w net.ipv4.conf.p1p1.log_martians=1 //特定网卡</div><div class="line"></div><div class="line">/proc/sys/net/ipv4/conf/eth0.9/log_martians</div></pre></td></tr></table></figure>
<p>/var/log/messages中：</p>
<p>messages-20120101:Dec 31 09:25:45 nixcraft-router kernel: martian source 74.xx.47.yy from 10.13.106.25, on dev eth1</p>
<h2 id="修改mac地址"><a href="#修改mac地址" class="headerlink" title="修改mac地址"></a>修改mac地址</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo ip link set dev eth1 down</div><div class="line">sudo ip link set dev eth1 address e8:61:1f:33:c5:fd</div><div class="line">sudo ip link set dev eth1 up</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.modb.pro/db/29135" target="_blank" rel="external">高斯在鲲鹏下跑TPCC的优化</a></p>
<p><a href="https://www.cyberciti.biz/faq/linux-log-suspicious-martian-packets-un-routable-source-addresses/" target="_blank" rel="external">https://www.cyberciti.biz/faq/linux-log-suspicious-martian-packets-un-routable-source-addresses/</a></p>
<p><a href="https://www.hikunpeng.com/document/detail/zh/kunpenggrf/tuningtip/kunpengtuning_12_0025.html" target="_blank" rel="external">鲲鹏性能优化十板斧</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2021/01/01/如何用1分钱建站来秒杀搜狐新浪等三大门户网站/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/01/01/如何用1分钱建站来秒杀搜狐新浪等三大门户网站/" itemprop="url">如何用1分钱建站速度秒杀三大门户网站站</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-01-01T12:30:03+08:00">
                2021-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何用1分钱建站速度秒杀三大门户网站"><a href="#如何用1分钱建站速度秒杀三大门户网站" class="headerlink" title="如何用1分钱建站速度秒杀三大门户网站"></a>如何用1分钱建站速度秒杀三大门户网站</h1><p>如何快速又便宜地建立一个高质量的网站呢(高质量指的是访问速度快)，还能够双站热备(国内国外热备两份内容)，整个开支大概一分钱吧</p>
<p>核心就是用阿里云的OSS来提供高速的访问。</p>
<h2 id="先看访问速度"><a href="#先看访问速度" class="headerlink" title="先看访问速度"></a>先看访问速度</h2><p>同样是访问下面三个网站首页:</p>
<p>OSS托管，页面大小 96.6MB、242个GET，耗时2.21秒加载，价格不到1分钱</p>
<p>搜狐首页，页面大小16.6MB、555个GET，耗时3.6秒</p>
<p>新浪首页， 页面大小17.8MB、404个GET，耗时9.63秒</p>
<h3 id="OSS托管的网站"><a href="#OSS托管的网站" class="headerlink" title="OSS托管的网站"></a>OSS托管的网站</h3><p>用OSS托管的网站加载速度，96MB页面（很大了）2.21秒加载完毕</p>
<p><img src="/images/951413iMgBlog/image-20210702140950863.png" alt="image-20210702140950863"></p>
<h3 id="访问搜狐首页"><a href="#访问搜狐首页" class="headerlink" title="访问搜狐首页"></a>访问搜狐首页</h3><p><img src="/images/951413iMgBlog/image-20210702141336301.png" alt="image-20210702141336301"></p>
<h3 id="访问新浪首页"><a href="#访问新浪首页" class="headerlink" title="访问新浪首页"></a>访问新浪首页</h3><p><img src="/images/951413iMgBlog/image-20210702142610162.png" alt="image-20210702142610162"></p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>OSS快的原因是：小网站并发不高，服务器、带宽资源充足，还不用花钱，没有机器、带宽维护成本以及人员成本</p>
<p>有专业的阿里云工程师负责运维，给的是最好的服务器、最大的带宽（你用的少就不用花钱，带宽资源费用超级便宜）</p>
<p>到底有多便宜呢？1.6万次GET请求才1分钱，0.52GB流量才0.25元，计价金额单位震惊我了</p>
<p><img src="/images/951413iMgBlog/image-20210702163910295.png" alt="image-20210702163910295"></p>
<h2 id="OSS托管网站方案"><a href="#OSS托管网站方案" class="headerlink" title="OSS托管网站方案"></a>OSS托管网站方案</h2><p>将所有内容静态化，然后上传到OSS就可以了</p>
<p>发布操作步骤：</p>
<ul>
<li>markdown编辑器中编写要发布的页面</li>
<li>用hexo静态化全站（将markdown转换成html页面）</li>
<li>git commit到github或者ossutil 同步到aliyun oss中</li>
</ul>
<p>比如下面就是我的网站发布脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#静态化网站，并同步到github，多活</div><div class="line">hexo g -d</div><div class="line"></div><div class="line">#sync all pages to oss</div><div class="line">ossutil --config-file=~/src/script/mac/.ossutilconfig sync ./public/ oss://mysite/ -u --output-dir=/tmp/</div></pre></td></tr></table></figure>
<p>实际我的网站通过github和OSS都能访问到，内容完全一样，github免费，但是多图页面速度太慢, 比如我一个页面几十个图，github加载偶尔失败, 但是我把图片放到了OSS，因为OSS超级快这样github加载也变得超级快了。</p>
<blockquote>
<p>hexo是一个node实现的网站生成工具</p>
</blockquote>
<p><a href="https://help.aliyun.com/document_detail/31872.html" target="_blank" rel="external">oss 托管网站介绍</a></p>
<p>感叹一下，个人建站现在真的是又便宜又方便，只是域名实名制恶心了点，那就干脆不要域名了。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/12/25/一个有意思的问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/25/一个有意思的问题/" itemprop="url">一个有意思的问题</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-25T17:30:03+08:00">
                2020-12-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一个有意思的问题"><a href="#一个有意思的问题" class="headerlink" title="一个有意思的问题"></a>一个有意思的问题</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$mysql -N -h127.0.0.1 -e &quot;select id from sbtest1 limit 1&quot;</div><div class="line">+--------+</div><div class="line">| 100024 |</div><div class="line">+--------+</div><div class="line"></div><div class="line">$mysql -N -h127.0.0.1 -e &quot;select id from sbtest1 limit 1&quot; | cat</div><div class="line">100024</div><div class="line"></div><div class="line">$mysql -t -N -h127.0.0.1 -e &quot;select id from sbtest1 limit 1&quot; | cat</div><div class="line">+--------+</div><div class="line">| 100024 |</div><div class="line">+--------+</div></pre></td></tr></table></figure>
<p>如上第一和第二个语句，<strong>为什么mysql client的输出重定向后就没有ascii制表符了呢</strong>？ </p>
<p>语句三加上 -t后再经过管道，也有制表符了。</p>
<p><a href="https://stackoverflow.com/questions/15640287/change-output-format-for-mysql-command-line-results-to-csv/17910254" target="_blank" rel="external">stackoverflow上也有很多人有同样的疑问</a>，不过不但没有给出第三行的解法，更没有人讲清楚这个里面的原理。所以接下来我们来分析下这是为什么</p>
<blockquote>
<p>-N 去掉表头</p>
<p>-B batch 模式，用tab键替换分隔符</p>
</blockquote>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>strace看看第一个语句：</p>
<p><img src="/images/oss/086f6cd952d2b91eae7eda6d576765f8.png" alt="image.png"></p>
<p>再对比下第二个语句的strace：</p>
<p><img src="/images/oss/984bcce23ff8766b52fdede8ff3eadec.png" alt="image.png"></p>
<p>从上面两个strace比较来看，似乎mysql client能检测到要输出到命名管道（S_IFIFO ）还是character device（S_IFCHR），如果是命名管道的话就不要输出制表符了，如果是character device那么就输出ascii制表符。</p>
<p><a href="https://linux.die.net/man/2/fstat64" target="_blank" rel="external">fstats里面对不同输出目标的说明</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">printf(&quot;File type:                &quot;);</div><div class="line">   switch (sb.st_mode &amp; S_IFMT) &#123;</div><div class="line">    case S_IFBLK:  printf(&quot;block device\n&quot;);            break;</div><div class="line">    case S_IFCHR:  printf(&quot;character device\n&quot;);        break;</div><div class="line">    case S_IFDIR:  printf(&quot;directory\n&quot;);               break;</div><div class="line">    case S_IFIFO:  printf(&quot;FIFO/pipe\n&quot;);               break;</div><div class="line">    case S_IFLNK:  printf(&quot;symlink\n&quot;);                 break;</div><div class="line">    case S_IFREG:  printf(&quot;regular file\n&quot;);            break;</div><div class="line">    case S_IFSOCK: printf(&quot;socket\n&quot;);                  break;</div><div class="line">    default:       printf(&quot;unknown?\n&quot;);                break;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>第4行和第6行两个类型就是导致mysql client选择了不同的输出内容</p>
<h2 id="误解"><a href="#误解" class="headerlink" title="误解"></a>误解</h2><p>所以这个问题不是： </p>
<blockquote>
<p><strong>为什么mysql client的输出重定向后就没有ascii制表符了呢</strong>？</p>
</blockquote>
<p>而是：</p>
<blockquote>
<p><strong>mysql client 可以检测到不同的输出目标然后输出不同的内容吗？</strong> 管道或者重定向是一个应用能感知的输出目标吗？</p>
</blockquote>
<p>误解：觉得管道写在后面，mysql client不应该知道后面是管道，mysql client输出内容到stdout，然后os将stdout的内容重定向给管道。</p>
<p>实际上mysql是可以检测（detect）输出目标的，如果是管道类的非交互输出那么没必要徒增一些制表符；如果是交互式界面那么就输出一些制表符好看一些。</p>
<p>要是想想在Unix下一切皆文件就更好理解了，输出到管道这个管道也是个文件，所以mysql client是可以感知各种输出文件的属性的。</p>
<p>背后的<a href="https://stackoverflow.com/questions/1312922/detect-if-stdin-is-a-terminal-or-pipe" target="_blank" rel="external">实现</a>大概是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#include &lt;stdio.h&gt;</div><div class="line">#include &lt;io.h&gt;</div><div class="line">...    </div><div class="line">if (isatty(fileno(stdout)))</div><div class="line">    printf( &quot;stdout is a terminal\n&quot; );      // 输出制表符</div><div class="line">else</div><div class="line">    printf( &quot;stdout is a file or a pipe\n&quot;); // 不输出制表符</div></pre></td></tr></table></figure>
<p><a href="https://linux.die.net/man/3/isatty" target="_blank" rel="external">isatty的解释</a></p>
<p>结论就是 mysql client根据输出目标的不同（stdout、重定向）输出不同的内容，不过这种做法对用户体感上不是太好。</p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>Linux管道居然不是按顺序，而是并发执行的：<a href="https://unix.stackexchange.com/questions/37508/in-what-order-do-piped-commands-run" target="_blank" rel="external">https://unix.stackexchange.com/questions/37508/in-what-order-do-piped-commands-run</a>  掉坑里了，并发问题就多了，实际测试也发现跑几千次 ps |grep 会出现，ps看不到后面的grep进程</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.pyrosoft.co.uk/blog/2014/09/08/how-to-stop-mysql-ascii-tables-column-separators-from-being-lost-when-redirecting-bash-output/" target="_blank" rel="external">https://www.pyrosoft.co.uk/blog/2014/09/08/how-to-stop-mysql-ascii-tables-column-separators-from-being-lost-when-redirecting-bash-output/</a></p>
<p><a href="https://www.oreilly.com/library/view/mysql-cookbook/0596001452/ch01s22.html" target="_blank" rel="external">https://www.oreilly.com/library/view/mysql-cookbook/0596001452/ch01s22.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/30/一台机器上最多能创建多少个TCP连接/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/30/一台机器上最多能创建多少个TCP连接/" itemprop="url">到底一台服务器上最多能创建多少个TCP连接</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-30T10:30:03+08:00">
                2020-11-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="到底一台服务器上最多能创建多少个TCP连接"><a href="#到底一台服务器上最多能创建多少个TCP连接" class="headerlink" title="到底一台服务器上最多能创建多少个TCP连接"></a>到底一台服务器上最多能创建多少个TCP连接</h1><blockquote>
<p>经常听到有同学说一台机器最多能创建65535个TCP连接，这其实是错误的理解，为什么会有这个错误的理解呢？</p>
</blockquote>
<h2 id="port-range"><a href="#port-range" class="headerlink" title="port range"></a>port range</h2><p>我们都知道linux下本地随机端口范围由参数控制，也就是listen、connect时候如果没有指定本地端口，那么就从下面的port range 中随机取一个可用的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># cat /proc/sys/net/ipv4/ip_local_port_range </div><div class="line">2000	65535</div></pre></td></tr></table></figure>
<p>port range的上限是65535，所以也经常看到这个<strong>误解</strong>：一台机器上最多能创建65535个TCP连接</p>
<h2 id="到底一台机器上最多能创建多少个TCP连接"><a href="#到底一台机器上最多能创建多少个TCP连接" class="headerlink" title="到底一台机器上最多能创建多少个TCP连接"></a>到底一台机器上最多能创建多少个TCP连接</h2><p>先说<strong>结论</strong>：在内存、文件句柄足够的话可以创建的连接是<strong>没有限制</strong>的（每个TCP连接至少要消耗一个文件句柄）。</p>
<p>那么/proc/sys/net/ipv4/ip_local_port_range指定的端口范围到底是什么意思呢？</p>
<p>核心规则：<strong>一个TCP连接只要保证四元组(src-ip src-port dest-ip dest-port)唯一就可以了，而不是要求src port唯一</strong></p>
<p>后面所讲都遵循这个规则，所以在心里反复默念：<strong>四元组唯一</strong> 五个大字，就能分析出来到底能创建多少TCP连接了。</p>
<p>比如如下这个机器上的TCP连接实际状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -ant |grep 18089</div><div class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:22         ESTABLISHED</div><div class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      ESTABLISHED</div><div class="line">tcp        0      0 192.168.0.79:18089      192.168.0.79:22         TIME_WAIT </div><div class="line">tcp        0      0 192.168.1.79:22         192.168.1.79:18089      ESTABLISHED</div><div class="line">tcp        0      0 192.168.1.79:18080      192.168.1.79:18089      ESTABLISHED</div></pre></td></tr></table></figure>
<p>从前三行可以清楚地看到18089被用了三次，第一第二行src-ip、dest-ip也是重复的，但是dest port不一样，第三行的src-port还是18089，但是src-ip变了。他们的四元组均不相同。</p>
<p>所以一台机器能创建的TCP连接是没有限制的，而ip_local_port_range是指没有bind的时候OS随机分配端口的范围，但是分配到的端口要同时满足五元组唯一，这样 ip_local_port_range 限制的是连同一个目标（dest-ip和dest-port一样）的port的数量（请忽略本地多网卡的情况，因为dest-ip为以后route只会选用一个本地ip）。</p>
<p><strong>那么为什么大家有这样的误解呢？</strong>我总结了下，大概是以下两个原因让大家误解了：</p>
<ul>
<li>如果是listen服务，那么肯定端口不能重复使用，这样就跟我们的误解对应上了，一个服务器上最多能监听65535个端口。比如nginx监听了80端口，那么tomcat就没法再监听80端口了，这里的80端口只能监听一次。</li>
<li>另外如果我们要连的server只有一个，比如：1.1.1.1:80 ，同时本机只有一个ip的话，那么这个时候即使直接调connect 也只能创建出65535个连接，因为四元组中的三个是固定的了。</li>
</ul>
<p>我们在创建连接前，经常会先调bind，bind后可以调listen当做服务端监听，也可以直接调connect当做client来连服务端。</p>
<p>bind(ip,port=0) 的时候是让系统绑定到某个网卡和自动分配的端口，此时系统没有办法确定接下来这个socket是要去connect还是listen. 如果是listen的话，那么肯定是不能出现端口冲突的，如果是connect的话，只要满足4元组唯一即可。在这种情况下，系统只能尽可能满足更强的要求，就是先要求端口不能冲突，即使之后去connect的时候四元组是唯一的。</p>
<p>比如 Nginx HaProxy envoy这些软件在创建到upstream的连接时，都会用 bind(0) 的方式, 导致到不同目的的连接无法复用同一个src port，这样后端的最大连接数受限于local_port_range。 </p>
<blockquote>
<p>Linux 4.2后的内核增加了IP_BIND_ADDRESS_NO_PORT 这个socket option来解决这个问题，将src port的选择延后到connect的时候</p>
<p><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=90c337da1524863838658078ec34241f45d8394d" target="_blank" rel="external">IP_BIND_ADDRESS_NO_PORT (since Linux 4.2)</a><br>              Inform the kernel to not reserve an ephemeral port when using bind(2) with a port number of 0.  The port will later be automatically chosen at connect(2) time, in a way that allows sharing a source port as long as the 4-tuple is unique.</p>
</blockquote>
<p>但如果我只是个client端，只需要连接server建立连接，也就不需要bind，直接调connect就可以了，这个时候只要保证四元组唯一就行。</p>
<p>bind()的时候内核是还不知道四元组的，只知道src_ip、src_port，所以这个时候单网卡下src_port是没法重复的，但是connect()的时候已经知道了四元组的全部信息，所以只要保证四元组唯一就可以了，那么这里的src_port完全是可以重复使用的。</p>
<p><img src="/images/951413iMgBlog/640-20220224103024676.png" alt="Image"></p>
<p><strong>是不是加上了 SO_REUSEADDR、SO_REUSEPORT 就能重用端口了呢？</strong></p>
<h2 id="TCP-SO-REUSEADDR"><a href="#TCP-SO-REUSEADDR" class="headerlink" title="TCP SO_REUSEADDR"></a>TCP SO_REUSEADDR</h2><p>文档描述：</p>
<blockquote>
<p>SO_REUSEADDR      Indicates that the rules used in validating addresses supplied in a bind(2) call should allow reuse of local addresses.  For AF_INET sockets this means that a socket may bind, except when there is an active listening socket bound to the address. When the listening socket is bound to INADDR_ANY with a specific port then it is not possible to bind to this port for any local address.  Argument is an integer boolean flag.</p>
</blockquote>
<p>从这段文档中我们可以知道三个事：</p>
<ol>
<li>使用这个参数后，bind操作是可以重复使用local address的，注意，这里说的是local address，即ip加端口组成的本地地址，也就是两个本地地址，如果有任意ip或端口部分不一样，它们本身就是可以共存的，不需要使用这个参数。</li>
<li>当local address被一个处于listen状态的socket使用时，加上该参数也不能重用这个地址。</li>
<li>当处于listen状态的socket监听的本地地址的ip部分是INADDR_ANY，即表示监听本地的所有ip，即使使用这个参数，也不能再bind包含这个端口的任意本地地址，这个和 2 中描述的其实是一样的。</li>
</ol>
<p>==SO_REUSEADDR 可以用本地相同的(sip, sport) 去连connect 远程的不同的（dip、dport）//而 SO_REUSEPORT主要是解决Server端的port重用==</p>
<p><a href="https://mp.weixin.qq.com/s/YWzuKBK3TMclejeN2ziAvQ" target="_blank" rel="external">SO_REUSEADDR 还可以重用TIME_WAIT状态的port</a>, 在程序崩溃后之前的TCP连接会进入到TIME_WAIT状态，需要一段时间才能释放，如果立即重启就会抛出<u>Address Already in use</u>的错误导致启动失败。这时候可以通过在调用bind函数之前设置SO_REUSEADDR来解决。</p>
<blockquote>
<p>What exactly does SO_REUSEADDR do?</p>
<p>This socket option tells the kernel that even if this port is busy (in the TIME_WAIT state), go ahead and reuse it anyway. If it is busy, but with another state, you will still get an address already in use error. It is useful if your server has been shut down, and then restarted right away while sockets are still active on its port. You should be aware that if any unexpected data comes in, it may confuse your server, but while this is possible, it is not likely.</p>
<p>It has been pointed out that “A socket is a 5 tuple (proto, local addr, local port, remote addr, remote port). SO_REUSEADDR just says that you can reuse local addresses. The 5 tuple still must be unique!” This is true, and this is why it is very unlikely that unexpected data will ever be seen by your server. The danger is that such a 5 tuple is still floating around on the net, and while it is bouncing around, a new connection from the same client, on the same system, happens to get the same remote port. </p>
</blockquote>
<p>By setting <code>SO_REUSEADDR</code> user informs the kernel of an intention to share the bound port with anyone else, but only if it doesn’t cause a conflict on the protocol layer. There are at least three situations when this flag is useful:</p>
<ol>
<li>Normally after binding to a port and stopping a server it’s neccesary to wait for a socket to time out before another server can bind to the same port. With <code>SO_REUSEADDR</code> set it’s possible to rebind immediately, even if the socket is in a <code>TIME_WAIT</code> state.</li>
<li>When one server binds to <code>INADDR_ANY</code>, say <code>0.0.0.0:1234</code>, it’s impossible to have another server binding to a specific address like <code>192.168.1.21:1234</code>. With <code>SO_REUSEADDR</code> flag this behaviour is allowed.</li>
<li>When using the bind before connect trick only a single connection can use a single outgoing source port. With this flag, it’s possible for many connections to reuse the same source port, given that they connect to different destination addresses.</li>
</ol>
<h2 id="TCP-SO-REUSEPORT"><a href="#TCP-SO-REUSEPORT" class="headerlink" title="TCP SO_REUSEPORT"></a>TCP SO_REUSEPORT</h2><p>SO_REUSEPORT主要用来解决惊群、性能等问题。通过多个进程、线程来监听同一端口，进来的连接通过内核来hash分发做到负载均衡，避免惊群。</p>
<blockquote>
<p>SO_REUSEPORT is also useful for eliminating the try-10-times-to-bind hack in ftpd’s data connection setup routine.  Without SO_REUSEPORT, only one ftpd thread can bind to TCP (lhost, lport, INADDR_ANY, 0) in preparation for connecting back to the client.  Under conditions of heavy load, there are more threads colliding here than the try-10-times hack can accomodate.  With SO_REUSEPORT, things  work nicely and the hack becomes unnecessary.</p>
</blockquote>
<p>SO_REUSEPORT使用场景：linux kernel 3.9 引入了最新的SO_REUSEPORT选项，使得多进程或者多线程创建多个绑定同一个ip:port的监听socket，提高服务器的接收链接的并发能力,程序的扩展性更好；此时需要设置SO_REUSEPORT（<strong>注意所有进程都要设置才生效</strong>）。</p>
<p>setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&amp;reuse , sizeof(int));</p>
<p>目的：每一个进程有一个独立的监听socket，并且bind相同的ip:port，独立的listen()和accept()；提高接收连接的能力。（例如nginx多进程同时监听同一个ip:port）</p>
<blockquote>
<p>(a) on Linux SO_REUSEPORT is meant to be used <em>purely</em> for load balancing multiple incoming UDP packets or incoming TCP connection requests across multiple sockets belonging to the same app.  ie. it’s a work around for machines with a lot of cpus, handling heavy load, where a single listening socket becomes a bottleneck because of cross-thread contention on the in-kernel socket lock (and state).</p>
<p>(b) set IP_BIND_ADDRESS_NO_PORT socket option for tcp sockets before binding to a specific source ip<br>with port 0 if you’re going to use the socket for connect() rather then listen() this allows the kernel<br>to delay allocating the source port until connect() time at which point it is much cheaper</p>
</blockquote>
<h2 id="The-Ephemeral-Port-Range"><a href="#The-Ephemeral-Port-Range" class="headerlink" title="The Ephemeral Port Range"></a><a href="http://www.ncftp.com/ncftpd/doc/misc/ephemeral_ports.html" target="_blank" rel="external">The Ephemeral Port Range</a></h2><p>Ephemeral Port Range就是我们前面所说的Port Range（/proc/sys/net/ipv4/ip_local_port_range）</p>
<blockquote>
<p>A TCP/IPv4 connection consists of two endpoints, and each endpoint consists of an IP address and a port number. Therefore, when a client user connects to a server computer, an established connection can be thought of as the 4-tuple of (server IP, server port, client IP, client port).</p>
<p>Usually three of the four are readily known – client machine uses its own IP address and when connecting to a remote service, the server machine’s IP address and service port number are required.</p>
<p>What is not immediately evident is that when a connection is established that the client side of the connection uses a port number. Unless a client program explicitly requests a specific port number, the port number used is an ephemeral port number.</p>
<p>Ephemeral ports are temporary ports assigned by a machine’s IP stack, and are assigned from a designated range of ports for this purpose. When the connection terminates, the ephemeral port is available for reuse, although most IP stacks won’t reuse that port number until the entire pool of ephemeral ports have been used.</p>
<p>So, if the client program reconnects, it will be assigned a different ephemeral port number for its side of the new connection.</p>
</blockquote>
<h2 id="linux-如何选择Ephemeral-Port"><a href="#linux-如何选择Ephemeral-Port" class="headerlink" title="linux 如何选择Ephemeral Port"></a>linux 如何选择Ephemeral Port</h2><p>有资料说是随机从Port Range选择port，有的说是顺序选择，那么实际验证一下。</p>
<p>如下测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">#include &lt;stdio.h&gt;      // printf</div><div class="line">#include &lt;stdlib.h&gt;     // atoi</div><div class="line">#include &lt;unistd.h&gt;     // close</div><div class="line">#include &lt;arpa/inet.h&gt;  // ntohs</div><div class="line">#include &lt;sys/socket.h&gt; // connect, socket</div><div class="line"></div><div class="line">void sample() &#123;</div><div class="line">    // Create socket</div><div class="line">    int sockfd;</div><div class="line">    if (sockfd = socket(AF_INET, SOCK_STREAM, 0), -1 == sockfd) &#123;</div><div class="line">        perror(&quot;socket&quot;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    // Connect to remote. This does NOT actually send a packet.</div><div class="line">    const struct sockaddr_in raddr = &#123;</div><div class="line">        .sin_family = AF_INET,</div><div class="line">        .sin_port   = htons(8080),     // arbitrary remote port</div><div class="line">        .sin_addr   = htonl(INADDR_ANY)  // arbitrary remote host</div><div class="line">    &#125;;</div><div class="line">    if (-1 == connect(sockfd, (const struct sockaddr *)&amp;raddr, sizeof(raddr))) &#123;</div><div class="line">        perror(&quot;connect&quot;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    // Display selected ephemeral port</div><div class="line">    const struct sockaddr_in laddr;</div><div class="line">    socklen_t laddr_len = sizeof(laddr);</div><div class="line">    if (-1 == getsockname(sockfd, (struct sockaddr *)&amp;laddr, &amp;laddr_len)) &#123;</div><div class="line">        perror(&quot;getsockname&quot;);</div><div class="line">    &#125;</div><div class="line">    printf(&quot;local port: %i\n&quot;, ntohs(laddr.sin_port));</div><div class="line"></div><div class="line">    // Close socket</div><div class="line">    close(sockfd);</div><div class="line">&#125;</div><div class="line"></div><div class="line">int main() &#123;</div><div class="line">    for (int i = 0; i &lt; 5; i++) &#123;</div><div class="line">        sample();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>bind逻辑测试代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">#include &lt;netinet/in.h&gt;</div><div class="line">#include &lt;arpa/inet.h&gt;</div><div class="line">#include &lt;stdio.h&gt;</div><div class="line">#include &lt;stdlib.h&gt;</div><div class="line">#include &lt;unistd.h&gt;</div><div class="line">#include &lt;errno.h&gt;</div><div class="line">#include &lt;string.h&gt;</div><div class="line">#include &lt;sys/types.h&gt;</div><div class="line">#include &lt;time.h&gt;</div><div class="line"></div><div class="line">void test_bind()&#123;</div><div class="line">    int listenfd = 0, connfd = 0;</div><div class="line">    struct sockaddr_in serv_addr;</div><div class="line">    char sendBuff[1025];</div><div class="line">    time_t ticks;</div><div class="line">	  socklen_t len;</div><div class="line"></div><div class="line">    listenfd = socket(AF_INET, SOCK_STREAM, 0);</div><div class="line">    memset(&amp;serv_addr, &apos;0&apos;, sizeof(serv_addr));</div><div class="line">    memset(sendBuff, &apos;0&apos;, sizeof(sendBuff));</div><div class="line"></div><div class="line">    serv_addr.sin_family = AF_INET;</div><div class="line">    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);</div><div class="line">    serv_addr.sin_port = htons(0);</div><div class="line"></div><div class="line">    bind(listenfd, (struct sockaddr*)&amp;serv_addr, sizeof(serv_addr));</div><div class="line"></div><div class="line">  	len = sizeof(serv_addr);</div><div class="line">	  if (getsockname(listenfd, (struct sockaddr *)&amp;serv_addr, &amp;len) == -1) &#123;</div><div class="line">		      perror(&quot;getsockname&quot;);</div><div class="line">			    return;</div><div class="line">	  &#125;</div><div class="line">	  printf(&quot;port number %d\n&quot;, ntohs(serv_addr.sin_port)); //只是挑选到了port，在系统层面保留，tcp连接还没有，netstat是看不到的</div><div class="line">&#125;</div><div class="line"></div><div class="line">int main(int argc, char *argv[])</div><div class="line">&#123;</div><div class="line">	    for (int i = 0; i &lt; 5; i++) &#123;</div><div class="line">			         test_bind();</div><div class="line">					     &#125;</div><div class="line">		    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-10-0-327-ali2017-alios7-x86-64"><a href="#3-10-0-327-ali2017-alios7-x86-64" class="headerlink" title="3.10.0-327.ali2017.alios7.x86_64"></a>3.10.0-327.ali2017.alios7.x86_64</h3><p>编译后，执行(3.10.0-327.ali2017.alios7.x86_64)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">#date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 ; echo &quot;-------&quot; &amp;&amp; ./client &amp;&amp; sleep 10; date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 &amp;&amp; echo &quot;******&quot;; ./client;</div><div class="line">Fri Nov 27 10:52:52 CST 2020</div><div class="line">local port: 17448</div><div class="line">local port: 17449</div><div class="line">local port: 17451</div><div class="line">local port: 17452</div><div class="line">local port: 17453</div><div class="line">+++++++</div><div class="line">local port: 17455</div><div class="line">local port: 17456</div><div class="line">local port: 17457</div><div class="line">local port: 17458</div><div class="line">local port: 17460</div><div class="line">-------</div><div class="line">local port: 17475</div><div class="line">local port: 17476</div><div class="line">local port: 17477</div><div class="line">local port: 17478</div><div class="line">local port: 17479</div><div class="line">Fri Nov 27 10:53:02 CST 2020</div><div class="line">local port: 17997</div><div class="line">local port: 17998</div><div class="line">local port: 17999</div><div class="line">local port: 18000</div><div class="line">local port: 18001</div><div class="line">+++++++</div><div class="line">local port: 18002</div><div class="line">local port: 18003</div><div class="line">local port: 18004</div><div class="line">local port: 18005</div><div class="line">local port: 18006</div><div class="line">******</div><div class="line">local port: 18010</div><div class="line">local port: 18011</div><div class="line">local port: 18012</div><div class="line">local port: 18013</div><div class="line">local port: 18014</div></pre></td></tr></table></figure>
<p>从测试看起来linux下端口选择跟时间有关系，起始端口肯定是顺序增加，起始端口应该是在Ephemeral Port范围内并且和时间戳绑定的某个值（也是递增的），即使没有使用任何端口，起始端口也会随时间增加而增加。</p>
<h3 id="4-19-91-19-1-al7-x86-64"><a href="#4-19-91-19-1-al7-x86-64" class="headerlink" title="4.19.91-19.1.al7.x86_64"></a>4.19.91-19.1.al7.x86_64</h3><p>换个内核版本编译后，执行(4.19.91-19.1.al7.x86_64)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">$date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 ; echo &quot;-------&quot; &amp;&amp; ./client &amp;&amp; sleep 10; date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 &amp;&amp; echo &quot;******&quot;; ./client;</div><div class="line">Fri Nov 27 14:10:47 CST 2020</div><div class="line">local port: 7890</div><div class="line">local port: 7892</div><div class="line">local port: 7894</div><div class="line">local port: 7896</div><div class="line">local port: 7898</div><div class="line">+++++++</div><div class="line">local port: 7900</div><div class="line">local port: 7902</div><div class="line">local port: 7904</div><div class="line">local port: 7906</div><div class="line">local port: 7908</div><div class="line">-------</div><div class="line">local port: 7910</div><div class="line">local port: 7912</div><div class="line">local port: 7914</div><div class="line">local port: 7916</div><div class="line">local port: 7918</div><div class="line">Fri Nov 27 14:10:57 CST 2020</div><div class="line">local port: 7966</div><div class="line">local port: 7968</div><div class="line">local port: 7970</div><div class="line">local port: 7972</div><div class="line">local port: 7974</div><div class="line">+++++++</div><div class="line">local port: 7976</div><div class="line">local port: 7978</div><div class="line">local port: 7980</div><div class="line">local port: 7982</div><div class="line">local port: 7984</div><div class="line">******</div><div class="line">local port: 7988</div><div class="line">local port: 7990</div><div class="line">local port: 7992</div><div class="line">local port: 7994</div><div class="line">local port: 7996</div></pre></td></tr></table></figure>
<p>以上测试时的参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</div><div class="line">1024    65535</div></pre></td></tr></table></figure>
<p>将1024改成1025后，分配出来的都是奇数端口了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</div><div class="line">1025    1034</div><div class="line"></div><div class="line">$./client</div><div class="line">local port: 1033</div><div class="line">local port: 1025</div><div class="line">local port: 1027</div><div class="line">local port: 1029</div><div class="line">local port: 1031</div><div class="line">local port: 1033</div><div class="line">local port: 1025</div><div class="line">local port: 1027</div><div class="line">local port: 1029</div><div class="line">local port: 1031</div><div class="line">local port: 1033</div><div class="line">local port: 1025</div><div class="line">local port: 1027</div><div class="line">local port: 1029</div><div class="line">local port: 1031</div></pre></td></tr></table></figure>
<p>之所以都是偶数端口，是因为port_range 从偶数开始, 每次从++变到+2的<a href="https://github.com/plantegg/linux/commit/1580ab63fc9a03593072cc5656167a75c4f1d173" target="_blank" rel="external">原因</a>，connect挑选随机端口时都是在起始端口的基础上+2，而bind挑选随机端口的起始端口是系统port_range起始端口+1（这样和connect错开），然后每次仍然尝试+2，这样connect和bind基本一个用偶数另外一个就用奇数，一旦不够了再尝试使用另外一组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</div><div class="line">1024    1047</div><div class="line"></div><div class="line">$./bind &amp;  ---bind程序随机挑选5个端口</div><div class="line">port number 1039</div><div class="line">port number 1043</div><div class="line">port number 1045</div><div class="line">port number 1041</div><div class="line">port number 1047  --用完所有奇数端口</div><div class="line"></div><div class="line">$./bind &amp;    --继续挑选偶数端口</div><div class="line">[8] 4170</div><div class="line">port number 1044</div><div class="line">port number 1042</div><div class="line">port number 1046</div><div class="line">port number 0    --实在没有了</div><div class="line">port number 0</div></pre></td></tr></table></figure>
<p>可见4.19内核下每次port是+2，在3.10内核版本中是+1. 并且都是递增的，同时即使port不使用，也会随着时间的变化这个起始port增大。</p>
<p>Port Range有点像雷达转盘数字，时间就像是雷达上的扫描指针，这个指针不停地旋转，如果这个时候刚好有应用要申请Port，那么就从指针正好指向的Port开始向后搜索可用port</p>
<h2 id="tcp-max-tw-buckets"><a href="#tcp-max-tw-buckets" class="headerlink" title="tcp_max_tw_buckets"></a>tcp_max_tw_buckets</h2><p>tcp_max_tw_buckets: 在 TIME_WAIT 数量等于 tcp_max_tw_buckets 时，新的连接断开不再进入TIME_WAIT阶段，而是直接断开，并打印warnning.</p>
<p>实际测试发现 在 TIME_WAIT 数量等于 tcp_max_tw_buckets 时 新的连接仍然可以不断地创建和断开，这个参数大小不会影响性能，只是影响TIME_WAIT 数量的展示（当然 TIME_WAIT 太多导致local port不够除外）, 这个值设置小一点会避免出现端口不够的情况</p>
<blockquote>
<p>tcp_max_tw_buckets - INTEGER<br>    Maximal number of timewait sockets held by system simultaneously.If this number is exceeded time-wait socket is immediately destroyed and warning is printed. This limit exists only to prevent simple DoS attacks, you <em>must</em> not lower the limit artificially, but rather increase it (probably, after increasing installed memory), if network conditions require more than default value.</p>
</blockquote>
<h2 id="SO-LINGER"><a href="#SO-LINGER" class="headerlink" title="SO_LINGER"></a><a href="https://notes.shichao.io/unp/ch7/" target="_blank" rel="external">SO_LINGER</a></h2><p>SO_LINGER选项<strong>用来设置延迟关闭的时间，等待套接字发送缓冲区中的数据发送完成</strong>。 没有设置该选项时，在调用close() 后，在发送完FIN后会立即进行一些清理工作并返回。 如果设置了SO_LINGER选项，并且等待时间为正值，则在清理之前会等待一段时间。</p>
<p>如果把延时设置为 0  时，Socket就丢弃数据，并向对方发送一个 <code>RST</code> 来终止连接，因为走的是 RST 包，所以就不会有 <code>TIME_WAIT</code> 了。</p>
<blockquote>
<p>This option specifies how the <code>close</code> function operates for a connection-oriented protocol (for TCP, but not for UDP). By default, <code>close</code> returns immediately, but ==if there is any data still remaining in the socket send buffer, the system will try to deliver the data to the peer==.</p>
</blockquote>
<p>SO_LINGER 有三种情况</p>
<ol>
<li>l_onoff 为false（0）， 那么 l_linger 的值没有意义，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入<code>TIME_WAIT</code>状态。<strong>这是默认情况</strong></li>
<li>l_onoff 为true（非0），  l_linger 为0，主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送<code>RST</code>，连接不会进入 time_wait 状态，对端会收到 <code>java.net.SocketException: Connection reset</code>异常</li>
<li>l_onoff 为true（非0），  l_linger 也为非 0，这表示 <code>SO_LINGER</code>选项生效，并且超时时间大于零，这时调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况：<ul>
<li>数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK）</li>
<li>超时，未发送完成的数据被丢弃，连接发送<code>RST</code>进行非正常关闭</li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">struct linger &#123;</div><div class="line">  int   l_onoff;        /* 0=off, nonzero=on */</div><div class="line">  int   l_linger;       /* linger time, POSIX specifies units as seconds */</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<h3 id="NIO下设置-SO-LINGER-的错误案例"><a href="#NIO下设置-SO-LINGER-的错误案例" class="headerlink" title="NIO下设置 SO_LINGER 的错误案例"></a>NIO下设置 SO_LINGER 的错误案例</h3><p>在使用NIO时，最好不设置<code>SO_LINGER</code>。比如Tomcat服务端接收到请求创建新连接时，做了这样的设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SocketChannel.setOption(SocketOption.SO_LINGER, 1000)</div></pre></td></tr></table></figure>
<p><code>SO_LINGER</code>的单位为<code>秒</code>！在网络环境比较好的时候，例如客户端、服务器都部署在同一个机房，close虽然会被阻塞，但时间极短可以忽略。但当网络环境不那么好时，例如存在丢包、较长的网络延迟，buffer中的数据一直无法发送成功，那么问题就出现了：<code>close会被阻塞较长的时间，从而直接或间接引起NIO的IO线程被阻塞</code>，服务器会不响应，不能处理accept、read、write等任何IO事件。也就是应用频繁出现挂起现象。解决方法就是删掉这个设置，close时立即返回，由操作系统接手后面的工作。</p>
<p>这时会看到如下连接状态</p>
<p><img src="/images/951413iMgBlog/image-20220721100246598.png" alt="image-20220721100246598"></p>
<p>以及对应的堆栈</p>
<p><img src="/images/951413iMgBlog/image-20220721100421130.png" alt="image-20220721100421130"></p>
<p>查看其中一个IO线程等待的锁，发现锁是被HTTP线程持有。这个线程正在执行<code>preClose0</code>，就是在这里等待连接的关闭<img src="/images/951413iMgBlog/image-20220721100446521.png" alt="image-20220721100446521"></p>
<p>每次HTTP线程在关闭连接被阻塞时，同时持有了<code>SocketChannelImpl</code>的对象锁，而IO线程在把这个连接移除出它的selector管理队列时，也要获得同一个<code>SocketChannelImpl</code>的对象锁。IO线程就这么一次次的被阻塞，悲剧的无以复加。有些NIO框架会让IO线程去做close，这时候就更加悲剧了。</p>
<p><strong>总之这里的错误原因有两点：1）网络状态不好；2）错误理解了l_linger 的单位，是秒，不是毫秒。 在这两个原因的共同作用下导致了数据迟迟不能发送完毕，l_linger 超时又需要很久，所以服务会出现一直阻塞的状态。</strong></p>
<h2 id="为什么要有-time-wait-状态"><a href="#为什么要有-time-wait-状态" class="headerlink" title="为什么要有 time_wait 状态"></a>为什么要有 time_wait 状态</h2><blockquote>
<p>TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.</p>
</blockquote>
<p><img src="/images/951413iMgBlog/image-20220721093116395.png" alt="alt text"></p>
<h2 id="由Nginx-SY-CPU高负载引发内核探索之旅"><a href="#由Nginx-SY-CPU高负载引发内核探索之旅" class="headerlink" title="由Nginx SY CPU高负载引发内核探索之旅"></a><a href="https://mp.weixin.qq.com/s/njpdTW5TndO4-H7nbEpXAA" target="_blank" rel="external">由Nginx SY CPU高负载引发内核探索之旅</a></h2><p>这个案例来自腾讯7层网关团队，网关用的Nginx，请求转发给后面的被代理机器(RS:real server)，发现 sys CPU异常高，CPU都用在搜索可用端口.</p>
<p><img src="/images/951413iMgBlog/640-8259033.png" alt="Image"></p>
<p><img src="/images/951413iMgBlog/640-20221112211814567.png" alt="Image"></p>
<p> local port 不够的时候inet_hash_connect 中的spin_lock 会消耗过高的 sys（特别注意4.6内核后 local port 分奇偶数，每次loop+2，所以更容易触发port不够的场景）</p>
<p>核心原因总结: 4.6后内核把本地端口分成奇偶数，奇数给connect, 偶数给listen，本来端口有6万，这样connect只剩下3万，当这3万用完后也不会报找不到本地可用端口的错误(这里报错可能更好)，而是在奇数里找不到就找偶数里的，每次都这样。 没改以前，总共6万端口，用掉3万，不分奇偶的话那么每找两个端口就有一个能用，也就是50%的概率。但是改了新的实现方案后，每次先要找奇数的3万个，全部在用，然后到偶数里继续找到第30001个才是可用的，也就是找到的概率变成了3万分之一，一下子复杂度高了15000倍，不慢才怪 如果你对</p>
<p>我的看法，这个分奇偶数的实现就是坑爹货，在内核里胡乱搞，为了一个小场景搞崩大多数正常场景，真没必要，当然我这是事后诸葛亮，如果当时这种feature拿给我看我也会认为很不错，想不到这个坑点！</p>
<h2 id="从STGW流量下降探秘内核收包机制"><a href="#从STGW流量下降探秘内核收包机制" class="headerlink" title="从STGW流量下降探秘内核收包机制"></a><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&amp;mid=2649745268&amp;idx=1&amp;sn=f72f164847060d7b19cba272a38485e5&amp;scene=21#wechat_redirect" target="_blank" rel="external">从STGW流量下降探秘内核收包机制</a></h2><p>listen port search消耗CPU异常高</p>
<p><img src="/images/951413iMgBlog/640-9840722.jpeg" alt="图片"></p>
<p>在正常的情况下，服务器的listen port数量，大概就是几w个这样的量级。这种量级下，一个port对应一个socket，哈希桶大小为32是可以接受的。</p>
<p>然而在内核支持了reuseport并且被广泛使用后，情况就不一样了，<strong>在多进程架构里，listen port对应的socket数量，是会被几十倍的放大的。</strong>以应用层监听了5000个端口，reuseport 使用了50个cpu核心为例，5000*50/32约等于7812，意味着每次握手包到来时，光是查找listen socket，就需要遍历7800多次。随着机器硬件性能越来越强，应用层使用的cpu数量增多，这个问题还会继续加剧。</p>
<p><strong>正因为上述原因，并且我们现网机器开启了reuseport，在端口数量较多的机器里，inet_lookup_listener的哈希桶大小太小，遍历过程消耗了cpu，导致出现了函数热点。</strong></p>
<h2 id="短连接的开销"><a href="#短连接的开销" class="headerlink" title="短连接的开销"></a>短连接的开销</h2><p>用ab通过短连接走 lo 网卡压本机 nginx，CPU0是 ab 进程，CPU3/4 是 Nginx 服务，可以看到 si 非常高，QPS 2.2万</p>
<p><img src="/images/951413iMgBlog/image-20220627154822263.png" alt="image-20220627154822263"></p>
<p>再将 ab 改用长连接来压，可以看到si、sy都有下降，并且 si 下降到短连接的20%，QPS 还能提升到 5.2万</p>
<p><img src="/images/951413iMgBlog/image-20220627154931495.png" alt="image-20220627154931495"></p>
<h2 id="一条连接的开销"><a href="#一条连接的开销" class="headerlink" title="一条连接的开销"></a><a href="https://mp.weixin.qq.com/s/BwddYkVLSYlkKFNeA-NUVg" target="_blank" rel="external">一条连接的开销</a></h2><p>主要是内存开销(如图，来源见水印)，另外就是每个连接都会占用一个文件句柄，可以通过参数来设置：fs.nr_open、nofile（其实 nofile 还分 soft 和 hard） 和 fs.file-max</p>
<p><img src="/images/951413iMgBlog/640-20220413134252639" alt="Image"></p>
<p>从上图可以看到：</p>
<ul>
<li><p>没有收发数据的时候收发buffer不用提前分配，3K多点的内存是指一个连接的元信息数据空间，不包含传输数据的内存buffer</p>
</li>
<li><p>客户端发送数据后，会根据数据大小分配send buffer（一般不超过wmem，默认kernel会根据系统内存压力来调整send buffer大小)</p>
</li>
<li><p>server端kernel收到数据后存放在rmem中，应用读走后就会释放对应的rmem</p>
</li>
<li><p>rmem和wmem都不会重用，用时分配用完释放</p>
</li>
</ul>
<p>可见，内核在 socket 内存开销优化上采取了不少方法:</p>
<ul>
<li>内核会尽量及时回收发送缓存区、接收缓存区，但高版本做的更好</li>
<li>发送接收缓存区最小并一定不是 rmem 内核参数里的最小值，实际大部分时间都是0</li>
<li>其它状态下，例如对于TIME_WAIT还会回收非必要的 socket_alloc 等对象</li>
</ul>
<p>或者看这篇分析：<a href="https://zhuanlan.zhihu.com/p/25241630" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25241630</a> </p>
<h2 id="不同进程使用相同端口，设置SO-REUSEADDR后被bind-导致可用-local-port-不够"><a href="#不同进程使用相同端口，设置SO-REUSEADDR后被bind-导致可用-local-port-不够" class="headerlink" title="不同进程使用相同端口，设置SO_REUSEADDR后被bind  导致可用 local port 不够"></a><a href="https://ata.alibaba-inc.com/articles/251853" target="_blank" rel="external">不同进程使用相同端口，设置SO_REUSEADDR后被bind  导致可用 local port 不够</a></h2><p>A进程选择某个端口当local port 来connect，并设置了 reuseaddr opt（表示其它进程还能继续用这个端口），这时B进程选了这个端口，并且bind了，B进程用完后把这个bind的端口释放了，但是如果 A 进程一直不释放这个端口对应的连接，那么这个端口会一直在内核中记录被bind用掉了（能bind的端口 是65535个，四元组不重复的连接你理解可以无限多），这样的端口越来越多后，剩下可供 A 进程发起连接的本地随机端口就越来越少了(也就是本来A进程选择端口是按四元组的，但因为前面所说的原因，导致不按四元组了，只按端口本身这个一元组来排重)，这时会造成新建连接的时候这个四元组高概率重复，一般这个时候对端大概率还在 time_wait 状态，会忽略掉握手 syn 包并回复 ack ，进而造成建连接卡顿的现象</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>在内存、文件句柄足够的话一台服务器上可以创建的TCP连接数量是没有限制的</li>
<li>SO_REUSEADDR 主要用于快速重用 TIME_WAIT状态的TCP端口，避免服务重启就会抛出Address Already in use的错误</li>
<li>SO_REUSEPORT主要用来解决惊群、性能等问题</li>
<li>全局范围可以用 net.ipv4.tcp_max_tw_buckets = 50000 来限制总 time_wait 数量，但是会掩盖问题</li>
<li>local port的选择是递增搜索的，搜索起始port随时间增加也变大</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://segmentfault.com/a/1190000002396411" target="_blank" rel="external">https://segmentfault.com/a/1190000002396411</a></p>
<p><a href="https://blog.csdn.net/a364572/article/details/40628171" target="_blank" rel="external">linux中TCP的socket、bind、listen、connect和accept的实现</a></p>
<p><a href="https://ops.tips/blog/how-linux-tcp-introspection/" target="_blank" rel="external">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
<p><a href="https://idea.popcount.org/2014-04-03-bind-before-connect/" target="_blank" rel="external">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>
<p><a href="https://mp.weixin.qq.com/s/C-Eeoeh9GHxugF4J30fz1A" target="_blank" rel="external">TCP连接中客户端的端口号是如何确定的？</a></p>
<p><a href="https://github.com/plantegg/linux/commit/9b3312bf18f6873e67f1f51dab3364c95c9dc54c" target="_blank" rel="external">对应4.19内核代码解析</a></p>
<p><a href="https://blog.cloudflare.com/how-to-stop-running-out-of-ephemeral-ports-and-start-to-love-long-lived-connections/" target="_blank" rel="external">How to stop running out of ephemeral ports and start to love long-lived connections</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/23/一次春节大促性能压测不达标的瓶颈推演/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/23/一次春节大促性能压测不达标的瓶颈推演/" itemprop="url">一次春节大促性能压测不达标的瓶颈推演</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-23T11:30:03+08:00">
                2020-11-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一次春节大促性能压测不达标的瓶颈推演"><a href="#一次春节大促性能压测不达标的瓶颈推演" class="headerlink" title="一次春节大促性能压测不达标的瓶颈推演"></a>一次春节大促性能压测不达标的瓶颈推演</h1><p>本文示范了教科书式的在分布式应用场景下如何通过一个节点的状态来推演分析瓶颈出在上下游的哪个环节上。</p>
<h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>某客户通过PTS（一个打压力工具）来压选号业务(HTTP服务在9108端口上），一个HTTP请求对应一次select seq-id 和 一次insert</p>
<p>PTS端看到RT900ms+，QPS大概5万（期望20万）， 数据库代理服务 rt 5ms，QPS 10万+</p>
<h3 id="链路："><a href="#链路：" class="headerlink" title="链路："></a>链路：</h3><p>pts发起压力 -&gt; 5个eip -&gt; slb -&gt; app(300个容器运行tomcat监听9108端口上） -&gt; slb -&gt; 数据库代理服务集群 -&gt; RDS集群</p>
<p>性能不达标，怀疑数据库代理服务或者RDS性能不行，作为数据库需要自证清白，所以从RDS和数据库代理服务开始分析问题在哪里。</p>
<p>略过一系列在数据库代理服务、RDS上分析数据和监控图表都证明数据库代理服务和RDS没问题的过程。</p>
<p>在明确给出证据数据库代理服务和RDS都没问题后还是要解决问题，所以只能进一步帮助前面的app来分析为什么性能不达标。</p>
<h2 id="在其中一个app应用上抓包（00-18秒到1-04秒），到数据库代理服务的一个连接分析："><a href="#在其中一个app应用上抓包（00-18秒到1-04秒），到数据库代理服务的一个连接分析：" class="headerlink" title="在其中一个app应用上抓包（00:18秒到1:04秒），到数据库代理服务的一个连接分析："></a>在其中一个app应用上抓包（00:18秒到1:04秒），到数据库代理服务的一个连接分析：</h2><p><img src="/images/oss/80374e55936bc36bbd243f79fcdb5f8d.png" alt="image.png"></p>
<p>数据库代理服务每个HTTP请求的响应时间都控制在15ms(一个前端HTTP请求对应一个select seq-id，一个 select readonly, 一个insert， 这个响应时间符合预期）。一个连接每秒才收到20 tps（因为压力不够，压力加大的话这个单连接tps还可以增加）， 20*3000 = 6万 ， 跟压测看到基本一致</p>
<p>300个容器，每个容器 10个连接到数据库代理服务</p>
<p>如果300个容器上的并发压力不够的话就没法将3000个连接跑满，所以看到的QPS是5万。</p>
<p><strong>从300个容器可以计算得到这个集群能支持的tps： 300*10（10个连接）* 1000/15(每秒钟每个连接能处理的请求数）=20万个tps （关键分析能力）</strong></p>
<p>也就是说通过单QPS 15ms，我们计算可得整个后端的吞吐能力在20万QPS。所以目前问题不在后端，而是压力没有打到后端就出现瓶颈了。</p>
<h2 id="9108的HTTP服务端口上的抓包分析"><a href="#9108的HTTP服务端口上的抓包分析" class="headerlink" title="9108的HTTP服务端口上的抓包分析"></a>9108的HTTP服务端口上的抓包分析</h2><p><img src="/images/oss/e239a12a1c3612263736256c8efc06e4.png" alt="image.png"></p>
<p>9108服务的每个HTTP response差不多都是15ms（<strong>这个响应时间基本符合预期</strong>），一个HTTP连接上在45秒的抓包时间范围只收到23个HTTP Request。</p>
<p>或者下图：</p>
<p><img src="/images/951413iMgBlog/image-20220627164250973.png" alt="image-20220627164250973" style="zoom:50%;"></p>
<p><img src="/images/951413iMgBlog/image-20220630101036341.png" alt="image-20220630101036341" style="zoom:50%;"></p>
<p>统计9108端口在45秒总共收到的HTTP请求数量是6745（如下图），也就是每个app每秒钟收到的请求是150个，300<em>150=4.5万（理论值，300个app可能压力分布不一样？），<em>*从这里看app收到的压力还不够</em></em>，所以压力还没有打到应用容器中的app，还在更前面</p>
<p><img src="/images/oss/6a289d1bba1e875d215032b6fdc7b084.png" alt="image.png"></p>
<p>后来从容器app监控也确认了这个响应时间和抓包看到的一致，所以从抓包分析http响应时间也基本得到15ms的rt关键结论</p>
<p>从wireshark IO Graphs 也能看到RT 和 QPS</p>
<p><img src="/images/951413iMgBlog/image-20220623003026351.png" alt="image-20220623003026351"></p>
<h2 id="从应用容器上的netstat统计来看，也是压力端回复太慢"><a href="#从应用容器上的netstat统计来看，也是压力端回复太慢" class="headerlink" title="从应用容器上的netstat统计来看，也是压力端回复太慢"></a>从应用容器上的netstat统计来看，也是压力端回复太慢</h2><p><img src="/images/oss/938ce314d19b47cba99e2a09c753f606.png" alt="image.png"></p>
<p>send-q表示回复从9108发走了，没收到对方的ack</p>
<h2 id="ARMS监控分析9108端口上的RT"><a href="#ARMS监控分析9108端口上的RT" class="headerlink" title="ARMS监控分析9108端口上的RT"></a>ARMS监控分析9108端口上的RT</h2><p>后来PTS的同学说ARMS可以捞到监控数据，如下是对rt时间降序排</p>
<p><img src="/images/oss/a479bad250c03aee41d58850afab9c14.png" alt="image.png"></p>
<p>中的rt平均时间，可以看到http的rt确实14.4ms，表现非常平稳，从这个监控也发现实际app是330个而不是用户自己描述的300个，这也就是为什么实际是tps是5万，但是按300个去算的话tps是4.5万（不要纠结客户为什么告诉你是300个容器而不是330个，有时候他们也搞不清楚，业务封装得太好了）</p>
<p><img src="/images/oss/2f3b76be63d331510eb6f2cecd91747f.png" alt="image.png"></p>
<p>5分钟时间，QPS是5万+，HTTP的平均rt是15ms， HTTP的最大rt才79ms，和前面抓包分析一致。</p>
<h2 id="从后端分析的总结"><a href="#从后端分析的总结" class="headerlink" title="从后端分析的总结"></a>从后端分析的总结</h2><p><strong>从9108端口响应时间15ms来看是符合预期的，为什么PTS看到的RT是900ms+，所以压力还没有打到APP上（也就是9108端口）</strong></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>最后发现是 eip 带宽不足，只有200M，调整到1G后 tps 也翻了5倍到了25万。</p>
<p>pts -&gt; 5个eip(总带宽200M) -&gt; slb -&gt; app(330个HTTP容器） -&gt; slb -&gt; 数据库代理服务 -&gt; RDS</p>
<p>这个案例有意思的地方是可以通过抓包就能分析出集群能扛的QPS20万（实际只有5万），那么可以把这个分析原则在每个角色上挨个分析一下，来看瓶颈出在了哪个环节。</p>
<p>应用端看到的rt是900ms，从后段开始往前面应用端来撸，看看每个环节的rt数据。</p>
<h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><ul>
<li>搞清楚 请求 从发起端到DB的链路路径，比如 pts -&gt; 5个eip(总带宽200M) -&gt; slb -&gt;  app(330个HTTP容器） -&gt; slb -&gt; 数据库代理服务 -&gt; RDS </li>
<li>压不上去得从发压力端开始往后端撸，撸每个产品的rt，每个产品给出自己的rt来自证清白</li>
<li>应用有arms的话学会看arms对平均rt和QPS的统计，不要纠结个别请求的rt抖动，看平均rt</li>
<li>通过抓包完全可以分析出来系统能扛多少并发，以及可能的瓶颈位置</li>
</ul>
<p>一包在手 万事无忧</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/18/TCP连接为啥互串了/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/18/TCP连接为啥互串了/" itemprop="url">活久见，TCP连接互串了</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-18T17:30:03+08:00">
                2020-11-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="活久见，TCP连接互串了"><a href="#活久见，TCP连接互串了" class="headerlink" title="活久见，TCP连接互串了"></a>活久见，TCP连接互串了</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>应用每过一段时间总是会抛出几个连接异常的错误，需要查明原因。</p>
<p>排查后发现是TCP连接互串了，这个案例实在是很珍惜，所以记录一下。</p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>业务结构： 应用-&gt;MySQL(10.112.61.163)</p>
<p>在 应用 机器上抓包这个异常连接如下（3269为MySQL服务端口）：</p>
<p><img src="/images/oss/dd657fee9d961a786c05e8d3cccbc297.png" alt="image.png"></p>
<p>粗一看没啥奇怪的，就是应用发查询给3269，但是一直没收到3269的ack，所以一直重传。这里唯一的解释就是网络不通。最后MySQL的3269还回复了一个rst，这个rst的id是42889，引起了我的好奇，跟前面的16439不连贯，正常应该是16440才对。（请记住上图中的绿框中的数字）</p>
<p>于是我过滤了一下端口61902上的所有包：</p>
<p><img src="/images/oss/8ca7da8ccec0041dd5d3f66f94d1f574.png" alt="image.png"></p>
<p>可以看到绿框中的查询从61902端口发给3269后，很奇怪居然收到了一个来自别的IP+3306端口的reset，这个包对这个连接来说自然是不认识（这个连接只接受3269的回包），就扔掉了。但是也没收到3269的ack，所以只能不停地重传，然后每次都收到3306的reset，reset包的seq、id都能和上图的绿框对应上。</p>
<p>明明他们应该是两个连接：</p>
<blockquote>
<p> 61902-&gt;10.141.16.0:3306</p>
<p> 61902-&gt;10.112.61.163:3269</p>
</blockquote>
<p>他们虽然用的本地ip端口（61902）是一样的， 但是根据四元组不一样，还是不同的TCP连接，所以应该是不会互相干扰的。但是实际看起来<strong>seq、id都重复了</strong>，不会有这么巧，非常像是TCP互串了。</p>
<h2 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h2><p>10.141.16.0 这个ip看起来像是lvs的ip，查了一下系统，果然是lvs，然后这个lvs 后面的rs就是10.112.61.163</p>
<p>那么这个连结构就是10.141.16.0:3306：</p>
<blockquote>
<p>应用 -&gt; lvs(10.141.16.0:3306)-&gt; 10.112.61.163:3269  跟应用直接连MySQL是一回事了</p>
</blockquote>
<p>所以这里的疑问就变成了：<strong>10.141.16.0 这个IP的3306端口为啥能知道 10.112.61.163:3269端口的seq和id，也许是TCP连接串了</strong></p>
<p>接着往下排查</p>
<h3 id="先打个岔，分析下这里的LVS的原理"><a href="#先打个岔，分析下这里的LVS的原理" class="headerlink" title="先打个岔，分析下这里的LVS的原理"></a><a href="/2019/06/20/就是要你懂负载均衡--lvs和转发模式/">先打个岔，分析下这里的LVS的原理</a></h3><p>这里使用的是 full NAT模型(full NetWork Address Translation-全部网络地址转换)</p>
<p>基本流程（类似NAT）：</p>
<ol>
<li>client发出请求（sip 200.200.200.2 dip 200.200.200.1）</li>
<li>请求包到达lvs，lvs修改请求包为<strong>（sip 200.200.200.1， dip rip）</strong> 注意这里sip/dip都被修改了</li>
<li>请求包到达rs， rs回复（sip rip，dip 200.200.200.1）</li>
<li>这个回复包的目的IP是VIP(不像NAT中是 cip)，所以LVS和RS不在一个vlan通过IP路由也能到达lvs</li>
<li>lvs修改sip为vip， dip为cip，修改后的回复包（sip 200.200.200.1，dip 200.200.200.2）发给client</li>
</ol>
<p><img src="/images/oss/94d55b926b5bb1573c4cab8353428712.png" alt="image.png"></p>
<p><strong>注意上图中绿色的进包和红色的出包他们的地址变化</strong></p>
<p>本来这个模型下都是正常的，但是为了Real Server能拿到client ip，也就是Real Server记录来源ip的时候希望记录的是client ip而不是LVS ip。这个时候LVS会将client ip放在tcp的options里面，然后在RealServer机器的内核里面将options中的client ip取出替换掉 lvs ip。所以Real Server上感知到的对端ip就是client ip。</p>
<p>回包的时候RealServer上的内核模块同样将目标地址从client ip改成lvs ip，同时将client ip放入options中。</p>
<h2 id="回到问题"><a href="#回到问题" class="headerlink" title="回到问题"></a>回到问题</h2><p>看完理论，再来分析这两个连接的行为</p>
<p>fulnat模式下连接经过lvs到达mysql后，mysql上看到的连接信息是，cip+port，也就是在MySQL上的连接</p>
<p><strong>lvs-ip:port -&gt; 10.112.61.163:3269  被修改成了 </strong>client-ip:61902 **-&gt; 10.112.61.163:3269</p>
<p>那么跟不走LVS的连接：</p>
<p><strong>client-ip:61902 -&gt;  10.112.61.163:3269 (直连) 完全重复了。</strong></p>
<p>MySQL端看到的两个连接四元组一模一样了：</p>
<blockquote>
<p>10.112.61.163:3269 -&gt; client-ip:61902 (走LVS，本来应该是lvs ip的，但是被替换成了client ip) </p>
<p>10.112.61.163:3269 -&gt; client-ip:61902 (直连) </p>
</blockquote>
<p>这个时候应用端看到的还是两个连接：</p>
<blockquote>
<p>client-ip:61902 -&gt; 10.141.16.0:3306 （走LVS） </p>
<p>client-ip:61902 -&gt;  10.112.61.163:3269 (直连) </p>
</blockquote>
<p>总结下，也就是这个连接经过LVS转换后在服务端（MYSQL）跟直连MySQL的连接四元组完全重复了，也就是MySQL会认为这两个连接就是同一个连接，所以必然出问题了。</p>
<p>实际两个连接建立的情况：</p>
<blockquote>
<p> 和mysqlserver的61902是04:22建起来的，和lvs的61902端口 是42:10建起来的，和lvs的61902建起来之后马上就出问题了</p>
</blockquote>
<h2 id="问题出现的条件"><a href="#问题出现的条件" class="headerlink" title="问题出现的条件"></a>问题出现的条件</h2><ul>
<li>fulnat模式的LVS，RS上装有slb_toa内核模块（RS上会将LVS ip还原成client ip）</li>
<li>client端正好重用一个相同的本地端口分别和RS以及LVS建立了两个连接</li>
</ul>
<p>这个时候这两个连接在MySQL端就会变成一个，然后两个连接的内容互串，必然导致rst</p>
<p>这个问题还挺有意思的，估计没几个程序员一辈子能碰上一次。推荐另外一个好玩的连接：<a href="/2020/07/01/如何创建一个自己连自己的TCP连接/">如何创建一个自己连自己的TCP连接</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="/2019/06/20/就是要你懂负载均衡--lvs和转发模式/">就是要你懂负载均衡–lvs和转发模式</a></p>
<p><a href="https://idea.popcount.org/2014-04-03-bind-before-connect/" target="_blank" rel="external">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>
<p><a href="https://github.com/kubernetes/kubernetes/issues/81775" target="_blank" rel="external">no route to host</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/127099484" target="_blank" rel="external">另一种形式的tcp连接互串，新连接重用了time_wait的port，导致命中lvs内核表中的维护的旧连接发给了老的realserver</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/18/MySQL针对秒杀场景的优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/18/MySQL针对秒杀场景的优化/" itemprop="url">MySQL针对秒杀场景的优化</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-18T07:30:03+08:00">
                2020-11-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MySQL针对秒杀场景的优化"><a href="#MySQL针对秒杀场景的优化" class="headerlink" title="MySQL针对秒杀场景的优化"></a>MySQL针对秒杀场景的优化</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>对于秒杀热点场景，MySQL官方版本500 TPS每秒，在对MySQL优化前只能用redis来扛，redis没有事务能力，比如一个item下有多个sku就搞不定了。同时在前端搞限流、答题等让秒杀流量控制在可以承受的范围内。</p>
<h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>对于秒杀热点场景，MySQL官方版本扣减只能做到 500 TPS每秒，扛不住大促的流量，需要优化。从控制并发量将500优化到1400，再通过新语法来消除网络rtt对加锁时间的控制这样达到了 4000 TPS。最后合并多个扣减到一个，累积比如10ms提交，能将TPS 能升到4万以上这个能力。</p>
<h3 id="排队控制并发"><a href="#排队控制并发" class="headerlink" title="排队控制并发"></a>排队控制并发</h3><p>拍减模式在整个交易过程中只有一次扣减交互，所以是不需要付款减库存那样的判重逻辑，就是说，拍减的减库存sql只有一条update语句就搞定了。而付减有两条，一条insert判重+一条update减库存（双十一拍减接口在高峰的rt约为8ms，而付减接口在高峰的rt约为15ms）；</p>
<p>其次，当大量请求（线程）落到mysql的同一条记录上进行减库存时，线程之间会存在竞争关系，因为要争夺InnoDB的行锁，当一个线程获得了行锁，其他并发线程就只能等待（InnoDB内部还有死锁检测等机制会严重影响性能），当并发度越高时，等待的线程就越多，此时tps会急剧下降，rt会飙升，性能就不能满足要求了。那如何减少锁竞争？答案是：排队！库存中心从几个层面做了排队策略。</p>
<p>首先，在应用端进行排队，因为很多商品都是有sku的，当sku库存变化时item的库存也要做相应变化，所以需要根据itemId来进行排队，相同itemId的减库存操作会进入串行化排队处理逻辑，不过应用端的排队只能做到单机内存排队，当应用服务器数量过多时，落到db的并发请求仍然很多，所以最好的办法是在db端也加上排队策略，今年库存中心db部署了两个的排队patch，一个叫“并发控制”，是做在InnoDB层的，另一个叫“queue on pk”，是做在mysql的server层的，两个patch各有优缺点，前者不需要应用修改代码，db自动判断，后者需要应用程序写特殊的sql hint，前者控制的全局的sql，后者是根据hint来控制指定sql，两个patch的本质和应用端的排队逻辑是一致的，具体实现不同。双十一库存中心使用的是“并发控制”的patch。</p>
<blockquote>
<p>2013年的单减库存TPS最高记录是1381次每秒。</p>
</blockquote>
<p>对于秒杀热点场景，官方版本500tps每秒，问题在于同时涌入的请求太多，每次取锁都要检查其它等锁的线程（防止死锁），这个线程队列太长的话导致这个检查时间太长； 继续在前面增加能够进入到后面的并发数的控制，通过增加线程池、控制并发能到1400（no deadlock list check）；</p>
<blockquote>
<p><strong>热点更新下的死锁检测(</strong>no deadlock list check<strong>)</strong></p>
<p>由于热点更新是分布式的客户端并发的向单点的数据库进行了并行更新一条记录，到数据库最后是把并行的线程转行成串行的操作。但在串行操作的时候，由于对同一记录的锁申请列表过大，死锁检测的机制在检测锁队列的时候，反而拖慢了每一个更新。</p>
<p> 原生版本的MySQL对于正常业务链接没有拒绝机制（除了TDDL的链接池或者MySQL的user_connnection不够用），对于同一行记录到innodb层修改的时候，凡是到innodb层的任务都必须拿到innodb_thread_concurrency的槽位才能执行(当然这里也有很多细节，这里就说最主要的代码改动点)，举例来说：开启一个事务，对于id=1的行记录更新，进到innodb层，占着1个innodb_thread_concurrency，等到id=1的事务结束，会释放innodb_thread_concurrency,从而达到innodb_thread_concurrency的平衡；</p>
<p>再进一步，开启一个事务，对id=1的行记录更新进到innodb层，占着1个innodb_thread_concurrency，事务不提交（假设innodb_thread_concurrency=32)，如果有下一个对id=1记录来更新的话，进到innodb层，又占着1个innodb_thread_concurrency，检测发现是对id=1的更新，排到第1个对id=1的队列的后面，同时释放innodb_thread_concurrency;以此类推这个链表有可能会很长比如1024；执行的时候又需要做死锁检测等一系列工作，都需要用到一个叫做kernel_mutex的mutex（这是一个全局互斥量用来管理锁系统，事务系统，MVCC多版本控制），对于大并发，整个链表非常长的时候，可想而知kernel_metex的竞争多么激烈，从而在链表检测的时间变长。</p>
</blockquote>
<h3 id="缩短锁时间"><a href="#缩短锁时间" class="headerlink" title="缩短锁时间"></a>缩短锁时间</h3><p>接下来的问题在于一个事务中有多条语句（最少也有一个update+一个commit），这样update(减库存，开始锁表），走网络，查询结果（走网络），commit，两次跨网络调用导致update锁行比较久，于是可以新造一个语法 select update一次搞定，继续优化 select update commit_on_success_or_fail_rollback，将所有操作一次网络操作全部搞定，能到4000；</p>
<p>比如库存扣减的业务逻辑可以简化为下面这个事务:</p>
<p>（1）begin;</p>
<p>（2）insert 交易流水表; – 交易流水对账</p>
<p>（3）update 库存明细表 where id in (sku_id，item_id);</p>
<p>（4）select 库存明细表;</p>
<p>（5）commit</p>
<p><img src="/images/951413iMgBlog/TB1yvFqOpXXX.png" alt="Snip20161116_88.png"></p>
<p>SQL case：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">4059550 Query   <span class="keyword">SET</span> autocommit=<span class="number">0</span></div><div class="line"><span class="number">4059550</span> <span class="keyword">Query</span>   <span class="keyword">update</span> ROLLBACK_ON_FAIL TARGET_AFFECT_ROW <span class="number">1</span> trade <span class="keyword">set</span> <span class="keyword">version</span> = <span class="keyword">version</span>+<span class="number">3</span> ,gmt_modified = <span class="keyword">now</span>()    ,           optype = <span class="number">2</span>          ,      feature = <span class="string">';abc;'</span>  <span class="keyword">where</span> sub_biz_order_id = <span class="string">'15'</span> <span class="keyword">and</span> biz_order_type = <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">id</span> = <span class="number">5</span> <span class="keyword">and</span> ti_id = <span class="number">1</span> <span class="keyword">and</span>      optype = <span class="number">3</span>          <span class="keyword">and</span>      root_id = <span class="number">11</span></div><div class="line"><span class="number">4059550</span> <span class="keyword">Query</span>   <span class="keyword">select</span>      <span class="keyword">id</span>,*     <span class="keyword">from</span>   <span class="keyword">update</span> COMMIT_ON_SUCCESS ROLLBACK_ON_FAIL TARGET_AFFECT_ROW <span class="number">1</span> invetory   <span class="keyword">set</span>                                           withholding_quantity = withholding_quantity + <span class="number">-1</span>,  flag=flag &amp;~ (<span class="number">1</span>&lt;&lt;<span class="number">10</span>) &amp;~ (<span class="number">1</span>&lt;&lt;<span class="number">11</span>) , <span class="keyword">version</span>=<span class="keyword">version</span>+<span class="number">3</span>,gmt_modified = <span class="keyword">now</span>()         <span class="keyword">WHERE</span>  root_id = <span class="number">11</span> <span class="keyword">and</span> <span class="keyword">status</span> = <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">in</span>    (     <span class="number">1</span>    )  <span class="keyword">and</span> (withholding_quantity + <span class="number">-1</span>) &gt;= <span class="number">0</span></div><div class="line"><span class="number">4059550</span> <span class="keyword">Query</span>   <span class="keyword">commit</span></div></pre></td></tr></table></figure>
<h3 id="批量提交"><a href="#批量提交" class="headerlink" title="批量提交"></a>批量提交</h3><p>其主要的核心思想是：针对应用层SQL做轻量化改造，带上”热点行SQL”的hint，当这种SQL进入内核后，在内存中维护一个hash表，将主键或唯一键相同的请求(一般也就是同一商品id)hash到同一个地方做请求的合并，经过一段时间后(默认100us)统一提交，从而实现了将串行处理变成了批处理，让每个热点行更新请求并不需要都去扫描和更新btree。</p>
<ol>
<li>热点的自动识别:前面已经讲过了，库存的扣减SQL都会有commit on success标记。mysql内部分为普通通道和热点扣减通道。普通通道里是正常的事务。热点通道里收集带有commit on success标记的事务。在一定的时间区间段内(0.1ms)，将收集到的热点按照主键或者唯一键进行hash; hash到同一个桶中为相同的sku; 分批组提交这0.1ms收集到的热点商品。</li>
<li>轮询处理: 第一批进行提交时，第二批进行收集； 当第一批完成了提交开始收集时，第二批就可以进行提交了。不断轮询，提高效率</li>
</ol>
<p>通过内存合并库存减操作，干到100000（每个减库存操作生成一条独立的update binlog，不影响其他业务2016年双11），实际这里还可以调整批提交时间间隔来进一步提升扣减QPS</p>
<p><img src="/images/951413iMgBlog/TB1I_BvOpXXXXasXVXXXXXXXXXX.png" alt="Snip20161116_87.png"></p>
<p>超卖：付款减库存会超卖，拍减库存要防止恶意拍不付款。拍减的话可以通过增加SQL新语法来进一步优化DB响应(select update)</p>
<p>innodb_buffer_pool_instance: 将buffer pool 分成几个（hash），避免高并发修改的时候一个大锁mutex导致性能不高</p>
<p>批量提交的压测效果：</p>
<p><img src="/images/951413iMgBlog/image-20230814104356084.png" alt="image-20230814104356084"></p>
<h3 id="业务优化"><a href="#业务优化" class="headerlink" title="业务优化"></a>业务优化</h3><p>延迟扣减item，一般一个item下会有多个sku（比如 iPhone14 不同的颜色、配置就是一个不同的sku），而库存会有总库存（item），也有sku 库存，sku库存加起来就是item库存</p>
<p>导致扣减的时候 item库存更热</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/17/MySQL线程池导致的延时卡顿排查/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/17/MySQL线程池导致的延时卡顿排查/" itemprop="url">MySQL线程池导致的延时卡顿排查</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-17T07:30:03+08:00">
                2020-11-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MySQL-线程池导致的卡顿"><a href="#MySQL-线程池导致的卡顿" class="headerlink" title="MySQL 线程池导致的卡顿"></a>MySQL 线程池导致的卡顿</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>简单小表的主键点查SQL，单条执行很快，但是放在业务端，有时快有时慢，取了一条慢sql，在MySQL侧查看，执行时间很短。</p>
<p>通过Tomcat业务端监控有显示慢SQL，取slow.log里显示有12秒执行时间的SQL，但是这次12秒的执行在MySQL上记录下来的执行时间都不到1ms。</p>
<p>所在节点的tsar监控没有异常，Tomcat manager监控上没有fgc，Tomcat实例规格 16C32g<em>8, MySQL  32c128g  </em>32 。</p>
<p>5-28号现象复现，从监控图上CPU、内存、网络都没发现异常，MySQL侧查到的SQL依然执行很快，Tomcat侧记录12S执行时间，当时Tomcat节点的网络流量、CPU压力都很小。</p>
<p>所以客户怀疑Tomcat有问题或者Tomcat上的代码写得有问题导致了这个问题，需要排查和解决掉。</p>
<p>接下来我们会先分析这个问题出现的原因，然后会分析这类问题的共性同时拓展到其它场景下的类似问题。</p>
<h2 id="Tomcat上抓包分析"><a href="#Tomcat上抓包分析" class="headerlink" title="Tomcat上抓包分析"></a>Tomcat上抓包分析</h2><h3 id="慢的连接"><a href="#慢的连接" class="headerlink" title="慢的连接"></a>慢的连接</h3><p>经过抓包分析发现在慢的连接上，所有操作都很慢，包括set 命令，慢的时间主要分布在3秒以上，1-3秒的慢查询比较少，这明显不太符合分布规律。并且目前看慢查询基本都发生在MySQL的0库的部分连接上（后端有一堆MySQL组成的集群），下面抓包的4637端口是MySQL的服务端口：</p>
<p><img src="/images/oss/b8ed95b7081ee80eb23465ee0e9acc74.png" alt="image.png"></p>
<p>以上两个连接都很慢，对应的慢查询在MySQL里面记录很快。</p>
<p>慢的SQL的response按时间排序基本都在3秒以上：</p>
<p><img src="/images/oss/36a2a60f64011bc73fee06c291bcd79f.png" alt="image.png" style="zoom:67%;"></p>
<p>或者只看response time 排序，中间几个1秒多的都是 Insert语句。也就是1秒到3秒之间的没有，主要是3秒以上的查询</p>
<p><img src="/images/oss/07146ff29534a1070adbdb8cedd280c9.png" alt="image.png" style="zoom:67%;"></p>
<h3 id="快的连接"><a href="#快的连接" class="headerlink" title="快的连接"></a>快的连接</h3><p>同样一个查询SQL，发到同一个MySQL上(4637端口)，下面的连接上的所有操作都很快，下面是两个快的连接上的执行截图</p>
<p><img src="/images/oss/d129dfe1a50b182f4d100ac7147f9099.png" alt="image.png"></p>
<p>别的MySQL上都比较快，比如5556分片上的所有response RT排序，只有偶尔极个别的慢SQL</p>
<p><img src="/images/oss/01531d138b9bc8dafda76b7c8bbb5bc9.png" alt="image.png"></p>
<h2 id="MySQL相关参数"><a href="#MySQL相关参数" class="headerlink" title="MySQL相关参数"></a>MySQL相关参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">mysql&gt; show variables like &apos;%thread%&apos;;</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">| Variable_name                              | Value           |</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">| innodb_purge_threads                       | 1               |</div><div class="line">| innodb_MySQL_thread_extra_concurrency        | 0               |</div><div class="line">| innodb_read_io_threads                     | 16              |</div><div class="line">| innodb_thread_concurrency                  | 0               |</div><div class="line">| innodb_thread_sleep_delay                  | 10000           |</div><div class="line">| innodb_write_io_threads                    | 16              |</div><div class="line">| max_delayed_threads                        | 20              |</div><div class="line">| max_insert_delayed_threads                 | 20              |</div><div class="line">| myisam_repair_threads                      | 1               |</div><div class="line">| performance_schema_max_thread_classes      | 50              |</div><div class="line">| performance_schema_max_thread_instances    | -1              |</div><div class="line">| pseudo_thread_id                           | 12882624        |</div><div class="line">| MySQL_is_dump_thread                         | OFF             |</div><div class="line">| MySQL_threads_running_ctl_mode               | SELECTS         |</div><div class="line">| MySQL_threads_running_high_watermark         | 50000           |</div><div class="line">| rocksdb_enable_thread_tracking             | OFF             |</div><div class="line">| rocksdb_enable_write_thread_adaptive_yield | OFF             |</div><div class="line">| rocksdb_signal_drop_index_thread           | OFF             |</div><div class="line">| thread_cache_size                          | 100             |</div><div class="line">| thread_concurrency                         | 10              |</div><div class="line">| thread_handling                            | pool-of-threads |</div><div class="line">| thread_pool_high_prio_mode                 | transactions    |</div><div class="line">| thread_pool_high_prio_tickets              | 4294967295      |</div><div class="line">| thread_pool_idle_timeout                   | 60              |</div><div class="line">| thread_pool_max_threads                    | 100000          |</div><div class="line">| thread_pool_oversubscribe                  | 10              |</div><div class="line">| thread_pool_size                           | 96              |</div><div class="line">| thread_pool_stall_limit                    | 30              |</div><div class="line">| thread_stack                               | 262144          |</div><div class="line">| threadpool_workaround_epoll_bug            | OFF             |</div><div class="line">| tokudb_cachetable_pool_threads             | 0               |</div><div class="line">| tokudb_checkpoint_pool_threads             | 0               |</div><div class="line">| tokudb_client_pool_threads                 | 0               |</div><div class="line">+--------------------------------------------+-----------------+</div><div class="line">33 rows in set (0.00 sec)</div></pre></td></tr></table></figure>
<h2 id="综上结论"><a href="#综上结论" class="headerlink" title="综上结论"></a>综上结论</h2><p>问题原因跟MySQL线程池比较相关，慢的连接总是慢，快的连接总是快。需要到MySQL Server下排查线程池相关参数。</p>
<p>同一个慢的连接上的回包，所有 ack 就很快（OS直接回，不需要进到MySQL），但是set就很慢，基本理解只要进到MySQL的就慢了，所以排除了网络原因（流量本身也很小，也没看到乱序、丢包之类的）</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>18点的时候将4637端口上的MySQL thread_pool_oversubscribe 从10调整到20后，基本没有慢查询了：</p>
<p><img src="/images/oss/92069e7521368e4d2519b3b861cc7faa.png" alt="image.png" style="zoom:50%;"></p>
<p>当时从MySQL的观察来看，并发压力很小，很难抓到running thread比较高的情况（update: 可能是任务积压在队列中，只是96个thread pool中的一个thread全部running，导致整体running不高）</p>
<p>MySQL记录的执行时间是指SQL语句开始解析后统计，中间的等锁、等Worker都不会记录在执行时间中，所以当时对应的SQL在MySQL日志记录中很快。</p>
<p>thread_pool_stall_limit 会控制一个SQL过长时间（默认60ms）占用线程，如果出现stall_limit就放更多的SQL进入到thread pool中直到达到thread_pool_oversubscribe个</p>
<blockquote>
<p>The thread_pool_stall_limit affects executing statements. The value is the amount of time a statement has to finish after starting to execute before it becomes defined as stalled, at which point the thread pool permits the thread group to begin executing another statement. The value is measured in 10 millisecond units, so the default of 6 means 60ms. Short wait values permit threads to start more quickly. Short values are also better for avoiding deadlock situations. Long wait values are useful for workloads that include long-running statements, to avoid starting too many new statements while the current ones execute.</p>
</blockquote>
<h2 id="类似案例"><a href="#类似案例" class="headerlink" title="类似案例"></a>类似案例</h2><p>一个其它客户同样的问题的解决过程，最终发现是因为thread pool group中的active thread count 计数有泄漏，导致达到thread_pool_oversubscribe 的上限(实际没有任何线程运行)</p>
<p>问题：1）为什么调整到20后 active_thread_count 没变，但是不卡了？那以前卡着的10个 thread在干嘛？</p>
<p>​             2）卡住的根本原因是，升级能解决？</p>
<p><img src="/images/951413iMgBlog/image-20230308214801877.png" alt="image-20230308214801877"></p>
<p>调整前的 thread pool 配置：</p>
<p><img src="/images/951413iMgBlog/image-20230308222538102.png" alt="image-20230308222538102"></p>
<p>出问题时候的线程池 32个 group状态，有两个group queue count、active thread都明显到了瓶颈：select * from information_schema.THREAD_POOL_STATUS;</p>
<p><img src="/images/951413iMgBlog/image-20230308222416238.png" alt="image-20230308222416238"></p>
<p>调整 thread_pool_oversubscribe由10到20后不卡了，这时的 pool status(重点注意 ACTIVE_THREAD_COUNT 数字没有任何变化)：</p>
<p><img src="/images/951413iMgBlog/image-20230308223126774.png" alt="image-20230308223126774"></p>
<p>看起来像是 ACTIVE_THREAD 全假死了，没有跑任务也不去take新任务一直卡在那里，类似线程泄露。查看了一下所有MySQLD 线程都是 S 的正常状态，并无异常。</p>
<p>继续分析统计了一下mysqld线程数量(157)，远小于 thread pool 中的active线程数量(202)，看起来像是计数器在线程异常(磁盘故障时)忘记 减减 了，导致计数器虚高进而无法新创建新线程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">#top -H -b -n 1 -p 130650 |grep mysqld | wc -l</div><div class="line">157</div><div class="line"></div><div class="line"># mysql -e &quot;select sum(THREAD_COUNT), sum(ACTIVE_THREAD_COUNT)  from information_schema.THREAD_POOL_STATUS&quot;</div><div class="line">+-------------------+--------------------------+</div><div class="line">| sum(THREAD_COUNT) | sum(ACTIVE_THREAD_COUNT) |</div><div class="line">+-------------------+--------------------------+</div><div class="line">|                71 |                      206 |</div><div class="line">+-------------------+--------------------------+</div><div class="line"># mysql -e &quot;select sum(THREAD_COUNT), sum(ACTIVE_THREAD_COUNT)  from information_schema.THREAD_POOL_STATUS&quot;</div><div class="line">+-------------------+--------------------------+</div><div class="line">| sum(THREAD_COUNT) | sum(ACTIVE_THREAD_COUNT) |</div><div class="line">+-------------------+--------------------------+</div><div class="line">|                75 |                      202 |</div><div class="line">+-------------------+--------------------------+</div><div class="line"># mysql_secbox  --exe-path=mysql -A -uroot -h127.0.0.1 -P3054 -e  &quot;select ID,THREAD_COUNT,ACTIVE_THREAD_COUNT AS ATC,CONNECTION_COUNT AS CC,LOW_QUEUE_COUNT,HIGH_QUEUE_COUNT  from information_schema.THREAD_POOL_STATUS&quot;</div><div class="line">+----+--------------+-----+----+-----------------+------------------+</div><div class="line">| ID | THREAD_COUNT | ATC | CC | LOW_QUEUE_COUNT | HIGH_QUEUE_COUNT |</div><div class="line">+----+--------------+-----+----+-----------------+------------------+</div><div class="line">|  0 |            3 |   7 | 13 |               0 |                0 |</div><div class="line">| 19 |            3 |  10 | 14 |               0 |                0 |</div><div class="line">| 20 |            3 |   8 | 13 |               0 |                0 |</div><div class="line">| 21 |            2 |   5 |  9 |               0 |                0 |</div><div class="line">| 28 |            2 |   6 |  6 |               0 |                0 |</div><div class="line">//增加了一个active count,执行完毕后active thread count会减下去</div><div class="line">| 29 |            2 |  12 | 14 |               0 |                0 | </div><div class="line">| 30 |            2 |   4 | 11 |               0 |                0 | </div><div class="line">| 31 |            2 |   8 | 10 |               0 |                0 |</div><div class="line">+----+--------------+-----+----+-----------------+------------------+</div></pre></td></tr></table></figure>
<p>正常的thread_pool状态, ACTIVE_THREAD_COUNT极小且小于 THREAD_COUNT：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">mysql&gt; select ID,THREAD_COUNT,ACTIVE_THREAD_COUNT AS ATC,CONNECTION_COUNT AS CC,LOW_QUEUE_COUNT,HIGH_QUEUE_COUNT from information_schema.THREAD_POOL_STATUS;</div><div class="line">+----+--------------+-----+----+-----------------+------------------+</div><div class="line">| ID | THREAD_COUNT | ATC | CC | LOW_QUEUE_COUNT | HIGH_QUEUE_COUNT |</div><div class="line">+----+--------------+-----+----+-----------------+------------------+</div><div class="line">|  0 |            2 |   0 |  3 |               0 |                0 |</div><div class="line">|  1 |            2 |   0 |  2 |               0 |                0 |</div><div class="line">|  2 |            2 |   0 |  2 |               0 |                0 |</div><div class="line">|  3 |            2 |   0 |  2 |               0 |                0 |</div><div class="line">|  4 |            2 |   0 |  4 |               0 |                0 |</div><div class="line">|  5 |            2 |   0 |  3 |               0 |                0 |</div><div class="line">|  6 |            2 |   0 |  4 |               0 |                0 |</div><div class="line">|  7 |            2 |   0 |  4 |               0 |                0 |</div><div class="line">+----+--------------+-----+----+-----------------+------------------+</div></pre></td></tr></table></figure>
<h2 id="Thread-Pool原理"><a href="#Thread-Pool原理" class="headerlink" title="Thread Pool原理"></a><a href="https://dbaplus.cn/news-11-1989-1.html" target="_blank" rel="external">Thread Pool原理</a></h2><p><img src="/images/oss/6fbe1c10f07dd1c26eba0c0e804fa9a8.png" alt="image.png"></p>
<p>MySQL 原有线程调度方式有每个连接一个线程(one-thread-per-connection)和所有连接一个线程（no-threads）。</p>
<p>no-threads一般用于调试，生产环境一般用one-thread-per-connection方式。one-thread-per-connection 适合于低并发长连接的环境，而在高并发或大量短连接环境下，大量创建和销毁线程，以及线程上下文切换，会严重影响性能。另外 one-thread-per-connection 对于大量连接数扩展也会影响性能。</p>
<p>为了解决上述问题，MariaDB、Percona、Aliyun RDS、Oracle MySQL 都推出了线程池方案，它们的实现方式大体相似，这里以 Percona 为例来简略介绍实现原理，同时会介绍我们在其基础上的一些改进。</p>
<p>线程池由一系列 worker 线程组成，这些worker线程被分为<code>thread_pool_size</code>个group。用户的连接按 round-robin 的方式映射到相应的group 中，一个连接可以由一个group中的一个或多个worker线程来处理。</p>
<p>thread_pool_oversubscribe  一个group中活跃线程和等待中的线程超过<code>thread_pool_oversubscribe</code>时，不会创建新的线程。 此参数可以控制系统的并发数，同时可以防止调度上的死锁，考虑如下情况，A、B、C三个事务，A、B 需等待C提交。A、B先得到调度，同时活跃线程数达到了<code>thread_pool_max_threads</code>上限，随后C继续执行提交，此时已经没有线程来处理C提交，从而导致A、B一直等待。<code>thread_pool_oversubscribe</code>控制group中活跃线程和等待中的线程总数，从而防止了上述情况。</p>
<p><strong>MySQL Thread Pool之所以分成多个小的Thread Group Pool而不是一个大的Pool，是为了分解锁（每个group中都有队列，队列需要加锁。类似ConcurrentHashMap提高并发的原理），提高并发效率。另外如果对每个Pool的 Worker做CPU 亲和性绑定也会对cache更友好、效果更高</strong></p>
<p>group中又有多个队列，用来区分优先级的，事务中的语句会放到高优先队列（非事务语句和autocommit 都会在低优先队列）；等待太久的SQL也会挪到高优先队列，防止饿死。</p>
<p>比如启用Thread Pool后，如果出现多个慢查询，容易导致拨测类请求超时，进而出现Server异常的判断（类似Nginx 边缘触发问题）；或者某个group满后导致慢查询和拨测失败之类的问题</p>
<h3 id="thread-pool-size过小的案例"><a href="#thread-pool-size过小的案例" class="headerlink" title="thread_pool_size过小的案例"></a>thread_pool_size过小的案例</h3><p>应用出现大量1秒超时报错：</p>
<p><img src="/images/951413iMgBlog/52dbeb1c1058e6dbff0a790b4b4ba477.png" alt="image.png"></p>
<p><img src="/images/951413iMgBlog/image-20211104130625676.png" alt="image-20211104130625676"></p>
<p>分析代码，这个Druid报错堆栈是数据库连接池在创建到MySQL的连接后或者从连接池取一个连接给业务使用前会发送一个ping来验证下连接是否有效，有效后才给应用使用。说明TCP连接创建成功，但是MySQL 超过一秒钟都没有响应这个 ping，说明 MySQL处理指令缓慢。</p>
<p>继续分析MySQL的参数：</p>
<p><img src="/images/oss/8987545cc311fdd3ae232aee8c3f855a.png" alt="image.png"></p>
<p>可以看到thread_pool_size是1，太小了，将所有MySQL线程都放到一个buffer里面来抢锁，锁冲突的概率太高。调整到16后可以明显看到MySQL的RT从原来的12ms下降到了3ms不到，整个QPS大概有8%左右的提升。这是因为pool size为1的话所有sql都在一个队列里面，多个worker thread加锁等待比较严重，导致rt延迟增加。</p>
<p><img src="/images/oss/114b5b71468b33128e76129bbc7fb8f4.png" alt="image.png"></p>
<p>这个问题发现是因为压力一上来的时候要创建大量新的连接，这些连结创建后会去验证连接的有效性，也就是Druid给MySQL发一个ping指令，一般都很快，同时Druid对这个valid操作设置了1秒的超时时间，从实际看到大量超时异常堆栈，从而发现MySQL内部响应有问题。</p>
<h3 id="MySQL-ping和MySQL协议相关知识"><a href="#MySQL-ping和MySQL协议相关知识" class="headerlink" title="MySQL ping和MySQL协议相关知识"></a>MySQL ping和MySQL协议相关知识</h3><blockquote>
<p><a href="https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-usagenotes-j2ee-concepts-connection-pooling.html#idm47306928802368" target="_blank" rel="external">Ping</a> use the JDBC method <a href="http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#isValid(int" target="_blank" rel="external">Connection.isValid(int timeoutInSecs)</a>). Digging into the MySQL Connector/J source, the actual implementation uses com.mysql.jdbc.ConnectionImpl.pingInternal() to send a simple ping packet to the DB and returns true as long as a valid response is returned.</p>
</blockquote>
<p>MySQL ping protocol是发送了一个 <code>0e</code> 的byte标识给Server，整个包加上2byte的Packet Length（内容为：1），2byte的Packet Number（内容为：0），总长度为5 byte。Druid、DRDS默认都会testOnBorrow，所以每个连接使用前都会先做ping。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">public class MySQLPingPacket implements CommandPacket &#123;</div><div class="line">    private final WriteBuffer buffer = new WriteBuffer();</div><div class="line">    public MySQLPingPacket() &#123;</div><div class="line">        buffer.writeByte((byte) 0x0e);</div><div class="line">    &#125;</div><div class="line">    public int send(final OutputStream os) throws IOException &#123;</div><div class="line">        os.write(buffer.getLengthWithPacketSeq((byte) 0)); // Packet Number</div><div class="line">        os.write(buffer.getBuffer(),0,buffer.getLength()); // Packet Length 固定为1</div><div class="line">        os.flush();</div><div class="line">        return 0;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="/images/oss/7cf291546a167b0ca6a017e98db5a821.png" alt="image.png"></p>
<p>也就是一个TCP包中的Payload为 MySQL协议中的内容长度 + 4（Packet Length+Packet Number）。</p>
<h2 id="线程池卡死案例：show-stats导致集群3406监控卡死"><a href="#线程池卡死案例：show-stats导致集群3406监控卡死" class="headerlink" title="线程池卡死案例：show stats导致集群3406监控卡死"></a>线程池卡死案例：show stats导致集群3406监控卡死</h2><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><p>应用用于获取监控信息的端口 3406卡死，监控脚本无法连接上3406，监控没有数据（需要从3406采集）、DDL操作、show processlist、show stats操作卡死（需要跟整个集群的3406端口同步）。</p>
<p>通过jstack看到应用进程的3406端口线程池都是这样: </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">"ManagerExecutor-1-thread-1" #47 daemon prio=5 os_prio=0 tid=0x00007fe924004000 nid=0x15c runnable [0x00007fe9034f4000]</div><div class="line">   java.lang.Thread.State: RUNNABLE</div><div class="line">    at java.net.SocketInputStream.socketRead0(Native Method)</div><div class="line">    at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</div><div class="line">    at java.net.SocketInputStream.read(SocketInputStream.java:171)</div><div class="line">    at java.net.SocketInputStream.read(SocketInputStream.java:141)</div><div class="line">    at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">    at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">    at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">    - locked &lt;0x0000000722538b60&gt; (a com.mysql.jdbc.util.ReadAheadInputStream)</div><div class="line">    at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3005)</div><div class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3466)</div><div class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3456)</div><div class="line">    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3897)</div><div class="line">    at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2524)</div><div class="line">    at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2677)</div><div class="line">    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2545)</div><div class="line">    - locked &lt;0x00000007432e19c8&gt; (a com.mysql.jdbc.JDBC4Connection)</div><div class="line">    at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2503)</div><div class="line">    at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1369)</div><div class="line">    - locked &lt;0x00000007432e19c8&gt; (a com.mysql.jdbc.JDBC4Connection)</div><div class="line">    at com.alibaba.druid.pool.ValidConnectionCheckerAdapter.isValidConnection(ValidConnectionCheckerAdapter.java:44)</div><div class="line">    at com.alibaba.druid.pool.DruidAbstractDataSource.testConnectionInternal(DruidAbstractDataSource.java:1298)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1057)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:997)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:987)</div><div class="line">    at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:103)</div><div class="line">    at com.taobao.tddl.atom.AbstractTAtomDataSource.getConnection(AbstractTAtomDataSource.java:32)</div><div class="line">    at com.alibaba.cobar.ClusterSyncManager$1.run(ClusterSyncManager.java:60)</div><div class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">    at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">    at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)</div><div class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)</div><div class="line">    at java.lang.Thread.run(Thread.java:882)</div></pre></td></tr></table></figure>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ol>
<li>用户监控采集数据通过访问3306端口上的show stats，这个show stats命令要访问集群下所有节点的3406端口来执行show stats，3406端口上是一个大小为8个的Manager 线程池在执行这些show stats命令，导致占满了manager线程池的8个线程，每个3306的show stats线程都在wait 所有节点3406上的子任务的返回</li>
<li>每个子任务的线程，都在等待向集群所有节点3406端口的manager建立连接，建连接后会先执行testValidatation操作验证连接的有效性，这个验证操作会执行SQL Query：select 1，这个query请求又要申请一个manager线程才能执行成功</li>
<li>默认isValidConnection操作没有超时时间，如果Manager线程池已满后需要等待至socketTimeout后才会返回，导致这里出现卡死，还不如快速返回错误，可以增加超时来改进</li>
</ol>
<p>从线程栈来说，就是出现了活锁</p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul>
<li>增加manager线程池大小</li>
<li>代码逻辑上优化3406 jdbc连接池参数，修改jdbc默认的socketTimeout超时时间以及替换默认checker（一般增加一个1秒超时的checker）</li>
</ul>
<p>对于checker，参考druid的实现，com/alibaba/druid/pool/vendor/MySqlValidConnectionChecker.java：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//druid的MySqlValidConnectionChecker设定了valid超时时间为1秒</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValidConnection</span><span class="params">(Connection conn, String validateQuery, <span class="keyword">int</span> validationQueryTimeout)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (conn.isClosed()) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (usePingMethod) &#123;</div><div class="line">            <span class="keyword">if</span> (conn <span class="keyword">instanceof</span> DruidPooledConnection) &#123;</div><div class="line">                conn = ((DruidPooledConnection) conn).getConnection();</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (conn <span class="keyword">instanceof</span> ConnectionProxy) &#123;</div><div class="line">                conn = ((ConnectionProxy) conn).getRawObject();</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (clazz.isAssignableFrom(conn.getClass())) &#123;</div><div class="line">                <span class="keyword">if</span> (validationQueryTimeout &lt;= <span class="number">0</span>) &#123;</div><div class="line">                    validationQueryTimeout = DEFAULT_VALIDATION_QUERY_TIMEOUT;<span class="comment">// 默认值1ms</span></div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    ping.invoke(conn, <span class="keyword">true</span>, validationQueryTimeout * <span class="number">1000</span>); <span class="comment">//1秒</span></div><div class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</div><div class="line">                    Throwable cause = e.getCause();</div><div class="line">                    <span class="keyword">if</span> (cause <span class="keyword">instanceof</span> SQLException) &#123;</div><div class="line">                        <span class="keyword">throw</span> (SQLException) cause;</div><div class="line">                    &#125;</div><div class="line">                    <span class="keyword">throw</span> e;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String query = validateQuery;</div><div class="line">        <span class="keyword">if</span> (validateQuery == <span class="keyword">null</span> || validateQuery.isEmpty()) &#123;</div><div class="line">            query = DEFAULT_VALIDATION_QUERY;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Statement stmt = <span class="keyword">null</span>;</div><div class="line">        ResultSet rs = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            stmt = conn.createStatement();</div><div class="line">            <span class="keyword">if</span> (validationQueryTimeout &gt; <span class="number">0</span>) &#123;</div><div class="line">                stmt.setQueryTimeout(validationQueryTimeout);</div><div class="line">            &#125;</div><div class="line">            rs = stmt.executeQuery(query);</div><div class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">            JdbcUtils.close(rs);</div><div class="line">            JdbcUtils.close(stmt);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line"><span class="comment">//使用如上validation</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String DEFAULT_DRUID_MYSQL_VALID_CONNECTION_CHECKERCLASS =</div><div class="line">        <span class="string">"com.alibaba.druid.pool.vendor.MySqlValidConnectionChecker"</span>;</div><div class="line"></div><div class="line"> String validConnnectionCheckerClassName =</div><div class="line">                    TAtomConstants.DEFAULT_DRUID_MYSQL_VALID_CONNECTION_CHECKERCLASS;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    Class.forName(validConnnectionCheckerClassName);</div><div class="line">                    localDruidDataSource.setValidConnectionCheckerClassName(validConnnectionCheckerClassName);</div></pre></td></tr></table></figure>
<p>这种线程池打满特别容易在分布式环境下出现，除了以上案例比如还有:</p>
<blockquote>
<p>drds-server线程池，接收一个逻辑SQL，如果需要查询1024分片的sort merge join，相当于派生了一批子任务，每个子任务占用一个线程，父任务等待子任务执行后返回数据。如果这样的逻辑SQL同时来一批并发，就会出现父任务都在等子任务，子任务又因为父任务占用了线程，导致子任务也在等着从线程池中取线程，这样父子任务就进入了死锁</p>
<p>比如并行执行的SQL MPP线程池也有这个问题，多个查询节点收到SQL，拆分出子任务做并行，互相等待资源</p>
</blockquote>
<h2 id="X应用对分布式任务打挂线程池的优化"><a href="#X应用对分布式任务打挂线程池的优化" class="headerlink" title="X应用对分布式任务打挂线程池的优化"></a>X应用对分布式任务打挂线程池的优化</h2><p>对如下这种案例：</p>
<blockquote>
<p>X应用通过线程池来接收一个逻辑SQL并处理，如果需要查询1024分片的sort merge join，相当于派生了1024个子任务，每个子任务占用一个线程，父任务等待子任务执行后返回数据。如果这样的逻辑SQL同时来一批并发，就会出现父任务都在等子任务，子任务又因为父任务占用了线程，导致子任务也在等着从线程池中取线程，这样父子任务就进入了死锁</p>
</blockquote>
<p>首先X对执行SQL 的线程池分成了多个bucket，每个SQL只跑在一个bucket里面的线程上，同时通过滑动窗口向线程池提交任务数，来控制并发量，进而避免线程池的死锁、活锁问题。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ServerThreadPool <span class="title">create</span><span class="params">(String name, <span class="keyword">int</span> poolSize, <span class="keyword">int</span> deadLockCheckPeriod, <span class="keyword">int</span> bucketSize)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ServerThreadPool(name, poolSize, deadLockCheckPeriod, bucketSize); <span class="comment">//bucketSize可以设置</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ServerThreadPool</span><span class="params">(String poolName, <span class="keyword">int</span> poolSize, <span class="keyword">int</span> deadLockCheckPeriod, <span class="keyword">int</span> bucketSize)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.poolName = poolName;</div><div class="line">    <span class="keyword">this</span>.deadLockCheckPeriod = deadLockCheckPeriod;</div><div class="line"></div><div class="line">    <span class="keyword">this</span>.numBuckets = bucketSize;</div><div class="line">    <span class="keyword">this</span>.executorBuckets = <span class="keyword">new</span> ThreadPoolExecutor[bucketSize];</div><div class="line">    <span class="keyword">int</span> bucketPoolSize = poolSize / bucketSize; <span class="comment">//将整个pool分成多个bucket</span></div><div class="line">    <span class="keyword">this</span>.poolSize = bucketPoolSize;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bucketIndex = <span class="number">0</span>; bucketIndex &lt; bucketSize; bucketIndex++) &#123;</div><div class="line">        ThreadPoolExecutor executor = <span class="keyword">new</span> ThreadPoolExecutor(bucketPoolSize,</div><div class="line">            bucketPoolSize,</div><div class="line">            <span class="number">0L</span>,</div><div class="line">            TimeUnit.MILLISECONDS,</div><div class="line">            <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;(),</div><div class="line">            <span class="keyword">new</span> NamedThreadFactory(poolName + <span class="string">"-bucket-"</span> + bucketIndex, <span class="keyword">true</span>));</div><div class="line"></div><div class="line">        executorBuckets[bucketIndex] = executor;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">this</span>.lastCompletedTaskCountBuckets = <span class="keyword">new</span> <span class="keyword">long</span>[bucketSize];</div><div class="line">    <span class="comment">// for check thread</span></div><div class="line">    <span class="keyword">if</span> (deadLockCheckPeriod &gt; <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">this</span>.timer = <span class="keyword">new</span> Timer(SERVER_THREAD_POOL_TIME_CHECK, <span class="keyword">true</span>);</div><div class="line">        buildCheckTask();</div><div class="line">        <span class="keyword">this</span>.timer.scheduleAtFixedRate(checkTask, deadLockCheckPeriod, deadLockCheckPeriod);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>通过bucketSize将一个大的线程池分成多个小的线程池，每个SQL 控制跑在一个小的线程池中，这里和MySQL的thread_pool是同样的设计思路，当然MySQL 的thread_pool主要是为了改进大锁的问题。</p>
<p>另外DRDS上线程池拆分后性能也有提升：</p>
<p><img src="/images/951413iMgBlog/image-20211104163732499.png" alt="image-20211104163732499"></p>
<p>测试结果说明：(以全局线程池为基准，分别关注：关日志、分桶线程池、协程)</p>
<blockquote>
<ol>
<li>关日志，整体性能提升在20%左右 (8core最好成绩在6.4w qps)</li>
<li>协程，整体性能15%左右</li>
<li>关日志+协程，整体提升在35%左右 (8core最好成绩在7w qps)</li>
<li>分桶，整体性能提升在18%左右 </li>
<li>分桶+关日志，整体提升在39%左右 (8core最好成绩在7.4w qps)</li>
<li>分桶+协程，整体提升在36%左右</li>
<li>分桶+关日志+协程，整体提升在60%左右 (8core最好成绩在8.3w qps)</li>
</ol>
</blockquote>
<h3 id="线程池拆成多个bucket优化分析"><a href="#线程池拆成多个bucket优化分析" class="headerlink" title="线程池拆成多个bucket优化分析"></a>线程池拆成多个bucket优化分析</h3><p>拆分前锁主要是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">Started [lock] profiling</div><div class="line">--- Execution profile ---</div><div class="line">Total samples:         <span class="number">496</span></div><div class="line"></div><div class="line">Frame buffer usage:    <span class="number">0.0052</span>%</div><div class="line"></div><div class="line">--- <span class="number">352227700</span> ns (<span class="number">53.09</span>%), <span class="number">248</span> samples</div><div class="line">  [ <span class="number">0</span>] java.util.Properties</div><div class="line">  [ <span class="number">1</span>] java.util.Hashtable.get</div><div class="line">  [ <span class="number">2</span>] java.util.Properties.getProperty</div><div class="line">  [ <span class="number">3</span>] com.taobao.tddl.common.properties.SystemPropertiesHelper.getPropertyValue</div><div class="line">  [ <span class="number">4</span>] com.taobao.tddl.executor.MatrixExecutor.configMppExecutionContext</div><div class="line">  [ <span class="number">5</span>] com.taobao.tddl.executor.MatrixExecutor.optimize</div><div class="line">  [ <span class="number">6</span>] com.taobao.tddl.matrix.jdbc.TConnection.optimizeThenExecute</div><div class="line">  [ <span class="number">7</span>] com.taobao.tddl.matrix.jdbc.TConnection.executeSQL</div><div class="line">  [ <span class="number">8</span>] com.taobao.tddl.matrix.jdbc.TPreparedStatement.executeSQL</div><div class="line">  [ <span class="number">9</span>] com.taobao.tddl.matrix.jdbc.TStatement.executeInternal</div><div class="line">  [<span class="number">10</span>] com.taobao.tddl.matrix.jdbc.TPreparedStatement.execute</div><div class="line">  [<span class="number">11</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [<span class="number">12</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [<span class="number">13</span>] com.alibaba.cobar.server.ServerConnection$<span class="number">1</span>.run</div><div class="line">  [<span class="number">14</span>] com.taobao.tddl.common.utils.thread.FlowControlThreadPool$RunnableAdapter.run</div><div class="line">  [<span class="number">15</span>] java.util.concurrent.Executors$RunnableAdapter.call</div><div class="line">  [<span class="number">16</span>] java.util.concurrent.FutureTask.run</div><div class="line">  [<span class="number">17</span>] java.util.concurrent.ThreadPoolExecutor.runWorker</div><div class="line">  [<span class="number">18</span>] java.util.concurrent.ThreadPoolExecutor$Worker.run</div><div class="line">  [<span class="number">19</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">--- <span class="number">307781689</span> ns (<span class="number">46.39</span>%), <span class="number">243</span> samples</div><div class="line">  [ <span class="number">0</span>] java.util.Properties</div><div class="line">  [ <span class="number">1</span>] java.util.Hashtable.get</div><div class="line">  [ <span class="number">2</span>] java.util.Properties.getProperty</div><div class="line">  [ <span class="number">3</span>] com.taobao.tddl.common.properties.SystemPropertiesHelper.getPropertyValue</div><div class="line">  [ <span class="number">4</span>] com.taobao.tddl.config.ConfigDataMode.isDrdsMasterMode</div><div class="line">  [ <span class="number">5</span>] com.taobao.tddl.matrix.jdbc.TConnection.updatePlanManagementInfo</div><div class="line">  [ <span class="number">6</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [ <span class="number">7</span>] com.alibaba.cobar.server.ServerConnection.innerExecute</div><div class="line">  [ <span class="number">8</span>] com.alibaba.cobar.server.ServerConnection$<span class="number">1</span>.run</div><div class="line">  [ <span class="number">9</span>] com.taobao.tddl.common.utils.thread.FlowControlThreadPool$RunnableAdapter.run</div><div class="line">  [<span class="number">10</span>] java.util.concurrent.Executors$RunnableAdapter.call</div><div class="line">  [<span class="number">11</span>] java.util.concurrent.FutureTask.run</div><div class="line">  [<span class="number">12</span>] java.util.concurrent.ThreadPoolExecutor.runWorker</div><div class="line">  [<span class="number">13</span>] java.util.concurrent.ThreadPoolExecutor$Worker.run</div><div class="line">  [<span class="number">14</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">--- <span class="number">3451038</span> ns (<span class="number">0.52</span>%), <span class="number">4</span> samples</div><div class="line">  [ <span class="number">0</span>] java.lang.Object</div><div class="line">  [ <span class="number">1</span>] sun.nio.ch.SocketChannelImpl.ensureReadOpen</div><div class="line">  [ <span class="number">2</span>] sun.nio.ch.SocketChannelImpl.read</div><div class="line">  [ <span class="number">3</span>] com.alibaba.cobar.net.AbstractConnection.read</div><div class="line">  [ <span class="number">4</span>] com.alibaba.cobar.net.NIOReactor$R.read</div><div class="line">  [ <span class="number">5</span>] com.alibaba.cobar.net.NIOReactor$R.run</div><div class="line">  [ <span class="number">6</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">--- <span class="number">4143</span> ns (<span class="number">0.00</span>%), <span class="number">1</span> sample</div><div class="line">  [ <span class="number">0</span>] com.taobao.tddl.common.IdGenerator</div><div class="line">  [ <span class="number">1</span>] com.taobao.tddl.common.IdGenerator.nextId</div><div class="line">  [ <span class="number">2</span>] com.alibaba.cobar.server.ServerConnection.genTraceId</div><div class="line">  [ <span class="number">3</span>] com.alibaba.cobar.server.ServerQueryHandler.query</div><div class="line">  [ <span class="number">4</span>] com.alibaba.cobar.net.FrontendConnection.query</div><div class="line">  [ <span class="number">5</span>] com.alibaba.cobar.net.handler.FrontendCommandHandler.handle</div><div class="line">  [ <span class="number">6</span>] com.alibaba.cobar.net.FrontendConnection$<span class="number">1</span>.run</div><div class="line">  [ <span class="number">7</span>] java.util.concurrent.ThreadPoolExecutor.runWorker</div><div class="line">  [ <span class="number">8</span>] java.util.concurrent.ThreadPoolExecutor$Worker.run</div><div class="line">  [ <span class="number">9</span>] java.lang.Thread.run</div><div class="line"></div><div class="line">          ns  percent  samples  top</div><div class="line">  ----------  -------  -------  ---</div><div class="line">   <span class="number">660009389</span>   <span class="number">99.48</span>%      <span class="number">491</span>  java.util.Properties</div><div class="line">     <span class="number">3451038</span>    <span class="number">0.52</span>%        <span class="number">4</span>  java.lang.Object</div><div class="line">        <span class="number">4143</span>    <span class="number">0.00</span>%        <span class="number">1</span>  com.taobao.tddl.common.IdGenerator</div></pre></td></tr></table></figure>
<p>com.taobao.tddl.matrix.jdbc.TConnection.optimizeThenExecute调用对应代码逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (InsertSplitter.needSplit(sql, policy, extraCmd)) &#123;</div><div class="line">             executionContext.setDoingBatchInsertBySpliter(<span class="keyword">true</span>);</div><div class="line">             InsertSplitter insertSplitter = <span class="keyword">new</span> InsertSplitter(executor);</div><div class="line">                        <span class="comment">// In batch insert, update transaction policy in writing</span></div><div class="line">                        <span class="comment">// broadcast table is also needed.</span></div><div class="line">     resultCursor = insertSplitter.execute(sql,executionContext,policy,</div><div class="line">         (String insertSql) -&gt; optimizeThenExecute(insertSql, executionContext,trxPolicyModified));</div><div class="line">                    &#125; <span class="keyword">else</span> &#123;</div><div class="line">       resultCursor = optimizeThenExecute(sql, executionContext,trxPolicyModified);</div><div class="line">                    &#125;</div><div class="line">                    </div><div class="line"> 最终会访问到：</div><div class="line"> <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configMppExecutionContext</span><span class="params">(ExecutionContext executionContext)</span> </span>&#123;</div><div class="line"></div><div class="line">        String instRole = (String) SystemPropertiesHelper.getPropertyValue(SystemPropertiesHelper.INST_ROLE);</div><div class="line">        SqlType sqlType = executionContext.getSqlType();</div><div class="line">        </div><div class="line"> 相当于执行每个SQL都要加锁访问HashMap(SystemPropertiesHelper.getPropertyValue)，这里排队比较厉害</div></pre></td></tr></table></figure>
<p>实际以上测试结果显示bucket对性能有提升这么大是不对的，刚好这个版本把对HashMap的访问去掉了，这才是提升的主要原因，当然如果线程池入队出队有等锁的话改成多个肯定是有帮助的，但是从等锁观察是没有这个问题的。</p>
<p>在这个代码基础上将bucket改成1，在4core机器下经过反复对比测试性能基本没有明显的差异，可能core越多这个问题会更明显些。总结</p>
<p>回到最开始部分查询卡顿这个问题，本质在于 MySQL线程池开启后，因为会将多个连接分配在一个池子中共享这个池子中的几个线程。导致一个池子中的线程特别慢的时候会影响这个池子中所有的查询都会卡顿。即使别的池子很空闲也不会将任务调度过去。</p>
<p>MySQL线程池设计成多个池子（Group）的原因是为了将任务队列拆成多个，这样每个池子中的线程只是内部竞争锁，跟其他池子不冲突，类似ConcurrentHashmap的实现，当然这个设计带来的问题就是多个池子中的任务不能均衡了。</p>
<p>同时从案例我们也可以清楚地看到这个池子太小会造成锁冲突严重的卡顿，池子太大（每个池子中的线程数量就少）容易造成等线程的卡顿。</p>
<p><strong>类似地这个问题也会出现在Nginx的多worker中，一旦一个连接分发到了某个worker，就会一直在这个worker上处理，如果这个worker上的某个连接有一些慢操作，会导致这个worker上的其它连接的所有操作都受到影响，特别是会影响一些探活任务的误判。</strong>Nginx的worker这么设计也是为了将单worker绑定到固定的cpu，然后避免多核之间的上下文切换。</p>
<p>如果池子卡顿后，调用方有快速fail，比如druid的MySqlValidConnectionChecker，那么调用方从堆栈很快能发现这个问题，如果没有异常一直死等的话对问题的排查不是很友好。</p>
<p>另外可以看到分布式环境下死锁、活锁还是很容易产生的，想要一次性提前设计好比较难，需要不断踩坑爬坑。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.atatech.org/articles/36343" target="_blank" rel="external">记一次诡异的数据库故障的排查过程</a></p>
<p><a href="http://mysql.taobao.org/monthly/2016/02/09/" target="_blank" rel="external">http://mysql.taobao.org/monthly/2016/02/09/</a></p>
<p><a href="https://dbaplus.cn/news-11-1989-1.html" target="_blank" rel="external">https://dbaplus.cn/news-11-1989-1.html</a></p>
<p><a href="https://kb.aliyun-inc.com/repo/921/article?id=G71264" target="_blank" rel="external">慢查询触发kill后导致集群卡死</a></p>
<p><a href="https://kb.aliyun-inc.com/repo/921/article?id=G56753" target="_blank" rel="external">青海湖、天津医保 RDS线程池过小导致DRDS查询卡顿问题排查 </a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--pagecache/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--pagecache/" itemprop="url">Linux内存--PageCache</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–PageCache"><a href="#Linux内存–PageCache" class="headerlink" title="Linux内存–PageCache"></a>Linux内存–PageCache</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="read-write"><a href="#read-write" class="headerlink" title="read/write"></a>read/write</h2><p><code>read(2)/write(2)</code> 是 Linux 系统中最基本的 I/O 读写系统调用，我们开发操作 I/O 的程序时必定会接触到它们，而在这两个系统调用和真实的磁盘读写之间存在一层称为 <code>Kernel buffer cache</code> 的缓冲区缓存。在 Linux 中 I/O 缓存其实可以细分为两个：<code>Page Cache</code> 和 <code>Buffer Cache</code>，这两个其实是一体两面，共同组成了 Linux 的内核缓冲区（Kernel Buffer Cache），Page Cache 是在应用程序读写文件的过程中产生的：</p>
<ul>
<li><strong>读磁盘</strong>：内核会先检查 <code>Page Cache</code> 里是不是已经缓存了这个数据，若是，直接从这个内存缓冲区里读取返回，若否，则穿透到磁盘去读取，然后再缓存在 <code>Page Cache</code> 里，以备下次缓存命中；</li>
<li><strong>写磁盘</strong>：内核直接把数据写入 <code>Page Cache</code>，并把对应的页标记为 dirty，添加到 dirty list 里，然后就直接返回，内核会定期把 dirty list 的页缓存 flush 到磁盘，保证页缓存和磁盘的最终一致性。</li>
</ul>
<p>在 Linux 还不支持虚拟内存技术之前，还没有页的概念，因此 <code>Buffer Cache</code> 是基于操作系统读写磁盘的最小单位 – 块（block）来进行的，所有的磁盘块操作都是通过 <code>Buffer Cache</code> 来加速，<strong>Linux 引入虚拟内存的机制来管理内存后，页成为虚拟内存管理的最小单位</strong>，因此也引入了 <code>Page Cache</code> 来缓存 Linux 文件内容，主要用来作为文件系统上的文件数据的缓存，提升读写性能，常见的是针对文件的 <code>read()/write()</code> 操作，另外也包括了通过 <code>mmap()</code> 映射之后的块设备，也就是说，事实上 Page Cache 负责了大部分的块设备文件的缓存工作。而 <code>Buffer Cache</code> 用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。</p>
<p>在 Linux 2.4 版本之后，kernel 就将两者进行了统一，<code>Buffer Cache</code> 不再以独立的形式存在，而是以融合的方式存在于 <code>Page Cache</code> 中</p>
<p><img src="/images/oss/cd1b3a9bebaf1e7219904fd537191cde.png" alt=""></p>
<p>融合之后就可以统一操作 <code>Page Cache</code> 和 <code>Buffer Cache</code>：处理文件 I/O 缓存交给 <code>Page Cache</code>，而当底层 RAW device 刷新数据时以 <code>Buffer Cache</code> 的块单位来实际处理。</p>
<h2 id="pagecache-的产生和释放"><a href="#pagecache-的产生和释放" class="headerlink" title="pagecache 的产生和释放"></a>pagecache 的产生和释放</h2><ul>
<li>标准 I/O 是写的 (write(2)) 用户缓冲区 (Userpace Page 对应的内存)，<strong>然后再将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)</strong>；如果是读的 (read(2)) 话则是先从内核缓冲区拷贝到用户缓冲区，再从用户缓冲区读数据，也就是 buffer 和文件内容不存在任何映射关系。</li>
<li>对于存储映射 I/O（Memory-Mapped I/O） 而言，则是直接将 Pagecache Page 给映射到用户地址空间，用户直接读写 Pagecache Page 中内容，效率相对标准IO更高一些</li>
</ul>
<p><img src="/images/oss/51bf36aa14dc01e7ad309c1bb9d252e9.png" alt="image.png" style="zoom: 20%;"></p>
<p>当 <strong>将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)</strong> 最容易发生缺页中断，OS需要先分配Page（应用感知到的就是卡顿了）</p>
<p><img src="/images/oss/d62ea00662f8342b7df3aab6b28e4cbb.png" alt="img.png" style="zoom: 25%;">  </p>
<ul>
<li>Page Cache 是在应用程序读写文件的过程中产生的，所以在读写文件之前你需要留意是否还有足够的内存来分配 Page Cache；</li>
<li>Page Cache 中的脏页很容易引起问题，你要重点注意这一块；</li>
<li>在系统可用内存不足的时候就会回收 Page Cache 来释放出来内存，可以通过 sar 或者 /proc/vmstat 来观察这个行为从而更好的判断问题是否跟回收有关</li>
</ul>
<p>缺页后kswapd在短时间内回收不了足够多的 free 内存，或kswapd 还没有触发执行，操作系统就会进行内存页直接回收。这个过程中，应用会进行自旋等待直到回收的完成，从而产生巨大的延迟。</p>
<p><img src="/images/oss/0a5cdeb75b7dee2068254cd4b7fe254d.png" style="zoom:50%;"></p>
<p>如果page被swapped，那么恢复进内存的过程也对延迟有影响，当匿名内存页被回收后，如果下次再访问就会产生IO的延迟。</p>
<p><img src="/images/oss/740b95056dace8ae6fb3b8f58d91572e.png" style="zoom:50%;"></p>
<h3 id="min-和-low的区别"><a href="#min-和-low的区别" class="headerlink" title="min 和 low的区别"></a>min 和 low的区别</h3><ol>
<li>min下的内存是保留给内核使用的；当到达min，会触发内存的direct reclaim （vm.min_free_kbytes）</li>
<li>low水位比min高一些，当内存可用量小于low的时候，会触发 kswapd回收内存，当kswapd慢慢的将内存 回收到high水位，就开始继续睡眠 </li>
</ol>
<h3 id="内存回收方式"><a href="#内存回收方式" class="headerlink" title="内存回收方式"></a>内存回收方式</h3><p>内存回收方式有两种，主要对应low ，min</p>
<ol>
<li>kswapd reclaim : 达到low水位线时执行 – 异步（实际还有，只是比较危险了，后台kswapd会回收，不会卡顿应用）</li>
<li>direct reclaim : 达到min水位线时执行 – 同步</li>
</ol>
<p>为了减少缺页中断，首先就要保证我们有足够的内存可以使用。由于Linux会尽可能多的使用free的内存，运行很久的应用free的内存是很少的。下面的图中，紫色表示已经使用的内存，白色表示尚未分配的内存。当我们的内存使用达到水位的low值的时候，kswapd就会开始回收工作，而一旦内存分配超过了min，就会进行内存的直接回收。</p>
<p><img src="/images/oss/5933cc4c28f86aa08410a8af4ff4410d.png" style="zoom:50%;"></p>
<p>针对这种情况，需要采用预留内存的手段，系统参数vm.extra_free_kbytes就是用来做这个事情的。这个参数设置了系统预留给应用的内存，可以避免紧急需要内存时发生内存回收不及时导致的高延迟。从下面图中可以看到，通过vm.extra_free_kbytes的设置，预留内存可以让内存的申请处在一个安全的水位。<strong>需要注意的是，因为内核的优化，在3.10以上的内核版本这个参数已经被取消。</strong></p>
<p><img src="/images/oss/f55022d4eb181b92ba5d2e142ec940c8.png" style="zoom: 50%;"></p>
<p>三个watermark的计算方法：</p>
<p>watermark[min] = vm.min_free_kbytes换算为page单位即可，假设为vm.min_free_kbytes。</p>
<p>watermark[low] = watermark[min] * 5 / 4</p>
<p>watermark[high] = watermark[min] * 3 / 2</p>
<p>比如默认 vm.min_free_kbytes = 65536是64K，很容易导致应用的毛刺，可以适当改大</p>
<p>或者禁止： vm.swappiness  来避免swapped来减少延迟</p>
<h3 id="direct-IO"><a href="#direct-IO" class="headerlink" title="direct IO"></a>direct IO</h3><p>绕过page cache，直接读写硬盘</p>
<h2 id="cache回收"><a href="#cache回收" class="headerlink" title="cache回收"></a>cache回收</h2><p>系统内存大体可分为三块，应用程序使用内存、系统Cache 使用内存（包括page cache、buffer，内核slab 等）和Free 内存。</p>
<ul>
<li>应用程序使用内存：应用使用都是虚拟内存，应用申请内存时只是分配了地址空间，并未真正分配出物理内存，等到应用真正访问内存时会触发内核的缺页中断，这时候才真正的分配出物理内存，映射到用户的地址空间，因此应用使用内存是不需要连续的，内核有机制将非连续的物理映射到连续的进程地址空间中（mmu），缺页中断申请的物理内存，内核优先给低阶碎内存。</li>
<li><p>系统Cache 使用内存：使用的也是虚拟内存，申请机制与应用程序相同。</p>
</li>
<li><p>Free 内存，未被使用的物理内存，这部分内存以4k 页的形式被管理在内核伙伴算法结构中，相邻的2^n 个物理页会被伙伴算法组织到一起，形成一块连续物理内存，所谓的阶内存就是这里的n (0&lt;= n &lt;=10)，高阶内存指的就是一块连续的物理内存，在OSS 的场景中，如果3阶内存个数比较小的情况下，如果系统有吞吐burst 就会触发Drop cache 情况。</p>
</li>
</ul>
<blockquote>
<p>echo 1/2/3 &gt;/proc/sys/vm/drop_caches</p>
</blockquote>
<p>查看回收后：</p>
<pre><code>cat /proc/meminfo
</code></pre><p><img src="/images/oss/7cedcb6daa53cbcfc9c68568086500b7.png" alt="image.png" style="zoom:20%;"></p>
<p>当我们执行 echo 2 来 drop slab 的时候，它也会把 Page Cache(inode可能会有对应的pagecache，inode释放后对应的pagecache也释放了)给 drop 掉</p>
<p>在系统内存紧张的时候，运维人员或者开发人员会想要通过 drop_caches 的方式来释放一些内存，但是由于他们清楚 Page Cache 被释放掉会影响业务性能，所以就期望只去 drop slab 而不去 drop pagecache。于是很多人这个时候就运行 echo 2 &gt; /proc/sys/vm/drop_caches，但是结果却出乎了他们的意料：Page Cache 也被释放掉了，业务性能产生了明显的下降。</p>
<p>查看 drop_caches 是否执行过释放：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ grep drop /proc/vmstat</div><div class="line">drop_pagecache 1</div><div class="line">drop_slab 0</div><div class="line"></div><div class="line">$ grep inodesteal /proc/vmstat </div><div class="line">pginodesteal 114341</div><div class="line">kswapd_inodesteal 1291853</div></pre></td></tr></table></figure>
<p>在内存紧张的时候会触发内存回收，内存回收会尝试去回收 reclaimable（可以被回收的）内存，这部分内存既包含 Page Cache 又包含 reclaimable kernel memory(比如 slab)。inode被回收后可以通过  grep inodesteal /proc/vmstat 观察到</p>
<blockquote>
<p>kswapd_inodesteal 是指在 kswapd 回收的过程中，因为回收 inode 而释放的 pagecache page 个数；</p>
<p>pginodesteal 是指 kswapd 之外其他线程在回收过程中，因为回收 inode 而释放的 pagecache page 个数;</p>
</blockquote>
<h2 id="Page回收–缺页中断"><a href="#Page回收–缺页中断" class="headerlink" title="Page回收–缺页中断"></a>Page回收–缺页中断</h2><p><img src="/images/oss/3fdffacd66c0981956b15be348fff46a.png" alt="image.png" style="zoom:20%;"></p>
<p>从图里你可以看到，在开始内存回收后，首先进行后台异步回收（上图中蓝色标记的地方），这不会引起进程的延迟；如果后台异步回收跟不上进程内存申请的速度，就会开始同步阻塞回收，导致延迟（上图中红色和粉色标记的地方，这就是引起 load 高的地址 – Sys CPU 使用率飙升/Sys load 飙升）。</p>
<p>那么，针对直接内存回收引起 load 飙高或者业务 RT 抖动的问题，一个解决方案就是及早地触发后台回收来避免应用程序进行直接内存回收，那具体要怎么做呢？</p>
<p><img src="/images/oss/4b341ba757d27e3a81145a55f54363e1.png" alt="image.png" style="zoom:25%;"></p>
<p>它的意思是：当内存水位低于 watermark low 时，就会唤醒 kswapd 进行后台回收，然后 kswapd 会一直回收到 watermark high。</p>
<p>那么，我们可以增大 min_free_kbytes 这个配置选项来及早地触发后台回收，该选项最终控制的是内存回收水位，不过，内存回收水位是内核里面非常细节性的知识点，我们可以先不去讨论。</p>
<p>对于大于等于 128G 的系统而言，将 min_free_kbytes 设置为 4G 比较合理，这是我们在处理很多这种问题时总结出来的一个经验值，既不造成较多的内存浪费，又能避免掉绝大多数的直接内存回收。</p>
<p>该值的设置和总的物理内存并没有一个严格对应的关系，我们在前面也说过，如果配置不当会引起一些副作用，所以在调整该值之前，我的建议是：你可以渐进式地增大该值，比如先调整为 1G，观察 sar -B 中 pgscand 是否还有不为 0 的情况；如果存在不为 0 的情况，继续增加到 2G，再次观察是否还有不为 0 的情况来决定是否增大，以此类推。</p>
<blockquote>
<p>sar -B :  Report paging statistics.</p>
<p>pgscand/s  Number of pages scanned directly per second.</p>
</blockquote>
<h3 id="系统中脏页过多引起-load-飙高"><a href="#系统中脏页过多引起-load-飙高" class="headerlink" title="系统中脏页过多引起 load 飙高"></a>系统中脏页过多引起 load 飙高</h3><p>直接回收过程中，如果存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟，而且因为这个过程本身是阻塞式的，所以又可能进一步导致系统中处于 D 状态的进程数增多，最终的表现就是系统的 load 值很高。</p>
<p><img src="/images/oss/f16438b744a248d7671d5ac7317b0a98.png" alt="image.png" style="zoom: 25%;"></p>
<p>可以通过 sar -r 来观察系统中的脏页个数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ sar -r 1</div><div class="line">07:30:01 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</div><div class="line">09:20:01 PM   5681588   2137312     27.34         0   1807432    193016      2.47    534416   1310876         4</div><div class="line">09:30:01 PM   5677564   2141336     27.39         0   1807500    204084      2.61    539192   1310884        20</div><div class="line">09:40:01 PM   5679516   2139384     27.36         0   1807508    196696      2.52    536528   1310888        20</div><div class="line">09:50:01 PM   5679548   2139352     27.36         0   1807516    196624      2.51    536152   1310892        24</div></pre></td></tr></table></figure>
<p>kbdirty 就是系统中的脏页大小，它同样也是对 /proc/vmstat 中 nr_dirty 的解析。你可以通过调小如下设置来将系统脏页个数控制在一个合理范围:</p>
<blockquote>
<p>vm.dirty_background_bytes = 0</p>
<p>vm.dirty_background_ratio = 10</p>
<p>vm.dirty_bytes = 0</p>
<p>vm.dirty_expire_centisecs = 3000</p>
<p>vm.dirty_ratio = 20</p>
</blockquote>
<p>至于这些值调整大多少比较合适，也是因系统和业务的不同而异，我的建议也是一边调整一边观察，将这些值调整到业务可以容忍的程度就可以了，即在调整后需要观察业务的服务质量 (SLA)，要确保 SLA 在可接受范围内。调整的效果可以通过 /proc/vmstat 来查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#grep &quot;nr_dirty_&quot; /proc/vmstat</div><div class="line">nr_dirty_threshold 3071708</div><div class="line">nr_dirty_background_threshold 1023902</div></pre></td></tr></table></figure>
<p>在4.20的内核并且sar 的版本为12.3.3可以看到PSI（Pressure-Stall Information）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">some avg10=45.49 avg60=10.23 avg300=5.41 total=76464318</div><div class="line">full avg10=40.87 avg60=9.05 avg300=4.29 total=58141082</div></pre></td></tr></table></figure>
<p>重点关注 avg10 这一列，它表示最近 10s 内存的平均压力情况，如果它很大（比如大于 40）那 load 飙高大概率是由于内存压力，尤其是 Page Cache 的压力引起的。</p>
<p><img src="/images/oss/cf58f10a523e1e4f0db443be3f54fc04.png" alt="image.png" style="zoom: 25%;"></p>
<h2 id="通过tracepoint分析内存卡顿问题"><a href="#通过tracepoint分析内存卡顿问题" class="headerlink" title="通过tracepoint分析内存卡顿问题"></a>通过tracepoint分析内存卡顿问题</h2><p><img src="/images/oss/d5446b656e8d91a9fb72200a7b97e723.png" alt="image.png" style="zoom:25%;"></p>
<p>我们继续以内存规整 (memory compaction) 为例，来看下如何利用 tracepoint 来对它进行观察：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#首先来使能compcation相关的一些tracepoing</div><div class="line">$ echo 1 &gt;</div><div class="line">/sys/kernel/debug/tracing/events/compaction/mm_compaction_begin/enable</div><div class="line">$ echo 1 &gt;</div><div class="line">/sys/kernel/debug/tracing/events/compaction/mm_compaction_end/enable </div><div class="line"></div><div class="line">#然后来读取信息，当compaction事件触发后就会有信息输出</div><div class="line">$ cat /sys/kernel/debug/tracing/trace_pipe</div><div class="line">           &lt;...&gt;-49355 [037] .... 1578020.975159: mm_compaction_begin: </div><div class="line">zone_start=0x2080000 migrate_pfn=0x2080000 free_pfn=0x3fe5800 </div><div class="line">zone_end=0x4080000, mode=async</div><div class="line">           &lt;...&gt;-49355 [037] .N.. 1578020.992136: mm_compaction_end: </div><div class="line">zone_start=0x2080000 migrate_pfn=0x208f420 free_pfn=0x3f4b720 </div><div class="line">zone_end=0x4080000, mode=async status=contended</div></pre></td></tr></table></figure>
<p>从这个例子中的信息里，我们可以看到是 49355 这个进程触发了 compaction，begin 和 end 这两个 tracepoint 触发的时间戳相减，就可以得到 compaction 给业务带来的延迟，我们可以计算出这一次的延迟为 17ms。</p>
<p>或者用 <a href="https://lore.kernel.org/linux-mm/20191001144524.GB3321@techsingularity.net/T/" target="_blank" rel="external">perf script</a> 脚本来分析, <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop.py" target="_blank" rel="external">基于 bcc(eBPF) 写的direct reclaim snoop</a>来观察进程因为 direct reclaim 而导致的延迟。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--HugePage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--HugePage/" itemprop="url">Linux内存--HugePage</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–HugePage"><a href="#Linux内存–HugePage" class="headerlink" title="Linux内存–HugePage"></a>Linux内存–HugePage</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="proc-buddyinfo"><a href="#proc-buddyinfo" class="headerlink" title="/proc/buddyinfo"></a>/proc/buddyinfo</h2><p>/proc/buddyinfo记录了内存的详细碎片情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#cat /proc/buddyinfo </div><div class="line">Node 0, zone      DMA      1      1      1      0      2      1      1      0      1      1      3 </div><div class="line">Node 0, zone    DMA32      2      5      3      6      2      0      4      4      2      2    404 </div><div class="line">Node 0, zone   Normal 243430 643847 357451  32531   9508   6159   3917   2960  17172   2633  22854</div></pre></td></tr></table></figure>
<p>Normal行的第二列表示：  643847*2^1*Page_Size(4K) ;  第三列表示：  357451*2^2*Page_Size(4K)  ，高阶内存指的是2^3及更大的内存块。</p>
<p>应用申请大块连续内存（高阶内存，一般之4阶及以上, 也就是64K以上–2^4*4K）时，容易导致卡顿。这是因为大块连续内存确实系统需要触发回收或者碎片整理，需要一定的时间。</p>
<h2 id="slabtop和-proc-slabinfo"><a href="#slabtop和-proc-slabinfo" class="headerlink" title="slabtop和/proc/slabinfo"></a>slabtop和/proc/slabinfo</h2><p>slabtop和/proc/slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<h2 id="关于hugetlb"><a href="#关于hugetlb" class="headerlink" title="关于hugetlb"></a>关于hugetlb</h2><p>This is an entry in the TLB that points to a HugePage (a large/big page larger than regular 4K and predefined in size). HugePages are implemented via hugetlb entries, i.e. we can say that a HugePage is handled by a “hugetlb page entry”. The ‘hugetlb” term is also (and mostly) used synonymously with a HugePage.</p>
<p> hugetlb 是TLB中指向HugePage的一个entry(通常大于4k或预定义页面大小)。 HugePage 通过hugetlb entries来实现，也可以理解为HugePage 是hugetlb page entry的一个句柄。</p>
<p><strong>Linux下的大页分为两种类型：标准大页（Huge Pages）和透明大页（Transparent Huge Pages）</strong></p>
<p>标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式</p>
<p>目前透明大页与传统HugePages联用会出现一些问题，导致性能问题和系统重启。Oracle 建议禁用透明大页（Transparent Huge Pages）</p>
<p>hugetlbfs比THP要好，开thp的机器碎片化严重（不开THP会有更严重的碎片化问题），最后和没开THP一样 <a href="https://www.atatech.org/articles/152660" target="_blank" rel="external">https://www.atatech.org/articles/152660</a></p>
<p>Linux 中的 HugePages 都被锁定在内存中，所以哪怕是在系统内存不足时，它们也不会被 Swap 到磁盘上，这也就能从根源上杜绝了重要内存被频繁换入和换出的可能。</p>
<blockquote>
<p><strong>Transparent Hugepages</strong> are similar to standard <strong>HugePages</strong>. However, while standard <strong>HugePages</strong> allocate memory at startup, <strong>Transparent Hugepages</strong> memory uses the khugepaged thread in the kernel to allocate memory dynamically during runtime, using swappable <strong>HugePages</strong>.</p>
</blockquote>
<p>HugePage要求OS启动的时候提前分配出来，管理难度比较大，所以Enterprise Linux 6增加了一层抽象层来动态创建管理HugePage，这就是THP，而这个THP对应用透明，由khugepaged thread在后台动态将小页组成大页给应用使用，这里会遇上碎片问题导致需要compact才能得到大页，应用感知到的就是SYS CPU飙高，应用卡顿了。</p>
<p>虽然 HugePages 的开启大都需要开发或者运维工程师的额外配置，但是在应用程序中启用 HugePages 却可以在以下几个方面降低内存页面的管理开销：</p>
<ul>
<li>更大的内存页能够减少内存中的页表层级，这不仅可以降低页表的内存占用，也能降低从虚拟内存到物理内存转换的性能损耗；</li>
<li>更大的内存页意味着更高的缓存命中率，CPU 有更高的几率可以直接在 TLB（Translation lookaside buffer）中获取对应的物理地址；</li>
<li>更大的内存页可以减少获取大内存的次数，使用 HugePages 每次可以获取 2MB 的内存，是 4KB 的默认页效率的 512 倍；</li>
</ul>
<h2 id="HugePage"><a href="#HugePage" class="headerlink" title="HugePage"></a>HugePage</h2><p><strong>为什么需要Huge Page</strong> 了解CPU Cache大致架构的话，一定听过TLB Cache。<code>Linux</code>系统中，对程序可见的，可使用的内存地址是<code>Virtual Address</code>。每个程序的内存地址都是从0开始的。而实际的数据访问是要通过<code>Physical Address</code>进行的。因此，每次内存操作，CPU都需要从<code>page table</code>中把<code>Virtual Address</code>翻译成对应的<code>Physical Address</code>，那么对于大量内存密集型程序来说<code>page table</code>的查找就会成为程序的瓶颈。</p>
<p>所以现代CPU中就出现了TLB(Translation Lookaside Buffer) Cache用于缓存少量热点内存地址的mapping关系。然而由于制造成本和工艺的限制，响应时间需要控制在CPU Cycle级别的Cache容量只能存储几十个对象。那么TLB Cache在应对大量热点数据<code>Virual Address</code>转换的时候就显得捉襟见肘了。我们来算下按照标准的Linux页大小(page size) 4K，一个能缓存64元素的TLB Cache只能涵盖<code>4K*64 = 256K</code>的热点数据的内存地址，显然离理想非常遥远的。于是Huge Page就产生了。</p>
<p>Huge pages require contiguous areas of memory, so allocating them at boot is the most reliable method since memory has not yet become fragmented. To do so, add the following parameters to the kernel boot command line:</p>
<p><strong>Huge pages kernel options</strong></p>
<ul>
<li><p>hugepages</p>
<p>Defines the number of persistent huge pages configured in the kernel at boot time. The default value is <code>0</code>. It is only possible to allocate (or deallocate) huge pages if there are sufficient physically contiguous free pages in the system. Pages reserved by this parameter cannot be used for other purposes.</p>
<p>Default size huge pages can be dynamically allocated or deallocated by changing the value of the <code>/proc/sys/vm/nr_hugepages</code> file.</p>
<p>In a NUMA system, huge pages assigned with this parameter are divided equally between nodes. You can assign huge pages to specific nodes at runtime by changing the value of the node’s <code>/sys/devices/system/node/node_id/hugepages/hugepages-1048576kB/nr_hugepages</code> file.</p>
<p>For more information, read the relevant kernel documentation, which is installed in <code>/usr/share/doc/kernel-doc-kernel_version/Documentation/vm/hugetlbpage.txt</code> by default. This documentation is available only if the <em>kernel-doc</em> package is installed.</p>
</li>
<li><p>hugepagesz</p>
<p>Defines the size of persistent huge pages configured in the kernel at boot time. Valid values are 2 MB and 1 GB. The default value is 2 MB.</p>
</li>
<li><p>default_hugepagesz</p>
<p>Defines the default size of persistent huge pages configured in the kernel at boot time. Valid values are 2 MB and 1 GB. The default value is 2 MB.</p>
</li>
</ul>
<p>应用程序想要利用大页优势，需要通过hugetlb文件系统来使用标准大页。<a href="https://ata.alibaba-inc.com/articles/208718" target="_blank" rel="external">操作步骤</a></p>
<h4 id="1-预留大页"><a href="#1-预留大页" class="headerlink" title="1.预留大页"></a>1.预留大页</h4><p>echo 20 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages</p>
<h4 id="2-挂载hugetlb文件系统"><a href="#2-挂载hugetlb文件系统" class="headerlink" title="2.挂载hugetlb文件系统"></a>2.挂载hugetlb文件系统</h4><p>mount hugetlbfs /mnt/huge -t hugetlbfs</p>
<h4 id="3-映射hugetbl文件"><a href="#3-映射hugetbl文件" class="headerlink" title="3.映射hugetbl文件"></a>3.映射hugetbl文件</h4><p>fd = open(“/mnt/huge/test.txt”, O_CREAT|O_RDWR);</p>
<p>addr = mmap(0, MAP_LENGTH, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);</p>
<h4 id="4-hugepage统计信息"><a href="#4-hugepage统计信息" class="headerlink" title="4.hugepage统计信息"></a>4.hugepage统计信息</h4><p>通过hugepage提供的sysfs接口，可以了解大页使用情况</p>
<p>HugePages_Total: 预先分配的大页数量</p>
<p>HugePages_Free：空闲大页数量</p>
<p>HugePages_Rsvd: mmap申请大页数量(还没有产生缺页)</p>
<p>HugePages_Surp: 多分配的大页数量(由nr_overcommit_hugepages决定)</p>
<h4 id="5-hugpage优缺点"><a href="#5-hugpage优缺点" class="headerlink" title="5 hugpage优缺点"></a>5 hugpage优缺点</h4><p>缺点:</p>
<p>1.需要提前预估大页使用量，并且预留的大页不能被其他内存分配接口使用。</p>
<p>2.兼容性不好，应用使用标准大页，需要对代码进行重构才能有效的使用标准大页。</p>
<p>优点:因为内存是预留的，缺页延时非常小</p>
<p>针对Hugepage的不足，内核又衍生出了THP大页(<strong>Transparent Huge pages)</strong></p>
<h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a><a href="https://www.golinuxcloud.com/configure-hugepages-vm-nr-hugepages-red-hat-7/" target="_blank" rel="external">工具</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">yum install libhugetlbfs-utils -y</div><div class="line"></div><div class="line">//列出</div><div class="line">hugeadm --pool-list</div><div class="line">      Size  Minimum  Current  Maximum  Default</div><div class="line">   2097152    12850    12850    12850        *</div><div class="line">1073741824        0        0        0</div><div class="line"></div><div class="line">hugeadm --list-all-mounts</div><div class="line">Mount Point          Options</div><div class="line">/dev/hugepages       rw,relatime,pagesize=2M</div></pre></td></tr></table></figure>
<h3 id="大页和-MySQL-性能-case"><a href="#大页和-MySQL-性能-case" class="headerlink" title="大页和 MySQL 性能 case"></a>大页和 MySQL 性能 case</h3><p>MySQL的页都是16K, 当查询的行不在内存中时需要按照16K为单位从磁盘读取页,而文件系统中的页是4k，也就是一次数据库请求需要有4次磁盘IO，如过查询比较随机，每次只需要一个页中的几行数据，存在很大的读放大。</p>
<p>那么我们是否可以把MySQL的页设置为4K来减少读放大呢？</p>
<p>在5.7里收益不大，因为每次IO存在 fil_system 的锁，导致IO的并发上不去</p>
<p>8.0中总算优化了这个场景，测试细节可以参考<a href="http://dimitrik.free.fr/blog/archives/2018/05/mysql-performance-1m-iobound-qps-with-80-ga-on-intel-optane-ssd.html" target="_blank" rel="external">这篇</a></p>
<p>16K VS 4K 性能对比（4K接近翻倍）</p>
<p><img src="/images/951413iMgBlog/1547605552845-d406952d-9857-462d-a666-1694b19fbedb.png" alt="img"></p>
<p>4K会带来的问题：顺序insert慢了10%（因为fsync更多了）；DDL更慢；二级索引更多的场景下4K性能较差；大BP下，刷脏代价大。</p>
<h3 id="HugePage-带来的问题"><a href="#HugePage-带来的问题" class="headerlink" title="HugePage 带来的问题"></a><a href="http://cenalulu.github.io/linux/huge-page-on-numa/" target="_blank" rel="external">HugePage 带来的问题</a></h3><h4 id="CPU对同一个Page抢占增多"><a href="#CPU对同一个Page抢占增多" class="headerlink" title="CPU对同一个Page抢占增多"></a>CPU对同一个Page抢占增多</h4><p>对于写操作密集型的应用，Huge Page会大大增加Cache写冲突的发生概率。由于CPU独立Cache部分的写一致性用的是<code>MESI协议</code>，写冲突就意味：</p>
<ul>
<li>通过CPU间的总线进行通讯，造成总线繁忙</li>
<li>同时也降低了CPU执行效率。</li>
<li>CPU本地Cache频繁失效</li>
</ul>
<p>类比到数据库就相当于，原来一把用来保护10行数据的锁，现在用来锁1000行数据了。必然这把锁在线程之间的争抢概率要大大增加。</p>
<h4 id="连续数据需要跨CPU读取"><a href="#连续数据需要跨CPU读取" class="headerlink" title="连续数据需要跨CPU读取"></a>连续数据需要跨CPU读取</h4><p>Page太大，更容易造成Page跨Numa/CPU 分布。</p>
<p>从下图我们可以看到，原本在4K小页上可以连续分配，并因为较高命中率而在同一个CPU上实现locality的数据。到了Huge Page的情况下，就有一部分数据为了填充统一程序中上次内存分配留下的空间，而被迫分布在了两个页上。而在所在Huge Page中占比较小的那部分数据，由于在计算CPU亲和力的时候权重小，自然就被附着到了其他CPU上。那么就会造成：本该以热点形式存在于CPU2 L1或者L2 Cache上的数据，不得不通过CPU inter-connect去remote CPU获取数据。 假设我们连续申明两个数组，<code>Array A</code>和<code>Array B</code>大小都是1536K。内存分配时由于第一个Page的2M没有用满，因此<code>Array B</code>就被拆成了两份，分割在了两个Page里。而由于内存的亲和配置，一个分配在Zone 0，而另一个在Zone 1。那么当某个线程需要访问Array B时就不得不通过代价较大的Inter-Connect去获取另外一部分数据。</p>
<p><img src="/images/951413iMgBlog/false_sharing.png" alt="img"></p>
<h3 id="Java进程开启HugePage"><a href="#Java进程开启HugePage" class="headerlink" title="Java进程开启HugePage"></a>Java进程开启HugePage</h3><p>从perf数据来看压满后tlab miss比较高，得想办法降低这个值</p>
<h4 id="修改JVM启动参数"><a href="#修改JVM启动参数" class="headerlink" title="修改JVM启动参数"></a>修改JVM启动参数</h4><p>JVM启动参数增加如下三个(-XX:LargePageSizeInBytes=2m, 这个一定要，有些资料没提这个，在我的JDK8.0环境必须要)：</p>
<blockquote>
<p>-XX:+UseLargePages -XX:LargePageSizeInBytes=2m -XX:+UseHugeTLBFS</p>
</blockquote>
<h4 id="修改机器系统配置"><a href="#修改机器系统配置" class="headerlink" title="修改机器系统配置"></a>修改机器系统配置</h4><p>设置HugePage的大小</p>
<blockquote>
<p>cat /proc/sys/vm/nr_hugepages</p>
</blockquote>
<p>nr_hugepages设置多大参考如下计算方法：</p>
<blockquote>
<p>If you are using the option <code>-XX:+UseSHM</code> or <code>-XX:+UseHugeTLBFS</code>, then specify the number of large pages. In the following example, 3 GB of a 4 GB system are reserved for large pages (assuming a large page size of 2048kB, then 3 GB = 3 <em> 1024 MB = 3072 MB = 3072 </em> 1024 kB = 3145728 kB and 3145728 kB / 2048 kB = 1536):</p>
<p>echo 1536 &gt; /proc/sys/vm/nr_hugepages </p>
</blockquote>
<p>透明大页是没有办法减少系统tlab，tlab是对应于进程的，系统分给进程的透明大页还是由物理上的4K page组成。</p>
<p>对于c++来说，他malloc经常会散落得全地址都是，因为会触发各种mmap，冷热区域。所以THP和hugepage都可能导致大量内存被浪费了，进而导致内存紧张，性能下滑。jvm的连续内存布局，加上gc会使得内存密度很紧凑。THP的问题是，他是逻辑页，不是物理页，tlb依旧要N份，所以他的收益来自page fault减少，是一次性的收益。</p>
<p>hugepage的在减少page_fault上和thp效果一样第二个作用是，他只需要一份TLB了，hugepage是真正的大页内存，thp是逻辑上的，物理上还是需要很多小的page。</p>
<p><strong>如果TLB miss，则可能需要额外三次内存读取操作才能将线性地址翻译为物理地址。</strong></p>
<h2 id="THP"><a href="#THP" class="headerlink" title="THP"></a>THP</h2><p>Linux kernel在2.6.38内核增加了Transparent Huge Pages (THP)特性 ，支持大内存页(2MB)分配，默认开启。当开启时可以降低fork子进程的速度，但fork之后，每个内存页从原来4KB变为2MB，会大幅增加重写期间父进程内存消耗。同时<strong>每次写命令引起的复制内存页单位放大了512倍</strong>，会拖慢写操作的执行时间，导致大量写操作慢查询。例如简单的incr命令也会出现在慢查询中。因此Redis日志中建议将此特性进行禁用。  </p>
<p>THP 的目的是用一个页表项来映射更大的内存（大页），这样可以减少 Page Fault，因为需要的页数少了。当然，这也会提升 TLB（Translation Lookaside Buffer，由存储器管理单元用于改进虚拟地址到物理地址的转译速度） 命中率，因为需要的页表项也少了。如果进程要访问的数据都在这个大页中，那么这个大页就会很热，会被缓存在 Cache 中。而大页对应的页表项也会出现在 TLB 中，从上一讲的存储层次我们可以知道，这有助于性能提升。但是反过来，假设应用程序的数据局部性比较差，它在短时间内要访问的数据很随机地位于不同的大页上，那么大页的优势就会消失。</p>
<h4 id="THP-原理"><a href="#THP-原理" class="headerlink" title="THP 原理"></a>THP 原理</h4><p>大页分配: 在缺页处理函数<strong>handle_mm_fault中判断是否使用大页 大页生成: 主要通过在分配大页内存时是否带</strong>GFP_DIRECT_RECLAIM 标志来控制大页的生成.</p>
<p>1.异步生成大页: 在缺页处理中，把需要异步生成大页的VMA注册到链表，内核态线程k<strong>hugepage</strong>d 动态为vma分配大页(内存回收，内存归整)</p>
<p>2.madvise系统调用只是给VMA加了VM_<strong>HUGEPAGE,用来</strong>标记这段虚拟地址需要使用大页</p>
<p>THP 对redis、mongodb 这种cache类推荐关闭，对drds这种java应用最好打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">#cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">[always] madvise never</div><div class="line"></div><div class="line">grep &quot;Huge&quot; /proc/meminfo</div><div class="line">AnonHugePages:   1286144 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">HugePages_Total:       0</div><div class="line">HugePages_Free:        0</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:               0 kB</div><div class="line"></div><div class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &apos;&#123; if($2&gt;4) print $0&#125; &apos; |  awk -F &quot;/&quot;  &apos;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &apos;</div><div class="line"></div><div class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &apos;&#123; if($2&gt;4) print $0&#125; &apos; |  awk -F &quot;/&quot;  &apos;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &apos;</div><div class="line"></div><div class="line">//查看pagesize（默认4K） </div><div class="line">$getconf PAGESIZE</div></pre></td></tr></table></figure>
<p>在透明大页功能打开时，造成系统性能下降的主要原因可能是 <code>khugepaged</code> 守护进程。该进程会在（它认为）系统空闲时启动，扫描系统中剩余的空闲内存，并将普通 4k 页转换为大页。该操作会在内存路径中加锁，而该守护进程可能会在错误的时间启动扫描和转换大页的操作，从而影响应用性能。</p>
<p>此外，当缺页异常(page faults)增多时，透明大页会和普通 4k 页一样，产生同步内存压缩(direct compaction)操作，以节省内存。该操作是一个同步的内存整理操作，如果应用程序会短时间分配大量内存，内存压缩操作很可能会被触发，从而会对系统性能造成风险。<a href="https://yq.aliyun.com/articles/712830" target="_blank" rel="external">https://yq.aliyun.com/articles/712830</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#查看系统级别的 THP 使用情况，执行下列命令：</div><div class="line">cat /proc/meminfo  | grep AnonHugePages</div><div class="line">#类似地，查看进程级别的 THP 使用情况，执行下列命令：</div><div class="line">cat /proc/1730/smaps | grep AnonHugePages |grep -v &quot;0 kB&quot;</div><div class="line">#是否开启了hugepage</div><div class="line">$cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">always [madvise] never</div></pre></td></tr></table></figure>
<p><code>/proc/sys/vm/nr_hugepages</code> 中存储的数据就是大页面的数量，虽然在默认情况下它的值都是 0，不过我们可以通过更改该文件的内容申请或者释放操作系统中的大页：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ echo 1 &gt; /proc/sys/vm/nr_hugepages</div><div class="line">$ cat /proc/meminfo | grep HugePages_</div><div class="line">HugePages_Total:       1</div><div class="line">HugePages_Free:        1</div></pre></td></tr></table></figure>
<h3 id="THP和perf"><a href="#THP和perf" class="headerlink" title="THP和perf"></a>THP和perf</h3><p>thp on后比off性能稳定好 10-15%</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div></pre></td><td class="code"><pre><div class="line">//on 419， thp off</div><div class="line">9,145,128,732      branch-instructions       #  229.068 M/sec                    (10.65%)</div><div class="line">       555,518,878      branch-misses             #    6.07% of all branches          (14.24%)</div><div class="line">     3,951,535,475      bus-cycles                #   98.979 M/sec                    (14.29%)</div><div class="line">       372,477,068      cache-misses              #    7.733 % of all cache refs      (14.34%)</div><div class="line">     4,816,702,013      cache-references          #  120.649 M/sec                    (14.36%)</div><div class="line">   114,521,174,305      cpu-cycles                #    2.869 GHz                      (14.36%)</div><div class="line">    48,969,565,344      instructions              #    0.43  insn per cycle           (17.93%)</div><div class="line">    98,728,666,922      ref-cycles                # 2472.967 M/sec                    (21.52%)</div><div class="line">                 0      alignment-faults          #    0.000 K/sec</div><div class="line">            12,187      context-switches          #    0.305 K/sec</div><div class="line">         39,922.47 msec cpu-clock                 #    7.898 CPUs utilized</div><div class="line">               147      cpu-migrations            #    0.004 K/sec</div><div class="line">                 0      dummy                     #    0.000 K/sec</div><div class="line">                 0      emulation-faults          #    0.000 K/sec</div><div class="line">                 0      major-faults              #    0.000 K/sec</div><div class="line">             1,727      minor-faults              #    0.043 K/sec</div><div class="line">             1,768      page-faults               #    0.044 K/sec</div><div class="line">         39,923.84 msec task-clock                #    7.898 CPUs utilized</div><div class="line">     1,848,336,574      L1-dcache-load-misses     #   13.31% of all L1-dcache hits    (21.51%)</div><div class="line">    13,889,399,043      L1-dcache-loads           #  347.903 M/sec                    (21.51%)</div><div class="line">     7,055,617,648      L1-dcache-stores          #  176.730 M/sec                    (21.50%)</div><div class="line">     2,017,950,458      L1-icache-load-misses                                         (21.50%)</div><div class="line">        88,802,885      LLC-load-misses           #    9.86% of all LL-cache hits     (14.35%)</div><div class="line">       900,379,398      LLC-loads                 #   22.553 M/sec                    (14.33%)</div><div class="line">       162,711,813      LLC-store-misses          #    4.076 M/sec                    (7.13%)</div><div class="line">       419,869,955      LLC-stores                #   10.517 M/sec                    (7.14%)</div><div class="line">       553,257,955      branch-load-misses        #   13.858 M/sec                    (10.71%)</div><div class="line">     9,195,874,519      branch-loads              #  230.339 M/sec                    (14.29%)</div><div class="line">       176,112,524      dTLB-load-misses          #    1.28% of all dTLB cache hits   (14.29%)</div><div class="line">    13,739,965,115      dTLB-loads                #  344.160 M/sec                    (14.28%)</div><div class="line">        33,087,849      dTLB-store-misses         #    0.829 M/sec                    (14.28%)</div><div class="line">     6,992,863,588      dTLB-stores               #  175.158 M/sec                    (14.26%)</div><div class="line">       170,555,902      iTLB-load-misses          #  107.90% of all iTLB cache hits   (14.24%)</div><div class="line">       158,070,998      iTLB-loads                #    3.959 M/sec                    (14.24%)</div><div class="line">        68,973,832      node-load-misses          #    1.728 M/sec                    (14.24%)</div><div class="line">        20,207,143      node-loads                #    0.506 M/sec                    (14.24%)</div><div class="line">        93,216,790      node-store-misses         #    2.335 M/sec                    (7.10%)</div><div class="line">        73,871,126      node-stores               #    1.850 M/sec                    (7.08%)</div><div class="line"></div><div class="line">//on 419， thp on</div><div class="line">    12,958,974,094      branch-instructions       #  227.392 M/sec                    (10.68%)</div><div class="line">       850,468,837      branch-misses             #    6.56% of all branches          (14.27%)</div><div class="line">     5,639,495,284      bus-cycles                #   98.957 M/sec                    (14.29%)</div><div class="line">       526,744,798      cache-misses              #    7.324 % of all cache refs      (14.32%)</div><div class="line">     7,192,328,925      cache-references          #  126.204 M/sec                    (14.34%)</div><div class="line">   163,419,436,811      cpu-cycles                #    2.868 GHz                      (14.33%)</div><div class="line">    68,638,583,038      instructions              #    0.42  insn per cycle           (17.90%)</div><div class="line">   140,882,455,768      ref-cycles                # 2472.076 M/sec                    (21.48%)</div><div class="line">                 0      alignment-faults          #    0.000 K/sec</div><div class="line">            18,171      context-switches          #    0.319 K/sec</div><div class="line">         56,987.52 msec cpu-clock                 #    7.932 CPUs utilized</div><div class="line">                68      cpu-migrations            #    0.001 K/sec</div><div class="line">                 0      dummy                     #    0.000 K/sec</div><div class="line">                 0      emulation-faults          #    0.000 K/sec</div><div class="line">                 0      major-faults              #    0.000 K/sec</div><div class="line">             2,323      minor-faults              #    0.041 K/sec</div><div class="line">             2,362      page-faults               #    0.041 K/sec</div><div class="line">         56,991.53 msec task-clock                #    7.932 CPUs utilized</div><div class="line">     2,471,392,118      L1-dcache-load-misses     #   12.69% of all L1-dcache hits    (21.47%)</div><div class="line">    19,480,914,771      L1-dcache-loads           #  341.833 M/sec                    (21.48%)</div><div class="line">    10,059,893,871      L1-dcache-stores          #  176.522 M/sec                    (21.46%)</div><div class="line">     3,184,073,065      L1-icache-load-misses                                         (21.46%)</div><div class="line">       128,467,945      LLC-load-misses           #   10.83% of all LL-cache hits     (14.31%)</div><div class="line">     1,186,653,892      LLC-loads                 #   20.822 M/sec                    (14.30%)</div><div class="line">       224,877,539      LLC-store-misses          #    3.946 M/sec                    (7.15%)</div><div class="line">       628,574,746      LLC-stores                #   11.030 M/sec                    (7.15%)</div><div class="line">       848,830,289      branch-load-misses        #   14.894 M/sec                    (10.71%)</div><div class="line">    13,074,297,582      branch-loads              #  229.416 M/sec                    (14.28%)</div><div class="line">       109,223,171      dTLB-load-misses          #    0.56% of all dTLB cache hits   (14.27%)</div><div class="line">    19,418,657,165      dTLB-loads                #  340.741 M/sec                    (14.29%)</div><div class="line">        13,930,402      dTLB-store-misses         #    0.244 M/sec                    (14.28%)</div><div class="line">    10,047,511,003      dTLB-stores               #  176.305 M/sec                    (14.28%)</div><div class="line">       194,902,860      iTLB-load-misses          #   61.23% of all iTLB cache hits   (14.27%)</div><div class="line">       318,292,771      iTLB-loads                #    5.585 M/sec                    (14.26%)</div><div class="line">       100,512,054      node-load-misses          #    1.764 M/sec                    (14.27%)</div><div class="line">        28,144,120      node-loads                #    0.494 M/sec                    (14.27%)</div><div class="line">       128,218,262      node-store-misses         #    2.250 M/sec                    (7.14%)</div><div class="line">       103,892,078      node-stores               #    1.823 M/sec                    (7.11%) </div><div class="line">       </div><div class="line">         7,599,757      dTLB-load-misses          #    0.09% of all dTLB cache hits   (15.59%)</div><div class="line">     8,425,208,450      dTLB-loads                #  528.778 M/sec                    (15.62%)</div><div class="line">         1,288,979      dTLB-store-misses         #    0.081 M/sec                    (15.67%)</div><div class="line">     3,989,148,957      dTLB-stores               #  250.365 M/sec                    (15.27%)</div><div class="line">        21,578,944      iTLB-load-misses          #   15.23% of all iTLB cache hits   (14.42%)</div><div class="line">       141,697,584      iTLB-loads                #    8.893 M/sec                    (14.34%)</div></pre></td></tr></table></figure>
<h2 id="MySQL-场景下代码大页对性能的影响"><a href="#MySQL-场景下代码大页对性能的影响" class="headerlink" title="MySQL 场景下代码大页对性能的影响"></a><a href="https://ata.alibaba-inc.com/articles/217859" target="_blank" rel="external">MySQL 场景下代码大页对性能的影响</a></h2><p>不只是数据可以用HugePage，代码段也可以开启HugePage, 无论在x86还是arm（arm下提升更明显）下，都可以得到大页优于透明大页，透明大页优于正常的4K page</p>
<blockquote>
<p>收益：代码大页 &gt; anon THP &gt; 4k</p>
</blockquote>
<p>arm下，对32core机器用32并发的sysbench来对比，代码大页带来的性能提升大概有11%，iTLB miss下降了10倍左右。</p>
<p>x86下，性能提升只有大概3-5%之间，iTLB miss下降了1.5-3倍左右。</p>
<h2 id="TLAB-miss高的案例"><a href="#TLAB-miss高的案例" class="headerlink" title="TLAB miss高的案例"></a><a href="https://ata.alibaba-inc.com/articles/152660" target="_blank" rel="external">TLAB miss高的案例</a></h2><p>程序运行久了之后会变慢大概10%</p>
<p>刚开始运行的时候perf各项数据:</p>
<p><img src="/images/951413iMgBlog/7a26deaf96bdcc07db4db34ae1178641.png" alt="img"></p>
<p>长时间运行后：</p>
<p><img src="/images/951413iMgBlog/3385ae6ffbd5b48b80efa759f42b8174.png" alt="img"></p>
<p>内存的利用以页为单位，当时分析认为，在此4k连续的基础上，页的碎片不应该对64 byte align的cache有什么影响。当时guest和host都没有开THP。</p>
<p>既然无法理解这个结果，那就只有按部就班的查看内核执行路径上各个函数的差别了，祭出ftrace:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">echo kerel_func_name1 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</div><div class="line"></div><div class="line">echo kerel_func_name2 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</div><div class="line"></div><div class="line">echo kerel_func_name3 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</div><div class="line">echo 1 &gt; /sys/kernel/debug/tracing/function_profile_enabled</div></pre></td></tr></table></figure>
<p>在CPU#20上执行代码:</p>
<p>taskset -c 20 ./b</p>
<p>代码执行完后:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo 0 &gt; /sys/kernel/debug/tracing/function_profile_enabled</div><div class="line">cat /sys/kernel/debug/tracing/trace_stat/function20</div></pre></td></tr></table></figure>
<p>这个时候就会打印出在各个函数上花费的时间，比如:</p>
<p><img src="/images/951413iMgBlog/329769dd1da2ed324ac11b8b922382cd.png" alt="img"></p>
<p>经过调试后，逐步定位到主要时间差距在  __mem_cgroup_commit_charge() (58%).</p>
<p>在阅读代码的过程中，注意到当前内核使能了CONFIG_SPARSEMEM_VMEMMAP=y</p>
<p>原因就是机器运行久了之后内存碎片化严重，导致TLAB Miss严重。</p>
<p>解决：开启THP后，性能稳定</p>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 4G/8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a href="https://www.atatech.org/articles/163233" target="_blank" rel="external">https://www.atatech.org/articles/163233</a> <a href="https://www.atatech.org/articles/97130" target="_blank" rel="external">https://www.atatech.org/articles/97130</a></p>
<p>compact: 在进行 compcation 时，线程会从前往后扫描已使用的 movable page，然后从后往前扫描 free page，扫描结束后会把这些 movable page 给迁移到 free page 里，最终规整出一个 2M 的连续物理内存，这样 THP 就可以成功申请内存了。</p>
<p><img src="/images/951413iMgBlog/image-20210628144121108.png" alt="image-20210628144121108"></p>
<p>一次THP compact堆栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">java          R  running task        0 144305 144271 0x00000080</div><div class="line"> ffff88096393d788 0000000000000086 ffff88096393d7b8 ffffffff81060b13</div><div class="line"> ffff88096393d738 ffffea003968ce50 000000000000000e ffff880caa713040</div><div class="line"> ffff8801688b0638 ffff88096393dfd8 000000000000fbc8 ffff8801688b0640</div><div class="line"></div><div class="line">Call Trace:</div><div class="line"> [&lt;ffffffff81060b13&gt;] ? perf_event_task_sched_out+0x33/0x70</div><div class="line"> [&lt;ffffffff8100bb8e&gt;] ? apic_timer_interrupt+0xe/0x20</div><div class="line"> [&lt;ffffffff810686da&gt;] __cond_resched+0x2a/0x40</div><div class="line"> [&lt;ffffffff81528300&gt;] _cond_resched+0x30/0x40</div><div class="line"> [&lt;ffffffff81169505&gt;] compact_checklock_irqsave+0x65/0xd0</div><div class="line"> [&lt;ffffffff81169862&gt;] compaction_alloc+0x202/0x460</div><div class="line"> [&lt;ffffffff811748d8&gt;] ? buffer_migrate_page+0xe8/0x130</div><div class="line"> [&lt;ffffffff81174b4a&gt;] migrate_pages+0xaa/0x480</div><div class="line"> [&lt;ffffffff81169660&gt;] ? compaction_alloc+0x0/0x460                 //compact and migrate</div><div class="line"> [&lt;ffffffff8116a1a1&gt;] compact_zone+0x581/0x950</div><div class="line"> [&lt;ffffffff8116a81c&gt;] compact_zone_order+0xac/0x100</div><div class="line"> [&lt;ffffffff8116a951&gt;] try_to_compact_pages+0xe1/0x120</div><div class="line"> [&lt;ffffffff8112f1ba&gt;] __alloc_pages_direct_compact+0xda/0x1b0</div><div class="line"> [&lt;ffffffff8112f80b&gt;] __alloc_pages_nodemask+0x57b/0x8d0</div><div class="line"> [&lt;ffffffff81167b9a&gt;] alloc_pages_vma+0x9a/0x150</div><div class="line"> [&lt;ffffffff8118337d&gt;] do_huge_pmd_anonymous_page+0x14d/0x3b0        //huge page</div><div class="line"> [&lt;ffffffff8152a116&gt;] ? rwsem_down_read_failed+0x26/0x30</div><div class="line"> [&lt;ffffffff8114b350&gt;] handle_mm_fault+0x2f0/0x300</div><div class="line"> [&lt;ffffffff810ae950&gt;] ? wake_futex+0x40/0x60</div><div class="line"> [&lt;ffffffff8104a8d8&gt;] __do_page_fault+0x138/0x480</div><div class="line"> [&lt;ffffffff810097cc&gt;] ? __switch_to+0x1ac/0x320</div><div class="line"> [&lt;ffffffff81527910&gt;] ? thread_return+0x4e/0x76e</div><div class="line"> [&lt;ffffffff8152d45e&gt;] do_page_fault+0x3e/0xa0                       //page fault</div><div class="line"> [&lt;ffffffff8152a815&gt;] page_fault+0x25/0x30</div></pre></td></tr></table></figure>
<h3 id="查看pagetypeinfo"><a href="#查看pagetypeinfo" class="headerlink" title="查看pagetypeinfo"></a>查看pagetypeinfo</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">#cat /proc/pagetypeinfo</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    0, zone      DMA, type    Unmovable      1      1      1      0      2      1      1      0      1      0      0</div><div class="line">Node    0, zone      DMA, type  Reclaimable      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      1      3</div><div class="line">Node    0, zone      DMA, type      Reserve      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type    Unmovable     89    144     98     42     21     14      5      2      1      0      1</div><div class="line">Node    0, zone    DMA32, type  Reclaimable     28     22      9      8      0      0      0      0      0      1      7</div><div class="line">Node    0, zone    DMA32, type      Movable    402     50     21      8    880    924    321     51      4      1    227</div><div class="line">Node    0, zone    DMA32, type      Reserve      0      0      0      0      0      0      0      0      0      0      1</div><div class="line">Node    0, zone    DMA32, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type    Unmovable  13709  15231   6637   2646    816    181     46      4      4      1      0</div><div class="line">Node    0, zone   Normal, type  Reclaimable      1      5      6   3293   1295    128     29      7      5      0      0</div><div class="line">Node    0, zone   Normal, type      Movable   6396 1383350 1301956 1007627 670102 366248 160232  54894  13126   1482     37</div><div class="line">Node    0, zone   Normal, type      Reserve      0      0      0      2      1      1      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</div><div class="line">Node 0, zone      DMA            1            0            7            0            0            0</div><div class="line">Node 0, zone    DMA32           24           38          889            1            0            0</div><div class="line">Node 0, zone   Normal         1568          795       127683            2            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    1, zone   Normal, type    Unmovable   3938   8735   5469   3221   2097    989    202      6      0      0      0</div><div class="line">Node    1, zone   Normal, type  Reclaimable      1      7      7      8      7      2      2      2      1      0      0</div><div class="line">Node    1, zone   Normal, type      Movable  18623 1001037 2084894 1261484 631159 276096  87272  17169   1389    797      0</div><div class="line">Node    1, zone   Normal, type      Reserve      0      0      0      8      0      0      0      0      0      0      0</div><div class="line">Node    1, zone   Normal, type          CMA      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    1, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</div><div class="line">Node 1, zone   Normal         1530          637       128903            2            0            0</div></pre></td></tr></table></figure>
<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</div><div class="line">Node 0, zone      DMA</div><div class="line">Node 0, zone    DMA32</div><div class="line">Node 0, zone   Normal</div><div class="line">Node 1, zone   Normal</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3975</div><div class="line">        min      20</div><div class="line">        low      25</div><div class="line">        high     30</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3996</div><div class="line">        managed  3975</div><div class="line">    nr_free_pages 3975</div><div class="line">    nr_alloc_batch 5</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     382873</div><div class="line">        min      2335</div><div class="line">        low      2918</div><div class="line">        high     3502</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  513024</div><div class="line">        managed  450639</div><div class="line">    nr_free_pages 382873</div><div class="line">    nr_alloc_batch 584</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     11105097</div><div class="line">        min      61463</div><div class="line">        low      76828</div><div class="line">        high     92194</div><div class="line">        scanned  0</div><div class="line">        spanned  12058624</div><div class="line">        present  12058624</div><div class="line">        managed  11859912</div><div class="line">    nr_free_pages 11105097</div><div class="line">    nr_alloc_batch 12344</div><div class="line">    </div><div class="line">    low = 5/4 * min</div><div class="line">high = 3/2 * min</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=499 MB</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=624 MB</div><div class="line"></div><div class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</div><div class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=802 MB</div></pre></td></tr></table></figure>
<h2 id="内存碎片化导致rt升高的诊断"><a href="#内存碎片化导致rt升高的诊断" class="headerlink" title="内存碎片化导致rt升高的诊断"></a>内存碎片化导致rt升高的诊断</h2><p>判定方法如下：</p>
<ol>
<li>运行 sar -B 观察 pgscand/s，其含义为每秒发生的直接内存回收次数，当在一段时间内持续大于 0 时，则应继续执行后续步骤进行排查；</li>
<li>运行 <code>cat /sys/kernel/debug/extfrag/extfrag_index</code> 观察内存碎片指数，重点关注 order &gt;= 3 的碎片指数，当接近 1.000 时，表示碎片化严重，当接近 0 时表示内存不足；</li>
<li>运行 <code>cat /proc/buddyinfo, cat /proc/pagetypeinfo</code> 查看内存碎片情况， 指标含义参考 （<a href="https://man7.org/linux/man-pages/man5/proc.5.html），同样关注" target="_blank" rel="external">https://man7.org/linux/man-pages/man5/proc.5.html），同样关注</a> order &gt;= 3 的剩余页面数量，pagetypeinfo 相比 buddyinfo 展示的信息更详细一些，根据迁移类型 （伙伴系统通过迁移类型实现反碎片化）进行分组，需要注意的是，当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重，我们需要结合其他工具来排查具体原因，在本文就不做过多介绍了；</li>
<li>对于 CentOS 7.6 等支持 BPF 的 kernel 也可以运行我们研发的 <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt" target="_blank" rel="external">drsnoop</a>，<a href="https://github.com/iovisor/bcc/blob/master/tools/compactsnoop_example.txt" target="_blank" rel="external">compactsnoop</a> 工具对延迟进行定量分析，使用方法和解读方式请参考对应文档；</li>
<li>(Opt) 使用 ftrace 抓取 mm_page_alloc_extfrag 事件，观察因内存碎片从备用迁移类型“盗取”页面的信息。</li>
</ol>
<p>​    </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--零拷贝/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--零拷贝/" itemprop="url">Linux内存--零拷贝</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–零拷贝"><a href="#Linux内存–零拷贝" class="headerlink" title="Linux内存–零拷贝"></a>Linux内存–零拷贝</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h2><blockquote>
<p>“<strong>Zero-copy</strong>“ describes computer operations in which the <a href="https://en.wikipedia.org/wiki/Central_processing_unit" target="_blank" rel="external">CPU</a> does not perform the task of copying data from one <a href="https://en.wikipedia.org/wiki/RAM" target="_blank" rel="external">memory</a> area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network.</p>
</blockquote>
<p>零拷贝技术是指计算机执行操作时，<a href="https://zh.wikipedia.org/wiki/中央处理器" target="_blank" rel="external">CPU</a>不需要先将数据从某处<a href="https://zh.wikipedia.org/wiki/随机存取存储器" target="_blank" rel="external">内存</a>复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省 CPU 和<a href="https://zh.wikipedia.org/wiki/内存带宽" target="_blank" rel="external">内存带宽</a>。</p>
<p>零拷贝可以做到用户空间和内核空间共用同一块内存（Java中的DirectBuffer），这样少做一次拷贝。普通Buffer是在JVM堆上分配的内存，而DirectBuffer是堆外分配的（内核和JVM可以同时读写），这样不需要再多一次内核到用户Buffer的拷贝 </p>
<p><img src="/images/951413iMgBlog/83e2dfbd25d703c58877b2faf71c4944.jpg" alt=""></p>
<p>比如通过网络下载文件，普通拷贝的流程会复制4次并有4次上下文切换，上下文切换是因为读写慢导致了IO的阻塞，进而线程被内核挂起，所以发生了上下文切换。在极端情况下如果read/write没有导致阻塞是不会发生上下文切换的：</p>
<p><img src="/images/951413iMgBlog/NoOptimization.jpg" alt="NoOptimization"></p>
<p>改成零拷贝后，也就是将read和write合并成一次，直接在内核中完成磁盘到网卡的数据复制</p>
<p><img src="/images/951413iMgBlog/ccdc10037d35349293cba8a63ad72af5.png" alt="image.png"></p>
<p>零拷贝就是操作系统提供的新函数(sendfile)，同时接收文件描述符和 TCP socket 作为输入参数，这样执行时就可以完全在内核态完成内存拷贝，既减少了内存拷贝次数，也降低了上下文切换次数。</p>
<p>而且，零拷贝取消了用户缓冲区后，不只降低了用户内存的消耗，还通过最大化利用 socket 缓冲区中的内存，间接地再一次减少了系统调用的次数，从而带来了大幅减少上下文切换次数的机会！</p>
<p>应用读取磁盘写入网络的时候还得考虑缓存的大小，一般会设置的比较小，这样一个大文件导致多次小批量的读取，每次读取伴随着多次上下文切换。</p>
<p>零拷贝使我们不必关心 socket 缓冲区的大小（socket缓冲区大小本身默认就是动态调整、或者应用代码指定大小）。比如，调用零拷贝发送方法时，尽可以把发送字节数设为文件的所有未发送字节数，例如 320MB，也许此时 socket 缓冲区大小为 1.4MB，那么一次性就会发送 1.4MB 到客户端，而不是只有 32KB。这意味着对于 1.4MB 的 1 次零拷贝，仅带来 2 次上下文切换，而不使用零拷贝且用户缓冲区为 32KB 时，经历了 176 次（4 * 1.4MB/32KB）上下文切换。</p>
<h2 id="read-write-和零拷贝优化"><a href="#read-write-和零拷贝优化" class="headerlink" title="read+write 和零拷贝优化"></a>read+write 和零拷贝优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">read(file, tmp_buf, len);</div><div class="line">write(socket, tmp_buf, len);</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20201104175056589.png" alt="image-20201104175056589"></p>
<h3 id="通过mmap替换read优化一下"><a href="#通过mmap替换read优化一下" class="headerlink" title="通过mmap替换read优化一下"></a>通过mmap替换read优化一下</h3><p>用 <code>mmap()</code> 替换原先的 <code>read()</code>，<code>mmap()</code> 也即是内存映射（memory map）：把用户进程空间的一段内存缓冲区（user buffer）映射到文件所在的内核缓冲区（kernel buffer）上。</p>
<p><img src="/images/951413iMgBlog/516c11b9b9d3f6092f00645c1742c111.png" alt="image.png"></p>
<p>通过使用 <code>mmap()</code> 来代替 <code>read()</code>， 可以减少一次数据拷贝的过程。</p>
<p>但这还不是最理想的零拷贝，因为首先仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次；另外内存映射技术是一个开销很大的虚拟存储操作：这种操作需要修改页表以及用内核缓冲区里的文件数据汰换掉当前 TLB 里的缓存以维持虚拟内存映射的一致性。</p>
<h3 id="sendfile"><a href="#sendfile" class="headerlink" title="sendfile"></a>sendfile</h3><p>在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 <code>sendfile()</code>，函数形式如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></div><div class="line"><span class="keyword">ssize_t</span> sendfile(<span class="keyword">int</span> out_fd, <span class="keyword">int</span> in_fd, <span class="keyword">off_t</span> *offset, <span class="keyword">size_t</span> count);</div></pre></td></tr></table></figure>
<p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。</p>
<p>首先，它可以替代前面的 <code>read()</code> 和 <code>write()</code> 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。</p>
<p>其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：</p>
<p><img src="/images/951413iMgBlog/bd72f4a031bcd88db0ca233e59234832.png" alt="image.png"></p>
<p>当然这里还是有一次CPU来拷贝内存的过程，仍然有文件截断的问题。<code>sendfile()</code> 依然是一个适用性很窄的技术，最适合的场景基本也就是一个静态文件服务器了。</p>
<p>然而 <code>sendfile()</code> 本身是有很大问题的，从不同的角度来看的话主要是：</p>
<ol>
<li>首先一个是这个接口并没有进行标准化，导致 <code>sendfile()</code> 在 Linux 上的接口实现和其他类 Unix 系统的实现并不相同；</li>
<li>其次由于网络传输的异步性，很难在接收端实现和 <code>sendfile()</code> 对接的技术，因此接收端一直没有实现对应的这种技术；</li>
<li>最后从性能方面考量，因为 <code>sendfile()</code> 在把磁盘文件从内核缓冲区（page cache）传输到到套接字缓冲区的过程中依然需要 CPU 参与，这就很难避免 CPU 的高速缓存被传输的数据所污染。</li>
</ol>
<h3 id="SG-DMA（The-Scatter-Gather-Direct-Memory-Access）技术"><a href="#SG-DMA（The-Scatter-Gather-Direct-Memory-Access）技术" class="headerlink" title="SG-DMA（The Scatter-Gather Direct Memory Access）技术"></a>SG-DMA（<em>The Scatter-Gather Direct Memory Access</em>）技术</h3><p>上一小节介绍的 <code>sendfile()</code> 技术已经把一次数据读写过程中的 CPU 拷贝的降低至只有 1 次了，但是人永远是贪心和不知足的，现在如果想要把这仅有的一次 CPU 拷贝也去除掉，有没有办法呢？</p>
<p>当然有！通过引入一个新硬件上的支持，我们可以把这个仅剩的一次 CPU 拷贝也给抹掉：Linux 在内核 2.4 版本里引入了 DMA 的 scatter/gather – 分散/收集功能，并修改了 <code>sendfile()</code> 的代码使之和 DMA 适配。scatter 使得 DMA 拷贝可以不再需要把数据存储在一片连续的内存空间上，而是允许离散存储，gather 则能够让 DMA 控制器根据少量的元信息：一个包含了内存地址和数据大小的缓冲区描述符，收集存储在各处的数据，最终还原成一个完整的网络包，直接拷贝到网卡而非套接字缓冲区，避免了最后一次的 CPU 拷贝：</p>
<p><img src="/images/951413iMgBlog/2361e8c6dcfd20a67f404b684196c160.png" alt="image.png"></p>
<p>如果网卡支持 SG-DMA（<em>The Scatter-Gather Direct Memory Access</em>）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。</p>
<p>这就是所谓的<strong>零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。</strong> 数据传输过程就再也没有 CPU 的参与了，也因此 CPU 的高速缓存再不会被污染了，也不再需要 CPU 来计算数据校验和了，CPU 可以去执行其他的业务计算任务，同时和 DMA 的 I/O 任务并行，此举能极大地提升系统性能。</p>
<p>零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，<strong>只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。</strong></p>
<p>所以，总体来看，<strong>零拷贝技术可以把文件传输的性能提高至少一倍以上</strong>。</p>
<h3 id="splice"><a href="#splice" class="headerlink" title="splice()"></a>splice()</h3><p><code>sendfile()</code> + DMA Scatter/Gather 的零拷贝方案虽然高效，但是也有两个缺点：</p>
<ol>
<li>这种方案需要引入新的硬件支持；</li>
<li>虽然 <code>sendfile()</code> 的输出文件描述符在 Linux kernel 2.6.33 版本之后已经可以支持任意类型的文件描述符，但是输入文件描述符依然只能指向文件。</li>
</ol>
<p>这两个缺点限制了 <code>sendfile()</code> + DMA Scatter/Gather 方案的适用场景。为此，Linux 在 2.6.17 版本引入了一个新的系统调用 <code>splice()</code>，它在功能上和 <code>sendfile()</code> 非常相似，但是能够实现在任意类型的两个文件描述符时之间传输数据；而在底层实现上，<code>splice()</code>又比 <code>sendfile()</code> 少了一次 CPU 拷贝，也就是等同于 <code>sendfile()</code> + DMA Scatter/Gather，完全去除了数据传输过程中的 CPU 拷贝。</p>
<p><code>splice()</code> 所谓的写入数据到管道其实并没有真正地拷贝数据，而是玩了个 tricky 的操作：只进行内存地址指针的拷贝而不真正去拷贝数据。所以，数据 <code>splice()</code> 在内核中并没有进行真正的数据拷贝，因此 <code>splice()</code> 系统调用也是零拷贝。</p>
<p>还有一点需要注意，前面说过管道的容量是 16 个内存页，也就是 16 * 4KB = 64 KB，也就是说一次往管道里写数据的时候最好不要超过 64 KB，否则的话会 <code>splice()</code> 会阻塞住，除非在创建管道的时候使用的是 <code>pipe2()</code> 并通过传入 <code>O_NONBLOCK</code> 属性将管道设置为非阻塞。</p>
<h3 id="send-with-MSG-ZEROCOPY"><a href="#send-with-MSG-ZEROCOPY" class="headerlink" title="send() with MSG_ZEROCOPY"></a>send() with MSG_ZEROCOPY</h3><p>Linux 内核在 2017 年的 v4.14 版本接受了来自 Google 工程师 Willem de Bruijn 在 TCP 网络报文的通用发送接口 <code>send()</code> 中实现的 zero-copy 功能 (MSG_ZEROCOPY) 的 patch，通过这个新功能，用户进程就能够把用户缓冲区的数据通过零拷贝的方式经过内核空间发送到网络套接字中去，这个新技术和前文介绍的几种零拷贝方式相比更加先进，因为前面几种零拷贝技术都是要求用户进程不能处理加工数据而是直接转发到目标文件描述符中去的。Willem de Bruijn 在他的论文里给出的压测数据是：采用 netperf 大包发送测试，性能提升 39%，而线上环境的数据发送性能则提升了 5%~8%，官方文档陈述说这个特性通常只在发送 10KB 左右大包的场景下才会有显著的性能提升。一开始这个特性只支持 TCP，到内核 v5.0 版本之后才支持 UDP。</p>
<p>这个技术是基于 redhat 红帽在 2010 年给 Linux 内核提交的 virtio-net zero-copy 技术之上实现的，至于底层原理，简单来说就是通过 <code>send()</code> 把数据在用户缓冲区中的分段指针发送到 socket 中去，利用 page pinning 页锁定机制锁住用户缓冲区的内存页，然后利用 DMA 直接在用户缓冲区通过内存地址指针进行数据读取，实现零拷贝</p>
<p>目前来说，这种技术的主要缺陷有：</p>
<ol>
<li>只适用于大文件 (10KB 左右) 的场景，小文件场景因为 page pinning 页锁定和等待缓冲区释放的通知消息这些机制，甚至可能比直接 CPU 拷贝更耗时；</li>
<li>因为可能异步发送数据，需要额外调用 <code>poll()</code> 和 <code>recvmsg()</code> 系统调用等待 buffer 被释放的通知消息，增加代码复杂度，以及会导致多次用户态和内核态的上下文切换；</li>
<li>MSG_ZEROCOPY 目前只支持发送端，接收端暂不支持。</li>
</ol>
<h3 id="零拷贝应用"><a href="#零拷贝应用" class="headerlink" title="零拷贝应用"></a>零拷贝应用</h3><p>kafka就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一(利用磁盘顺序写；PageCache)。</p>
<p>非零拷贝代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">File.read(fileDesc, buf, len);</div><div class="line">Socket.send(socket, buf, len);</div></pre></td></tr></table></figure>
<p>Traditional data copying approach：</p>
<p><img src="/images/951413iMgBlog/gif/figure1.gif" alt="Traditional data copying approach"></p>
<p>Traditional context switches：</p>
<p><img src="/images/951413iMgBlog/gif/figure2.gif" alt="Traditional context switches"></p>
<p>如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 <code>transferTo</code> 方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Overridepublic</span> </div><div class="line"><span class="function"><span class="keyword">long</span> <span class="title">transferFrom</span><span class="params">(FileChannel fileChannel, <span class="keyword">long</span> position, <span class="keyword">long</span> count)</span> <span class="keyword">throws</span> IOException </span>&#123; </div><div class="line">    <span class="keyword">return</span> fileChannel.transferTo(position, count, socketChannel);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Data copy with transferTo()</p>
<p><img src="/images/951413iMgBlog/gif/figure3.gif" alt="Data copy with transferTo()"></p>
<p>Context switching with transferTo()：</p>
<p><img src="/images/951413iMgBlog/gif/figure4.gif" alt="Context switching when using transferTo()"></p>
<p>Data copies when transferTo() and gather operations are used</p>
<p><img src="/images/951413iMgBlog/gif/figure5.gif" alt="Data copies when transferTo() and gather operations are used"></p>
<p>如果 Linux 系统支持 <code>sendfile()</code> 系统调用，那么 <code>transferTo()</code> 实际上最后就会使用到 <code>sendfile()</code> 系统调用函数。</p>
<p>Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">http &#123;</div><div class="line">...</div><div class="line">    sendfile on</div><div class="line">...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>sendfile 配置的具体意思:</p>
<ul>
<li>设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。</li>
<li>设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。</li>
</ul>
<p>如果是大文件很容易消耗非常多的PageCache，不推荐使用PageCache（或者说零拷贝），建议使用异步IO+直接IO。</p>
<p>在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">location /video/ &#123; </div><div class="line">    sendfile on; </div><div class="line">    aio on; </div><div class="line">    directio 1024m; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当文件大小大于 <code>directio</code> 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。</p>
<h4 id="零拷贝性能"><a href="#零拷贝性能" class="headerlink" title="零拷贝性能"></a>零拷贝性能</h4><p>如果用零拷贝和不用零拷贝来做一个文件服务器，来对比下他们的性能</p>
<p><a href="https://developer.ibm.com/articles/j-zerocopy/" target="_blank" rel="external">Performance comparison: Traditional approach vs. zero copy</a></p>
<table>
<thead>
<tr>
<th style="text-align:left">File size</th>
<th style="text-align:left">Normal file transfer (ms)</th>
<th style="text-align:left">transferTo (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">7MB</td>
<td style="text-align:left">156</td>
<td style="text-align:left">45</td>
</tr>
<tr>
<td style="text-align:left">21MB</td>
<td style="text-align:left">337</td>
<td style="text-align:left">128</td>
</tr>
<tr>
<td style="text-align:left">63MB</td>
<td style="text-align:left">843</td>
<td style="text-align:left">387</td>
</tr>
<tr>
<td style="text-align:left">98MB</td>
<td style="text-align:left">1320</td>
<td style="text-align:left">617</td>
</tr>
<tr>
<td style="text-align:left">200MB</td>
<td style="text-align:left">2124</td>
<td style="text-align:left">1150</td>
</tr>
<tr>
<td style="text-align:left">350MB</td>
<td style="text-align:left">3631</td>
<td style="text-align:left">1762</td>
</tr>
<tr>
<td style="text-align:left">700MB</td>
<td style="text-align:left">13498</td>
<td style="text-align:left">4422</td>
</tr>
<tr>
<td style="text-align:left">1GB</td>
<td style="text-align:left">18399</td>
<td style="text-align:left">8537</td>
</tr>
</tbody>
</table>
<h2 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h2><p>什么是 DMA 技术？简单理解就是，<strong>在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务</strong></p>
<h2 id="RDMA"><a href="#RDMA" class="headerlink" title="RDMA"></a>RDMA</h2><p>remote DMA</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>
<p><a href="https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA" target="_blank" rel="external">https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA</a> 从Linux内存管理到零拷贝</p>
<p><a href="https://developer.ibm.com/articles/j-zerocopy/" target="_blank" rel="external">Efficient data transfer through zero copy</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/272200286" target="_blank" rel="external">CPU：一个故事看懂DMA</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--管理和碎片/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/15/Linux内存--管理和碎片/" itemprop="url">Linux内存--管理和碎片</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内存–管理和碎片"><a href="#Linux内存–管理和碎片" class="headerlink" title="Linux内存–管理和碎片"></a>Linux内存–管理和碎片</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="物理结构分析"><a href="#物理结构分析" class="headerlink" title="物理结构分析"></a>物理结构分析</h2><p>内存从物理结构上面分为：<strong>Channel &gt; DIMM（对应物理上售卖的内存条） &gt;Rank &gt; Chip &gt; Bank &gt; Row/Column。</strong></p>
<p>Chip就是DRAM芯片，一个chip里面会有很多bank。每个bank就是数据存储的实体，相当于一个二维矩阵，只要声明了column和row就可以从每个bank中取出8bit的数据。</p>
<p>具体可以看如下图，一个通道Channel可以是一个DIMM也可以是两个DIMM，甚至3个DIMM，图中是2个DIMM。</p>
<p><img src="/images/951413iMgBlog/image-20211222135852796.png" alt="image-20211222135852796"></p>
<h2 id="虚拟内存和物理内存"><a href="#虚拟内存和物理内存" class="headerlink" title="虚拟内存和物理内存"></a>虚拟内存和物理内存</h2><p>进程所操作的内存是一个虚拟内存，由OS来将这块虚拟内存映射到实际的物理内存上，这样做的好处是每个进程可以独占 128T 内存，任意地使用，系统上还运行了哪些进程已经与我们完全没有关系了（不需要考虑和其它进程之间的地址会冲突）。为变量和函数分配地址的活，我们交给链接器去自动安排就可以了。这一切都是因为虚拟内存能够提供内存地址空间的隔离，极大地扩展了可用空间。</p>
<p>操作系统管理着这种映射关系，所以你在写代码的时候，就不用再操心物理内存的使用情况了，你看到的内存就是虚拟内存。无论一个进程占用的内存资源有多大，在任一时刻，它需要的物理内存都是很少的。在这个推论的基础上，CPU 为每个进程只需要保留很少的物理内存就可以保证进程的正常执行了。</p>
<p>当程序中使用 malloc 等分配内存的接口时会将内存从待分配状态变成已分配状态，此时这块分配好的内存还没有真正映射到对应的物理内存上，这块内存就是未映射状态，因为它并没有被映射到相应的物理内存，直到对该块内存进行读写时，操作系统才会真正地为它分配物理内存。然后这个页面才能成为正常页面。</p>
<p>i7 处理器的页表也是存储在内存页里的，每个页表项都是 4 字节。所以，人们就将 1024 个页表项组成一张页表。这样一张页表的大小就刚好是 4K，占据一个内存页，这样就更加方便管理。而且，当前市场上主流的处理器也都选择将页大小定为 4K。</p>
<blockquote>
<p>虚拟地址在计算机体系结构里可以评为特优的一项技术；超线程、流水线、多发射只是优；cache则只是良好（成本高）</p>
</blockquote>
<h3 id="CPU-如何找到真实地址"><a href="#CPU-如何找到真实地址" class="headerlink" title="CPU 如何找到真实地址"></a>CPU 如何找到真实地址</h3><p><img src="/images/951413iMgBlog/image-20211107175201297.png" alt="image-20211107175201297"></p>
<ul>
<li>第一步是确定页目录基址。每个 CPU 都有一个页目录基址寄存器，最高级页表的基地址就存在这个寄存器里。在 X86 上，这个寄存器是 CR3。每一次计算物理地址时，MMU 都会从 CR3 寄存器中取出页目录所在的物理地址。</li>
<li>第二步是定位页目录项（PDE）。一个 32 位的虚拟地址可以拆成 10 位，10 位和 12 位三段，上一步找到的页目录表基址加上高 10 位的值乘以 4，就是页目录项的位置。这是因为，一个页目录项正好是 4 字节，所以 1024 个页目录项共占据 4096 字节，刚好组成一页，而 1024 个页目录项需要 10 位进行编码。这样，我们就可以通过最高 10 位找到该地址所对应的 PDE 了。</li>
<li>第三步是定位页表项（PTE）。页目录项里记录着页表的位置，CPU 通过页目录项找到页表的位置以后，再用中间 10 位计算页表中的偏移，可以找到该虚拟地址所对应的页表项了。页表项也是 4 字节的，所以一页之内刚好也是 1024 项，用 10 位进行编码。所以计算公式与上一步相似，用页表基址加上中间 10 位乘以 4，可以得到页表项的地址。</li>
<li>最后一步是确定真实的物理地址。上一步 CPU 已经找到页表项了，这里存储着物理地址，这才真正找到该虚拟地址所对应的物理页。虚拟地址的低 12 位，刚好可以对一页内的所有字节进行编码，所以我们用低 12 位来代表页内偏移。计算的公式是物理页的地址直接加上低 12 位。</li>
</ul>
<p>前面我们分析的是 32 位操作系统，那对于 64 位机器是不是有点不同呢？在 64 位的机器上，使用了 48 位的虚拟地址，所以它需要使用 4 级页表。它的结构与 32 位的 3 级页表是相似的，<strong>只是多了一级页目录，定位的过程也从 32 位的 4 步变成了 5 步。</strong></p>
<p><img src="/images/951413iMgBlog/image-20211107182305732.png" alt="image-20211107182305732"></p>
<p>8086最开始是按不同的作用将内存分为代码段、数据段等，386开始按页开始管理内存（混合有按段管理）。 现代的操作系统都是采用段式管理来做基本的权限管理，而对于内存的分配、回收、调度都是依赖页式管理。</p>
<h3 id="tlab-miss"><a href="#tlab-miss" class="headerlink" title="tlab miss"></a><a href="https://lwn.net/Articles/379748" target="_blank" rel="external">tlab miss</a></h3><p>tlb：从各级cache里分配的一块专用空间，用来存放页表(虚拟地址和物理地址的对应关系)–存放在CPU cache里的索引</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">#x86info -c</div><div class="line">Monitor/Mwait: min/max line size 64/64, ecx bit 0 support, enumeration extension</div><div class="line">SVM: revision 1, 32768 ASIDs</div><div class="line">Address Size: 48 bits virtual, 48 bits physical</div><div class="line">The physical package has 96 of 32768 possible cores implemented.</div><div class="line">L1 Data TLB (1G):           Fully associative. 64 entries.</div><div class="line">L1 Instruction TLB (1G):    Fully associative. 64 entries.</div><div class="line">L1 Data TLB (2M/4M):        Fully associative. 64 entries.</div><div class="line">L1 Instruction TLB (2M/4M): Fully associative. 64 entries.</div><div class="line">L1 Data TLB (4K):           Fully associative. 64 entries.</div><div class="line">L1 Instruction TLB (4K):    Fully associative. 64 entries.</div><div class="line">L1 Data cache:</div><div class="line">	Size: 32Kb	8-way associative.</div><div class="line">	lines per tag=1	line size=64 bytes.</div><div class="line">L1 Instruction cache:</div><div class="line">	Size: 32Kb	8-way associative.</div><div class="line">	lines per tag=1	line size=64 bytes.</div><div class="line">L2 Data TLB (1G):           Fully associative. 64 entries.</div><div class="line">L2 Instruction TLB (1G):    Disabled. 0 entries.</div><div class="line">L2 Data TLB (2M/4M):        4-way associative. 2048 entries.</div><div class="line">L2 Instruction TLB (2M/4M): 2-way associative. 512 entries.</div><div class="line">L2 Data TLB (4K):           8-way associative. 2048 entries.</div><div class="line">L2 Instruction TLB (4K):    4-way associative. 512 entries.</div><div class="line">L2 cache:</div><div class="line">	Size: 512Kb	8-way associative.</div><div class="line">	lines per tag=1	line size=64 bytes.</div><div class="line"></div><div class="line"> running at an estimated 2.55GHz</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20220928160318893.png" alt="image-20220928160318893"></p>
<p>TLB(Translation Lookaside Buffer) Cache用于缓存少量热点内存地址的mapping关系。TLB和L1一样每个core独享，由于制造成本和工艺的限制，响应时间需要控制在CPU Cycle级别的Cache容量只能存储几十个对象。那么TLB Cache在应对大量热点数据<code>Virual Address</code>转换的时候就显得捉襟见肘了。我们来算下按照标准的Linux页大小(page size) 4K，一个能缓存64元素的TLB Cache只能涵盖<code>4K*64 = 256K</code>的热点数据的内存地址，显然离理想非常遥远的。于是Huge Page就产生了。</p>
<p><a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer" target="_blank" rel="external">These are typical performance levels of a TLB</a>:</p>
<ul>
<li>Size: 12 bits – 4,096 entries</li>
<li>Hit time: 0.5 – 1 clock cycle</li>
<li>Miss penalty: 10 – 100 clock cycles</li>
<li>Miss rate: 0.01 – 1% (20–40% for sparse/graph applications)</li>
</ul>
<p>TLB也分为iTLB和dTLB, 分别顶在L1i和L1d前面（比L1更小更快，每个core独享tlb）</p>
<p><img src="/images/951413iMgBlog/tlb_lookup.png" alt="img"></p>
<p>以intel x86为例，一个cpu也就32到64个tlb, 超出这个范畴，就得去查页表。 每个型号的cpu都不一样，需要查看<a href="https://en.wikichip.org/wiki/WikiChip" target="_blank" rel="external">spec</a></p>
<p>进程分配到的不是内存的实际物理地址，而是一个经过映射后的虚拟地址，这么做的原因是为了让每个应用可以独享完整的虚拟地址，而不需要每个进程互相考虑使用内存的协调。</p>
<p>但是虚拟地址到物理地址的映射需要巨大的映射空间，如何用更少的内存消耗来管理庞大的内存（如果没有分级，4G内存对应着4MB的索引空间，一级比如使用4K就够了，多个二级使总共用4M，但是这4M大部分时候不用提前分配），Linux通过四级表项来做虚拟地址到物理地址的映射，这样4Kb就能管理256T内存，4级映射是时间换空间的典型案例。不过一般而言一个进程是远远用不了256T内存的，那么这四级映射大部分时候都是没必要的，所以实际用不了那么大的PageTable。</p>
<p><a href="https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA" target="_blank" rel="external">虚拟内存的核心原理</a>是：为每个程序设置一段”连续”的虚拟地址空间，把这个地址空间分割成多个具有连续地址范围的页 (page)，并把这些页和物理内存做映射，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令：</p>
<p><img src="/images/951413iMgBlog/1610941678194-bb42451f-b59a-475d-9b8d-b1085c18766d.png" alt="img"></p>
<p>在 <strong>内存管理单元（Memory Management Unit，MMU）</strong>进行地址转换时，如果页表项的 “在/不在” 位是 0，则表示该页面并没有映射到真实的物理页框，则会引发一个<strong>缺页中断</strong>，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出 (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个脏页 (dirty page)，需要写回磁盘更新改页面在磁盘上的副本，如果该页面是”干净”的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。</p>
<p>还需要了解的一个概念是<strong>转换检测缓冲器（Translation Lookaside Buffer，TLB，每个core一个TLB，类似L1 cache）</strong>，也叫快表，是用来加速虚拟地址映射的，因为虚拟内存的分页机制，页表一般是保存内存中的一块固定的存储区，导致进程通过 MMU 访问内存比直接访问内存多了一次内存访问，性能至少下降一半，因此需要引入加速机制，即 TLB 快表，TLB 可以简单地理解成页表的高速缓存，保存了最高频被访问的页表项，由于一般是硬件实现的，因此速度极快，MMU 收到虚拟地址时一般会先通过硬件 TLB 查询对应的页表号，若命中且该页表项的访问操作合法，则直接从 TLB 取出对应的物理页框号返回，若不命中则穿透到内存页表里查询，并且会用这个从内存页表里查询到最新页表项替换到现有 TLB 里的其中一个，以备下次缓存命中。</p>
<p>如果没有TLB那么每一次内存映射都需要查表四次然后才是一次真正的内存访问，代价比较高。</p>
<p>有了TLB之后，CPU访问某个虚拟内存地址的过程如下</p>
<ol>
<li>CPU产生一个虚拟地址</li>
<li>MMU从TLB中获取页表，翻译成物理地址</li>
<li>MMU把物理地址发送给L1/L2/L3/内存</li>
<li>L1/L2/L3/内存将地址对应数据返回给CPU</li>
</ol>
<p>由于第2步是类似于寄存器的访问速度，所以<strong>如果TLB能命中，则虚拟地址到物理地址的时间开销几乎可以忽</strong>略。tlab miss比较高的话开启内存大页对性能是有提升的，但是会有一定的内存浪费。</p>
<h2 id="内存布局"><a href="#内存布局" class="headerlink" title="内存布局"></a>内存布局</h2><ul>
<li>代码段：CPU 运行一个程序，实质就是在顺序执行该程序的机器码。一个程序的机器码会被组织到同一个地方。</li>
<li>数据段：程序在运行过程中必然要操作数据。这其中，对于有初值的变量，它的初始值会存放在程序的二进制文件中，而且，这些数据部分也会被装载到内存中，即程序的数据段。数据段存放的是程序中已经初始化且不为 0 的全局变量和静态变量。</li>
<li>BSS 段: 对于未初始化的全局变量和静态变量，因为编译器知道它们的初始值都是 0，因此便不需要再在程序的二进制映像中存放这么多 0 了，只需要记录他们的大小即可，这便是BSS段 。BSS 段这个缩写名字是 Block Started by Symbol，但很多人可能更喜欢把它记作 Better Save Space 的缩写。</li>
<li>堆是程序员可以自由申请的空间，当我们在写程序时要保存数据，优先会选择堆；</li>
<li>栈是函数执行时的活跃记录，这将是我们下一节课要重点分析的内容。</li>
</ul>
<p>数据段和 BSS 段里存放的数据也只能是部分数据，主要是全局变量和静态变量，但程序在运行过程中，仍然需要记录大量的临时变量，以及运行时生成的变量，这里就需要新的内存区域了，即程序的堆空间跟栈空间。与代码段以及数据段不同的是，堆和栈并不是从磁盘中加载，它们都是由程序在运行的过程中申请，在程序运行结束后释放。</p>
<p>总的来说，一个程序想要运行起来所需要的几块基本内存区域：代码段、数据段、BSS 段、堆空间和栈空间。下面就是内存布局的示意图：</p>
<p><img src="/images/951413iMgBlog/image-20211108115717113.png" alt="image-20211108115717113" style="zoom:35%;"></p>
<p>其它内存形态：</p>
<ul>
<li>存放加载的共享库的内存空间：如果一个进程依赖共享库，那对应的，该共享库的代码段、数据段、BSS 段也需要被加载到这个进程的地址空间中。</li>
<li>共享内存段：我们可以通过系统调用映射一块匿名区域作为共享内存，用来进行进程间通信。</li>
<li>内存映射文件：我们也可以将磁盘的文件映射到内存中，用来进行文件编辑或者是类似共享内存的方式进行进程通信。</li>
</ul>
<p>32位 x86机器下，通过 cat /proc/pid/maps 看到的进程所使用的内存分配空间：</p>
<p><img src="/images/951413iMgBlog/image-20211108120532370.png" alt="image-20211108120532370" style="zoom:33%;"></p>
<p>64位 x86机器下，通过 cat /proc/pid/maps 看到的进程所使用的内存分配空间：</p>
<p><img src="/images/951413iMgBlog/image-20211108120718732.png" alt="image-20211108120718732" style="zoom:40%;"></p>
<p>目前的 64 系统下的寻址空间是 2^48(太多用不完，高16位为Canonical空间），即 256TB。而且根据 canonical address 的划分，地址空间天然地被分割成两个区间，分别是 0x0 - 0x00007fffffffffff 和 0xffff800000000000 - 0xffffffffffffffff。这样就直接将低 128T 的空间划分为用户空间，高 128T 划分为内核空间。</p>
<p>brk:内核维护指向堆的顶部</p>
<p>Java程序对应的maps：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">#cat /proc/14011/maps</div><div class="line">00400000-00401000 r-xp 00000000 08:03 3935494                            /opt/taobao/install/ajdk-8.3.6_fp9-b30/bin/java</div><div class="line">00600000-00601000 rw-p 00000000 08:03 3935494                            /opt/taobao/install/ajdk-8.3.6_fp9-b30/bin/java</div><div class="line">ed400000-1001e0000 rw-p 00000000 00:00 0</div><div class="line">1001e0000-140000000 ---p 00000000 00:00 0</div><div class="line">7f86e8000000-7f8a7fc00000 rw-p 00000000 00:00 0</div><div class="line">..</div><div class="line">7f8aaecfa000-7f8aaeff8000 rw-p 00000000 00:00 0</div><div class="line">7f8aaeff8000-7f8aaf000000 r-xp 00000000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</div><div class="line">7f8aaf000000-7f8aaf1ff000 ---p 00008000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</div><div class="line">7f8aaf1ff000-7f8aaf200000 rw-p 00007000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</div><div class="line">..</div><div class="line">7f8ad8cea000-7f8ad8cec000 r--s 00004000 08:05 7078938                    /home/admin/drds-worker/lib/netty-handler-proxy-4.1.17.Final.jar</div><div class="line">7f8ad8cec000-7f8ad8cf5000 r--s 0006f000 08:05 7078952                    /home/admin/drds-worker/lib/log4j-1.2.17.jar</div><div class="line">7f8ad8cf5000-7f8ad8cf7000 r--s 00005000 08:05 7078960                    /home/admin/drds-worker/lib/objenesis-1.0.jar</div><div class="line">7f8ad8cf7000-7f8ad8cff000 r--s 0004b000 08:05 7078929                    /home/admin/drds-worker/lib/spring-aop-3.2.18.RELEASE.jar</div><div class="line">7f8ad8cff000-7f8ad8d00000 ---p 00000000 00:00 0</div><div class="line">7f8ad8d00000-7f8ad9000000 rw-p 00000000 00:00 0</div><div class="line">7f8ad90e3000-7f8ad90ef000 r--s 000b6000 08:05 7079066                    /home/admin/drds-worker/lib/transmittable-thread-local-2.5.1.jar</div><div class="line">7f8ad9dd8000-7f8ad9dfe000 r--s 0026f000 08:05 7078997                    /home/admin/drds-worker/lib/druid-1.1.7-preview_12.jar</div><div class="line">7f8ad9dfe000-7f8ad9dff000 ---p 00000000 00:00 0</div><div class="line">7f8ad9dff000-7f8ad9eff000 rw-p 00000000 00:00 0</div><div class="line">7f8ad9eff000-7f8ad9f00000 ---p 00000000 00:00 0</div><div class="line">7f8ad9f00000-7f8ada200000 rw-p 00000000 00:00 0</div><div class="line">7f8ada200000-7f8ada202000 r--s 00003000 08:05 7078944                    /home/admin/drds-worker/lib/liberate-rest-1.0.2.jar</div><div class="line">7f8ada202000-7f8ada206000 r--s 00036000 08:05 7078912                    /home/admin/drds-worker/lib/jackson-core-lgpl-1.9.6.jar</div><div class="line">7f8ada289000-7f8ada28b000 r--s 00001000 08:05 7078998                    /home/admin/drds-worker/lib/opencensus-contrib-grpc-metrics-0.10.0.jar</div><div class="line">7f8ada2b5000-7f8ada2b9000 r--s 0003a000 08:05 7079099                    /home/admin/drds-worker/lib/tddl-repo-mysql-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8ada2b9000-7f8ada2c6000 r--s 0007d000 08:05 7078982                    /home/admin/drds-worker/lib/grpc-core-1.9.0.jar</div><div class="line">7f8ada2c6000-7f8ada2d6000 r--s 00149000 08:05 7079000                    /home/admin/drds-worker/lib/protobuf-java-3.5.1.jar</div><div class="line">7f8ada2d6000-7f8ada2db000 r--s 0002b000 08:05 7078927                    /home/admin/drds-worker/lib/tddl-net-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8ada2db000-7f8ada2e0000 r--s 0002a000 08:05 7078939                    /home/admin/drds-worker/lib/grpc-netty-1.9.0.jar</div><div class="line">7f8ada2e0000-7f8ada2ff000 r--s 00150000 08:05 7078965                    /home/admin/drds-worker/lib/mockito-core-1.9.5.jar</div><div class="line">7f8ada2ff000-7f8ada300000 ---p 00000000 00:00 0</div><div class="line">7f8ada300000-7f8ada600000 rw-p 00000000 00:00 0</div><div class="line">7f8ada600000-7f8ada601000 r--s 00003000 08:05 7079089                    /home/admin/drds-worker/lib/ushura-1.0.jar</div><div class="line">7f8ae9ba2000-7f8ae9baa000 r-xp 00000000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</div><div class="line">7f8ae9baa000-7f8ae9da9000 ---p 00008000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</div><div class="line">7f8ae9da9000-7f8ae9daa000 rw-p 00007000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</div><div class="line">7f8ae9daa000-7f8ae9db6000 r-xp 00000000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9db6000-7f8ae9fb5000 ---p 0000c000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9fb5000-7f8ae9fb6000 r--p 0000b000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9fb6000-7f8ae9fb7000 rw-p 0000c000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</div><div class="line">7f8ae9fb7000-7f8ae9fbd000 rw-p 00000000 00:00 0</div><div class="line">7f8ae9fbd000-7f8ae9fe7000 r-xp 00000000 08:03 3935961                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libjava.so</div><div class="line">7f8aebc03000-7f8aebc05000 r--s 00008000 08:05 7079098                    /home/admin/drds-worker/lib/grpc-stub-1.9.0.jar</div><div class="line">7f8aebc05000-7f8aebc07000 r--s 00020000 08:05 7078930                    /home/admin/drds-worker/lib/tddl-group-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8aebc07000-7f8aebc1f000 r--s 001af000 08:05 7079085                    /home/admin/drds-worker/lib/aspectjweaver-1.8.5.jar</div><div class="line">7f8aebc1f000-7f8aebc20000 ---p 00000000 00:00 0</div><div class="line">7f8aebc20000-7f8aebd20000 rw-p 00000000 00:00 0</div><div class="line">7f8aebd20000-7f8aebd35000 r-xp 00000000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebd35000-7f8aebf34000 ---p 00015000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebf34000-7f8aebf35000 r--p 00014000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebf35000-7f8aebf36000 rw-p 00015000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</div><div class="line">7f8aebf36000-7f8aec037000 r-xp 00000000 08:03 1837579                    /usr/lib64/libm-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aec037000-7f8aec236000 ---p 00101000 08:03 1837579                    /usr/lib64/libm-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aeca17000-7f8aeca18000 rw-p 0000d000 08:03 3936057                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/jli/libjli.so</div><div class="line">7f8aeca18000-7f8aeca2d000 r-xp 00000000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aeca2d000-7f8aecc2c000 ---p 00015000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aecc2c000-7f8aecc2d000 r--p 00014000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aecc2d000-7f8aecc2e000 rw-p 00015000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</div><div class="line">7f8aecc2e000-7f8aecc45000 r-xp 00000000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aecc45000-7f8aece44000 ---p 00017000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aece44000-7f8aece45000 r--p 00016000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aece45000-7f8aece46000 rw-p 00017000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aece46000-7f8aece4a000 rw-p 00000000 00:00 0</div><div class="line">7f8aece4a000-7f8aecea1000 r-xp 00000000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</div><div class="line">7f8aecea1000-7f8aed0a0000 ---p 00057000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</div><div class="line">7f8aed0a0000-7f8aed0a3000 rw-p 00056000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</div><div class="line">7f8aed0a3000-7f8aed0b5000 rw-p 00000000 00:00 0</div><div class="line">7f8aed0b5000-7f8aed0d7000 r-xp 00000000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aed0d7000-7f8aed0dc000 r--s 00038000 08:05 7079012                    /home/admin/drds-worker/lib/org.osgi.core-4.2.0.jar</div><div class="line">7f8aed0dc000-7f8aed0e1000 r--s 00038000 08:05 7079018                    /home/admin/drds-worker/lib/commons-beanutils-1.9.3.jar</div><div class="line">7f8aed0e1000-7f8aed0e3000 r--s 00001000 08:05 7079033                    /home/admin/drds-worker/lib/j2objc-annotations-1.1.jar</div><div class="line">7f8aed0e3000-7f8aed0e8000 r--s 00017000 08:05 7079056                    /home/admin/drds-worker/lib/hibernate-jpa-2.1-api-1.0.0.Final.jar</div><div class="line">7f8aed1be000-7f8aed1c6000 rw-s 00000000 08:04 393222                     /tmp/hsperfdata_admin/14011</div><div class="line">7f8aed1c6000-7f8aed1ca000 ---p 00000000 00:00 0</div><div class="line">7f8aed1ca000-7f8aed2cd000 rw-p 00000000 00:00 0</div><div class="line">7f8aed2cd000-7f8aed2ce000 r--s 00005000 08:05 7079029                    /home/admin/drds-worker/lib/jersey-apache-connector-2.26.jar</div><div class="line">7f8aed2ce000-7f8aed2d1000 r--s 0000a000 08:05 7079027                    /home/admin/drds-worker/lib/metrics-jvm-1.7.4.jar</div><div class="line">7f8aed2d1000-7f8aed2d3000 r--s 00006000 08:05 7078961                    /home/admin/drds-worker/lib/tddl-client-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</div><div class="line">7f8aed2d3000-7f8aed2d4000 rw-p 00000000 00:00 0</div><div class="line">7f8aed2d4000-7f8aed2d5000 r--p 00000000 00:00 0</div><div class="line">7f8aed2d5000-7f8aed2d6000 rw-p 00000000 00:00 0</div><div class="line">7f8aed2d6000-7f8aed2d7000 r--p 00021000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aed2d7000-7f8aed2d8000 rw-p 00022000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</div><div class="line">7f8aed2d8000-7f8aed2d9000 rw-p 00000000 00:00 0</div><div class="line">7fff087e0000-7fff08801000 rw-p 00000000 00:00 0                          [stack]</div><div class="line">7fff089c2000-7fff089c4000 r-xp 00000000 00:00 0                          [vdso]</div><div class="line">ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]</div></pre></td></tr></table></figure>
<h2 id="内存管理和使用"><a href="#内存管理和使用" class="headerlink" title="内存管理和使用"></a>内存管理和使用</h2><h3 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a><a href="https://mp.weixin.qq.com/s?__biz=MzAxODI5ODMwOA==&amp;mid=2666566845&amp;idx=1&amp;sn=3d4b181d4928adae408f570ba46c17b3" target="_blank" rel="external">malloc</a></h3><p>malloc()分配内存时：</p>
<ul>
<li>如果用户分配的内存小于 128 KB，则通过 brk() 申请内存–在堆顶分配；</li>
<li>如果用户分配的内存大于 128 KB，则通过 mmap()  申请内存–从文件映射区域分配；</li>
</ul>
<p><img src="/images/951413iMgBlog/640-20220323120420806.png" alt="图片"></p>
<p><img src="/images/951413iMgBlog/640-20220323120443853.png" alt="图片"></p>
<p>对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」：</p>
<ul>
<li>malloc 通过 <strong>brk()</strong> 方式申请的内存，free 释放内存的时候，<strong>并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用</strong>–小内存分配避免反复调用系统操作导致上下文切换，缺点是没回收容易导致内存碎片进而浪费内存。brk分配出来的内存在maps中显示有heap字样；</li>
<li>malloc 通过 <strong>mmap()</strong> 方式申请的内存，free 释放内存的时候，<strong>会把内存归还给操作系统，内存得到真正的释放</strong>。并且mmap分配的虚拟内存都是缺页状态的。</li>
</ul>
<h3 id="malloc和mmap"><a href="#malloc和mmap" class="headerlink" title="malloc和mmap"></a>malloc和mmap</h3><p>glibc中的malloc/free 负责向内核批发内存（不需要每次分配都真正地去调用内核态来分配），分配好的内存按大小分成不同的桶，每次malloc的时候实际到对应的桶上摘取对应的块(slab)就好，用完free的时候挂回去。</p>
<p><img src="/images/951413iMgBlog/image-20211118121500859.png" alt="image-20211118121500859"></p>
<p>mmap映射内存</p>
<p><img src="/images/951413iMgBlog/image-20211108122432263.png" alt="image-20211108122432263" style="zoom:20%;"></p>
<p>私有匿名映射常用于分配内存，也就是申请堆内存</p>
<p>分桶式内存管理比简单算法无论是在算法效率方面，还是在碎片控制方面都有很大的提升。但它的缺陷也很明显：区域内部的使用率不够高和动态扩展能力不够好。例如，4 字节的区域提前消耗完了，但 8 字节的空闲区域还有很多，此时就会面临两难选择，如果直接分配 8 字节的区域，则区域内部浪费就比较多，如果不分配，则明明还有空闲区域，却无法成功分配。</p>
<p>为了解决以上问题所以搞了buddy</p>
<h3 id="node-gt-zone-gt-buddy-gt-slab"><a href="#node-gt-zone-gt-buddy-gt-slab" class="headerlink" title="node-&gt;zone-&gt;buddy-&gt;slab"></a>node-&gt;zone-&gt;buddy-&gt;slab</h3><p><img src="/images/oss/debfe12e-d1b9-49cd-988d-3f7fcba6ecd2.png" alt="img"></p>
<p>假如需要分配一块 4 字节大小的空间，但是在 4 字节的 free list 上找不到空闲区域，系统就会往上找，假如 8 字节和 16 字节的 free list 中也没有空闲区域，就会一直向上找到 32 字节的 free list。</p>
<p><img src="/images/951413iMgBlog/image-20211118120823595.png" alt="image-20211118120823595"></p>
<p>伙伴系统不会直接把 32 的空闲区域分配出去，因为这样做的话，会带来巨大的浪费。它会先把 32 字节分成两个 16 字节，把后边一个挂入到 16 字节的 free list 中。然后继续拆分前一半。前一半继续拆成两个 8 字节，再把后一半挂入到 8 字节的 free list，最后，把前一半 8 字节拿去分配，当然这里也要继续拆分成两个 4 字节的空闲区域，其中一个用于本次 malloc 分配，另一个则挂入到 4 字节的 free list。分配后的内存的状态如下所示：</p>
<p><img src="/images/951413iMgBlog/image-20211118120851731.png" alt="image-20211118120851731"></p>
<h3 id="查看zone"><a href="#查看zone" class="headerlink" title="查看zone"></a><a href="https://mp.weixin.qq.com/s/Cn-oX0W5DrI2PivaWLDpPw" target="_blank" rel="external">查看zone</a></h3><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/KernelMemoryZones" target="_blank" rel="external">The zones are</a>:</p>
<ul>
<li><code>DMA</code> is the low 16 MBytes of memory. At this point it exists for historical reasons; once upon what is now a long time ago, there was hardware that could only do DMA into this area of physical memory.</li>
<li><code>DMA32</code> exists only in 64-bit Linux; it is the low 4 GBytes of memory, more or less. It exists because the transition to large memory 64-bit machines has created a class of hardware that can only do DMA to the low 4 GBytes of memory.(This is where people mutter about everything old being new again.)</li>
<li><strong><code>Normal</code></strong> is different on 32-bit and 64-bit machines. On 64-bit machines, it is all RAM from 4GB or so on upwards. On 32-bit machines it is all RAM from 16 MB to 896 MB for complex and somewhat historical reasons. Note that this implies that machines with a 64-bit kernel can have very small amounts of Normal memory unless they have significantly more than 4GB of RAM. For example, a 2 GB machine running a 64-bit kernel will have no Normal memory at all while a 4 GB machine will have only a tiny amount of it.</li>
<li><code>HighMem</code> exists only on 32-bit Linux; it is all RAM above 896 MB, including RAM above 4 GB on sufficiently large machines.</li>
</ul>
<p>每个zone下很多pages(大小为4K)，buddy就是这些Pages的组织管理者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"># cat /proc/zoneinfo |grep Node -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3972</div><div class="line">        min      0</div><div class="line">        low      0</div><div class="line">        high     0</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3993</div><div class="line">        managed  3972</div><div class="line">    nr_free_pages 3972</div><div class="line">    nr_alloc_batch 0</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     361132</div><div class="line">        min      30</div><div class="line">        low      37</div><div class="line">        high     45</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  430773</div><div class="line">        managed  361133</div><div class="line">    nr_free_pages 361132</div><div class="line">    nr_alloc_batch 8</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     96017308</div><div class="line">        min      16864</div><div class="line">        low      21080</div><div class="line">        high     25296</div><div class="line">        scanned  0</div><div class="line">        spanned  200736768</div><div class="line">        present  200736768</div><div class="line">        managed  197571780</div><div class="line">    nr_free_pages 96017308</div><div class="line">    nr_alloc_batch 3807</div><div class="line">    </div><div class="line"># free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            755         150         367           3         236         589</div><div class="line">Swap:             0           0           0</div></pre></td></tr></table></figure>
<p>每个页面大小是4K，很容易可以计算出每个 zone 的大小。比如对于上面 Node0 的 Normal， 197571780 <em> 4K/(1024</em>1024) = 753 GB。</p>
<p>dmidecode 可以查看到服务器上插着的所有内存条，也可以看到它是和哪个CPU直接连接的。每一个CPU以及和他直连的内存条组成了一个 <strong>node（节点）</strong></p>
<h3 id="proc-buddyinfo"><a href="#proc-buddyinfo" class="headerlink" title="/proc/buddyinfo"></a>/proc/buddyinfo</h3><p>/proc/buddyinfo记录了<strong>可用内存</strong>的情况。</p>
<p>Normal那行之后的第二列表示：  643847*2^1*Page_Size(4K) ;  第三列表示：  357451*2^2*Page_Size(4K)  ，高阶内存指的是2^3及更大的内存块。</p>
<p>应用申请大块连续内存（高阶内存，一般之4阶及以上, 也就是64K以上–2^4*4K）时，容易导致卡顿。这是因为大块连续内存确实系统需要触发回收或者碎片整理，需要一定的时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div></pre></td><td class="code"><pre><div class="line">#cat /proc/buddyinfo </div><div class="line">Node 0, zone      DMA      1      1      1      0      2      1      1      0      1      1      3 </div><div class="line">Node 0, zone    DMA32      2      5      3      6      2      0      4      4      2      2    404 </div><div class="line">Node 0, zone   Normal 243430 643847 357451  32531   9508   6159   3917   2960  17172   2633  22854</div><div class="line"></div><div class="line">如果是多node机器：</div><div class="line">#cat /proc/buddyinfo</div><div class="line">Node 0, zone      DMA      4      6      3      2      3      3      1      1      2      3      1</div><div class="line">Node 0, zone    DMA32   1607   1619   1552   1520   1370   1065    827    576    284    105     13</div><div class="line">Node 0, zone   Normal  38337 145731 222145 199776 151452  91969  38086  10037   1762    104      1</div><div class="line">Node 1, zone   Normal  21521 147637 299185 245533 172451  81459  19451   7198    579      3      0</div><div class="line">Node 2, zone   Normal  68427 538670 446906 229138 123555  62539  21161   4407   1122    166    274</div><div class="line">Node 3, zone   Normal  27353  54601 114355 123568 101892  79098  48610  21036   5021    475      6</div><div class="line">Node 4, zone   Normal  45802  42758   8573 184548 148397  70540  20772   4147    381    148    109</div><div class="line">Node 5, zone   Normal  19514  39583 140493 167901 134774  61888  22998   6326    457     32      0</div><div class="line">Node 6, zone   Normal 104493 378362 355158  93138  12928   2248   1019    663    172     40    121</div><div class="line">Node 7, zone   Normal  34185 256886 249560  95547  54526  51022  28180   9757   2038   1351    280</div><div class="line"></div><div class="line">[root@hygon8 15:50 /root]</div><div class="line">#numactl -H</div><div class="line">available: 8 nodes (0-7)</div><div class="line">node 0 cpus: 0 1 2 3 4 5 6 7 64 65 66 67 68 69 70 71</div><div class="line">node 0 size: 64083 MB</div><div class="line">node 0 free: 49838 MB</div><div class="line">node 1 cpus: 8 9 10 11 12 13 14 15 72 73 74 75 76 77 78 79</div><div class="line">node 1 size: 64480 MB</div><div class="line">node 1 free: 43596 MB</div><div class="line">node 2 cpus: 16 17 18 19 20 21 22 23 80 81 82 83 84 85 86 87</div><div class="line">node 2 size: 64507 MB</div><div class="line">node 2 free: 44216 MB</div><div class="line">node 3 cpus: 24 25 26 27 28 29 30 31 88 89 90 91 92 93 94 95</div><div class="line">node 3 size: 64507 MB</div><div class="line">node 3 free: 51095 MB</div><div class="line">node 4 cpus: 32 33 34 35 36 37 38 39 96 97 98 99 100 101 102 103</div><div class="line">node 4 size: 64507 MB</div><div class="line">node 4 free: 32877 MB</div><div class="line">node 5 cpus: 40 41 42 43 44 45 46 47 104 105 106 107 108 109 110 111</div><div class="line">node 5 size: 64507 MB</div><div class="line">node 5 free: 33430 MB</div><div class="line">node 6 cpus: 48 49 50 51 52 53 54 55 112 113 114 115 116 117 118 119</div><div class="line">node 6 size: 64507 MB</div><div class="line">node 6 free: 14233 MB</div><div class="line">node 7 cpus: 56 57 58 59 60 61 62 63 120 121 122 123 124 125 126 127</div><div class="line">node 7 size: 63483 MB</div><div class="line">node 7 free: 36577 MB</div><div class="line">node distances:</div><div class="line">node   0   1   2   3   4   5   6   7</div><div class="line">  0:  10  16  16  16  28  28  22  28</div><div class="line">  1:  16  10  16  16  28  28  28  22</div><div class="line">  2:  16  16  10  16  22  28  28  28</div><div class="line">  3:  16  16  16  10  28  22  28  28</div><div class="line">  4:  28  28  22  28  10  16  16  16</div><div class="line">  5:  28  28  28  22  16  10  16  16</div><div class="line">  6:  22  28  28  28  16  16  10  16</div><div class="line">  7:  28  22  28  28  16  16  16  10</div><div class="line">  </div><div class="line">[root@hygon8 15:51 /root]</div><div class="line">#cat /proc/pagetypeinfo</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    0, zone      DMA, type    Unmovable      1      2      1      1      3      2      0      0      1      0      0</div><div class="line">Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      3      1</div><div class="line">Node    0, zone      DMA, type  Reclaimable      3      4      2      1      0      1      1      1      1      0      0</div><div class="line">Node    0, zone      DMA, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone      DMA, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type    Unmovable    151    164    162    165    140     78     19      8      0      0      0</div><div class="line">Node    0, zone    DMA32, type      Movable   1435   1430   1374   1335   1214    974    798    563    281     98     12</div><div class="line">Node    0, zone    DMA32, type  Reclaimable     21     25     16     20     16     13     10      5      3      7      1</div><div class="line">Node    0, zone    DMA32, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone    DMA32, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type    Unmovable   4849   6607   4133   1629    654    121     15      3      0      0      0</div><div class="line">Node    0, zone   Normal, type      Movable  21088 &gt;100000 &gt;100000 &gt;100000 &gt;100000  90231  37197   9379   1552     83      1</div><div class="line">Node    0, zone   Normal, type  Reclaimable    153    139   3012   3113   2437   1617    874    655    210     21      0</div><div class="line">Node    0, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    0, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 0, zone      DMA            1            6            1            0            0</div><div class="line">Node 0, zone    DMA32           27          974           15            0            0</div><div class="line">Node 0, zone   Normal          856        30173          709            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    1, zone   Normal, type    Unmovable    842   2898   2495   1316    490    102     23      1      2      0      0</div><div class="line">Node    1, zone   Normal, type      Movable  22484 &gt;100000 &gt;100000 &gt;100000 &gt;100000  80084  18922   6889     48      4      0</div><div class="line">Node    1, zone   Normal, type  Reclaimable      1   2022   3850   3534   2582   1273    506    308    529      0      0</div><div class="line">Node    1, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    1, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 1, zone   Normal          810        31221          737            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    2, zone   Normal, type    Unmovable   2833   6802   3888   1636    329      3      1      2      0      0      0</div><div class="line">Node    2, zone   Normal, type      Movable  72017 &gt;100000 &gt;100000 &gt;100000 &gt;100000  61710  20764   4242    841     55    239</div><div class="line">Node    2, zone   Normal, type  Reclaimable    114      8   2056   2221   1544    826    396    163    281    111     35</div><div class="line">Node    2, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    2, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 2, zone   Normal         1066        31063          639            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    3, zone   Normal, type    Unmovable   2508   6171   3802   1502    365     93     30      1      2      0      0</div><div class="line">Node    3, zone   Normal, type      Movable  23396  48450 &gt;100000 &gt;100000  99802  77850  47910  20587   4796    428      5</div><div class="line">Node    3, zone   Normal, type  Reclaimable     10      0    609   2111   1726   1155    670    448    223     46      1</div><div class="line">Node    3, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    3, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 3, zone   Normal          768        31425          575            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    4, zone   Normal, type    Unmovable   3817   3739   1716    992    261     39      4      1      0      0      1</div><div class="line">Node    4, zone   Normal, type      Movable  27857  39138   6875 &gt;100000 &gt;100000  70501  20752   4115    362     49    104</div><div class="line">Node    4, zone   Normal, type  Reclaimable      1      8      3      5      0      0     16     31     19     97      4</div><div class="line">Node    4, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    4, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 4, zone   Normal          712        31706          350            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    5, zone   Normal, type    Unmovable   4875   4728   3165   1202    464     67      3      0      0      0      0</div><div class="line">Node    5, zone   Normal, type      Movable  18382  34874 &gt;100000 &gt;100000 &gt;100000  61296  22711   6235    348     32      0</div><div class="line">Node    5, zone   Normal, type  Reclaimable     16      0      1      7      2    525    284     91    109      0      0</div><div class="line">Node    5, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    5, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 5, zone   Normal          736        31716          316            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    6, zone   Normal, type    Unmovable  10489   6842   2821    434    257     22      1      1      1      3      0</div><div class="line">Node    6, zone   Normal, type      Movable  90841 &gt;100000 &gt;100000  92129  11336   1526    704    552    141     34    118</div><div class="line">Node    6, zone   Normal, type  Reclaimable    434     41      0    576   1338    700    314    110     30      5      3</div><div class="line">Node    6, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    6, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 6, zone   Normal          807        31686          275            0            0</div><div class="line">Page block order: 9</div><div class="line">Pages per block:  512</div><div class="line"></div><div class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</div><div class="line">Node    7, zone   Normal, type    Unmovable   1516   1894   2285    908    633    121     16      7      4      2      0</div><div class="line">Node    7, zone   Normal, type      Movable  18209 &gt;100000 &gt;100000  93283  52811  50349  27973   9703   2026   1341    248</div><div class="line">Node    7, zone   Normal, type  Reclaimable      0      1      0   1341   1082    552    191     47      8      8     32</div><div class="line">Node    7, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</div><div class="line">Node    7, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</div><div class="line"></div><div class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</div><div class="line">Node 7, zone   Normal         1262        31265          241            0            0</div></pre></td></tr></table></figure>
<h3 id="proc-pagetypeinfo"><a href="#proc-pagetypeinfo" class="headerlink" title="/proc/pagetypeinfo"></a>/proc/pagetypeinfo</h3><p><code>cat /proc/pagetypeinfo</code>, 你可以看到当前系统里伙伴系统里各个尺寸的可用连续内存块数量。unmovable pages是不可以被迁移的，比如slab等kmem都不可以被迁移，因为内核里面对这些内存很多情况下是通过指针来访问的，而不是通过页表，如果迁移的话，就会导致原来的指针访问出错。</p>
<p><img src="/images/oss/2e73247c-8a10-43e6-bb0e-49ecfff14268.png" alt="img"></p>
<p><strong>当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重</strong></p>
<p>alloc_pages分配内存的时候就到上面对应大小的free_area的链表上寻找可用连续页面。<code>alloc_pages</code>是怎么工作的呢？我们举个简单的小例子。假如要申请8K-连续两个页框的内存。为了描述方便，我们先暂时忽略UNMOVEABLE、RELCLAIMABLE等不同类型</p>
<p><img src="/images/oss/16ebe996-0e3a-4d67-810f-3121b457271e.png" alt="img"></p>
<p>基于伙伴系统的内存分配中，有可能需要将大块内存拆分成两个小伙伴。在释放中，可能会将两个小伙伴合并再次组成更大块的连续内存。</p>
<blockquote>
<p>伙伴系统中的伙伴指的是两个内存块，大小相同，地址连续，同属于一个大块区域。</p>
</blockquote>
<p>对于应用来说基本分配单位是4K(开启大页后一般是2M)，对于内核来说4K有点浪费。所以内核又专门给自己定制了一个更精细的内存管理系统slab。</p>
<h3 id="slab"><a href="#slab" class="headerlink" title="slab"></a>slab</h3><p>对于内核运行中实际使用的对象来说，多大的对象都有。有的对象有1K多，但有的对象只有几百、甚至几十个字节。如果都直接分配一个 4K的页面 来存储的话也太浪费了，所以伙伴系统并不能直接使用。</p>
<p>在伙伴系统之上，<strong>内核又给自己搞了一个专用的内存分配器， 叫slab</strong>。</p>
<p>这个分配器最大的特点就是，一个slab内只分配特定大小、甚至是特定的对象。这样当一个对象释放内存后，另一个同类对象可以直接使用这块内存。通过这种办法极大地降低了碎片发生的几率。</p>
<h4 id="kmem-cache"><a href="#kmem-cache" class="headerlink" title="kmem cache"></a>kmem cache</h4><p>slabtop和/proc/slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<p>通过查看 <code>/proc/slabinfo</code> 我们可以查看到所有的 kmem cache。</p>
<p><img src="/images/oss/5135d81f-6985-4d6a-8896-e451c0ba20f5.png" alt="img"></p>
<p>slabtop 按占用内存从大往小进行排列。用来分析 slab 内存开销。</p>
<p><img src="/images/oss/0d8a26db-3663-40af-b215-f8601ef23676.png" alt="0d8a26db-3663-40af-b215-f8601ef23676.png (1388×1506)"></p>
<p>无论是 <code>/proc/slabinfo</code>，还是 slabtop 命令的输出。里面都包含了每个 cache 中 slab的如下几个关键属性。</p>
<ul>
<li>objsize：每个对象的大小</li>
<li><p>objperslab：一个 slab 里存放的对象的数量</p>
</li>
<li><p>pagesperslab：一个slab 占用的页面的数量，每个页面4K，这样也就能算出每个 slab 占用的内存大小。</p>
</li>
</ul>
<p>比如如下TCP slabinfo中可以看到每个slab占用8(pagesperslab)个Page(8<em>4096=32768)，每个对象的大小是1984(objsize)，每个slab存放了16(objperslab)个对象. 那么1984 </em>16=31744，现在的空间基本用完，剩下接近1K，又放不下一个1984大小的对象，算是额外开销了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#cat /proc/slabinfo |grep -E &quot;active_objs|TCP&quot;</div><div class="line"># name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</div><div class="line">tw_sock_TCP         5372   5728    256   32    2 : tunables    0    0    0 : slabdata    179    179      0</div><div class="line">TCP                 6090   6144   1984   16    8 : tunables    0    0    0 : slabdata    384    384      0</div></pre></td></tr></table></figure>
<h2 id="内存分配和延迟"><a href="#内存分配和延迟" class="headerlink" title="内存分配和延迟"></a>内存分配和延迟</h2><p>内存不够、脏页太多、碎片太多，都会导致分配失败，从而触发回收，导致卡顿。</p>
<h3 id="系统中脏页过多引起-load-飙高"><a href="#系统中脏页过多引起-load-飙高" class="headerlink" title="系统中脏页过多引起 load 飙高"></a>系统中脏页过多引起 load 飙高</h3><p>直接回收过程中，如果存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟，而且因为这个过程本身是阻塞式的，所以又可能进一步导致系统中处于 D 状态的进程数增多，最终的表现就是系统的 load 值很高。</p>
<p><img src="/images/oss/f16438b744a248d7671d5ac7317b0a98.png" alt="image.png" style="zoom: 50%;"></p>
<p>可以通过 sar -r 来观察系统中的脏页个数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ sar -r 1</div><div class="line">07:30:01 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</div><div class="line">09:20:01 PM   5681588   2137312     27.34         0   1807432    193016      2.47    534416   1310876         4</div><div class="line">09:30:01 PM   5677564   2141336     27.39         0   1807500    204084      2.61    539192   1310884        20</div><div class="line">09:40:01 PM   5679516   2139384     27.36         0   1807508    196696      2.52    536528   1310888        20</div><div class="line">09:50:01 PM   5679548   2139352     27.36         0   1807516    196624      2.51    536152   1310892        24</div></pre></td></tr></table></figure>
<p>kbdirty 就是系统中的脏页大小，它同样也是对 /proc/vmstat 中 nr_dirty 的解析。你可以通过调小如下设置来将系统脏页个数控制在一个合理范围:</p>
<blockquote>
<p>vm.dirty_background_bytes = 0</p>
<p>vm.dirty_background_ratio = 10</p>
<p>vm.dirty_bytes = 0</p>
<p>vm.dirty_expire_centisecs = 3000</p>
<p>vm.dirty_ratio = 20</p>
</blockquote>
<p>至于这些值调整大多少比较合适，也是因系统和业务的不同而异，我的建议也是一边调整一边观察，将这些值调整到业务可以容忍的程度就可以了，即在调整后需要观察业务的服务质量 (SLA)，要确保 SLA 在可接受范围内。调整的效果你可以通过 /proc/vmstat 来查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#grep &quot;nr_dirty_&quot; /proc/vmstat</div><div class="line">nr_dirty_threshold 3071708</div><div class="line">nr_dirty_background_threshold 1023902</div></pre></td></tr></table></figure>
<p>在4.20的内核并且sar 的版本为12.3.3可以看到PSI（Pressure-Stall Information）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">some avg10=45.49 avg60=10.23 avg300=5.41 total=76464318</div><div class="line">full avg10=40.87 avg60=9.05 avg300=4.29 total=58141082</div></pre></td></tr></table></figure>
<p>你需要重点关注 avg10 这一列，它表示最近 10s 内存的平均压力情况，如果它很大（比如大于 40）那 load 飙高大概率是由于内存压力，尤其是 Page Cache 的压力引起的。</p>
<p><img src="/images/oss/cf58f10a523e1e4f0db443be3f54fc04.png" alt="image.png"></p>
<h3 id="容器中的内存回收"><a href="#容器中的内存回收" class="headerlink" title="容器中的内存回收"></a>容器中的内存回收</h3><blockquote>
<p>kswapd线程(每个node一个kswapd进程，负责本node）回收内存时，可以先对脏页进行回写（writeback）再进行回收，而直接内存回收只回收干净页。也叫同步回收.</p>
<p>直接内存回收是在当前进程的上下文中进行的，要等内存回收完成才能继续尝试进行分配，所以是阻塞了当前进程的执行，会导致响应延迟增加</p>
</blockquote>
<p>如果是在容器里，也就是在某个子memory cgroup 中，那么在分配内存后，还有一个记账（charge）的步骤，就是要把这次分配的内存页记在某个memory cgroup的账上，这样才能控制这个容器里的进程所能使用的内存数量。</p>
<p>在开源社区的linux代码中，如果charge 失败，也就是说，当新分配的内存加上原先的usage超过了limit，就会触发内存回收，try_to_free_mem_cgroup_pages，这个也是同步回收，等同于直接内存回收（发生在当前进程的上下文忠），所以会对应用的响应造成影响（表现为卡顿）。</p>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 4G/8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a href="https://www.atatech.org/articles/163233" target="_blank" rel="external">https://www.atatech.org/articles/163233</a> <a href="https://www.atatech.org/articles/97130" target="_blank" rel="external">https://www.atatech.org/articles/97130</a></p>
<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</div><div class="line">Node 0, zone      DMA</div><div class="line">Node 0, zone    DMA32</div><div class="line">Node 0, zone   Normal</div><div class="line">Node 1, zone   Normal</div><div class="line"></div><div class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</div><div class="line">Node 0, zone      DMA</div><div class="line">  pages free     3975</div><div class="line">        min      20</div><div class="line">        low      25</div><div class="line">        high     30</div><div class="line">        scanned  0</div><div class="line">        spanned  4095</div><div class="line">        present  3996</div><div class="line">        managed  3975</div><div class="line">    nr_free_pages 3975</div><div class="line">    nr_alloc_batch 5</div><div class="line">--</div><div class="line">Node 0, zone    DMA32</div><div class="line">  pages free     382873</div><div class="line">        min      2335</div><div class="line">        low      2918</div><div class="line">        high     3502</div><div class="line">        scanned  0</div><div class="line">        spanned  1044480</div><div class="line">        present  513024</div><div class="line">        managed  450639</div><div class="line">    nr_free_pages 382873</div><div class="line">    nr_alloc_batch 584</div><div class="line">--</div><div class="line">Node 0, zone   Normal</div><div class="line">  pages free     11105097</div><div class="line">        min      61463</div><div class="line">        low      76828</div><div class="line">        high     92194</div><div class="line">        scanned  0</div><div class="line">        spanned  12058624</div><div class="line">        present  12058624</div><div class="line">        managed  11859912</div><div class="line">    nr_free_pages 11105097</div><div class="line">    nr_alloc_batch 12344</div><div class="line">    </div><div class="line">    low = 5/4 * min</div><div class="line">high = 3/2 * min</div><div class="line"></div><div class="line"></div><div class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=499 MB</div><div class="line"></div><div class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=624 MB</div><div class="line"></div><div class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</div><div class="line">sum=802 MB</div></pre></td></tr></table></figure>
<h2 id="内存碎片化导致rt升高的诊断"><a href="#内存碎片化导致rt升高的诊断" class="headerlink" title="内存碎片化导致rt升高的诊断"></a>内存碎片化导致rt升高的诊断</h2><p>判定方法如下：</p>
<ol>
<li>运行 sar -B 观察 pgscand/s，其含义为每秒发生的直接内存回收次数，当在一段时间内持续大于 0 时，则应继续执行后续步骤进行排查；</li>
<li>运行 <code>cat /sys/kernel/debug/extfrag/extfrag_index</code> 观察内存碎片指数，重点关注 order &gt;= 3 的碎片指数，当接近 1.000 时，表示碎片化严重，当接近 0 时表示内存不足；</li>
<li>运行 <code>cat /proc/buddyinfo, cat /proc/pagetypeinfo</code> 查看内存碎片情况， 指标含义<a href="https://man7.org/linux/man-pages/man5/proc.5.html" target="_blank" rel="external">参考</a> ，同样关注 order &gt;= 3 的剩余页面数量，pagetypeinfo 相比 buddyinfo 展示的信息更详细一些，根据迁移类型 （伙伴系统通过迁移类型实现反碎片化）进行分组，需要注意的是，<strong>当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重</strong>，我们需要结合其他工具来排查具体原因，在本文就不做过多介绍了；</li>
<li>对于 CentOS 7.6 等支持 BPF 的 kernel 也可以运行我们研发的 <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt" target="_blank" rel="external">drsnoop</a>，<a href="https://github.com/iovisor/bcc/blob/master/tools/compactsnoop_example.txt" target="_blank" rel="external">compactsnoop</a> 工具对延迟进行定量分析，使用方法和解读方式请参考对应文档；</li>
<li>(Opt) 使用 ftrace 抓取 mm_page_alloc_extfrag 事件，观察因内存碎片从备用迁移类型“盗取”页面的信息。</li>
</ol>
<h2 id="一个阿里云ECS-因为宿主机碎片导致性能衰退的案例"><a href="#一个阿里云ECS-因为宿主机碎片导致性能衰退的案例" class="headerlink" title="一个阿里云ECS 因为宿主机碎片导致性能衰退的案例"></a>一个阿里云ECS 因为宿主机碎片导致性能衰退的案例</h2><p>LVS后面三个RS在同样压力流量下，其中一个节点CPU非常高，通过top看起来是所有操作都很慢，像是CPU被降频了一样，但是直接跑CPU Prime性能又没有问题</p>
<p><img src="/images/oss/8bbb5c886dc06196546daec46712ff71.png" alt="image.png"></p>
<p>原因：ECS所在的宿主机内存碎片比较严重，导致分配到的内存主要是4K Page，在ECS中大页场景下会慢很多</p>
<p>通过 <strong>openssl speed aes-256-ige 能稳定重现</strong> 在大块的加密上慢很多</p>
<p><img src="/images/oss/8e15e91d4dcc61bbd329e7283c7c7500.png" alt="image.png"></p>
<p>小块上性能一致，这也就是为什么算Prime性能没问题。导致慢只涉及到大块内存分配的场景，这里需要映射到宿主机，但是碎片多分配慢导致了问题。</p>
<p>如果reboot ECS的话实际只是就地重启ECS，仍然使用的reboot前分配好的宿主机内存，不会解决问题。重启ECS中的进程也不会解决问题，只有将ECS迁移到别的物理机（也就是通过控制台重启，会重新选择物理机）才有可能解决这个问题。</p>
<p>或者购买新的ECS机型（比如第6代之后ECS）能避免这个问题。</p>
<p>ECS内部没法查看到这个碎片，只能在宿主机上通过命令查看大页情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">有问题NC上buddyinfo信息</div><div class="line">$cat /proc/buddyinfo</div><div class="line">Node 0, zone      DMA      1      1      0      0      2      1      1      0      1      1      3</div><div class="line">Node 0, zone    DMA32     23     23     17     15     13      9      8      8      4      3    367</div><div class="line">Node 0, zone   Normal 295291 298652 286048 266597 218191 156837  93156  45930  25856      0      0</div><div class="line"></div><div class="line">最新建的vm，大页不多</div><div class="line">$sudo cat /proc/9550/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">210944</div><div class="line">------------------------</div><div class="line">第一台正常ECS所在的NC</div><div class="line">$cat /proc/buddyinfo</div><div class="line">Node 0, zone      DMA      1      1      0      0      2      1      1      0      1      1      3</div><div class="line">Node 0, zone    DMA32      7      5      5      9      8      4      6     10      5      5    366</div><div class="line">Node 0, zone   Normal 203242 217888 184465 176280 148612 102122  55787  26642  24824      0      0</div><div class="line"></div><div class="line">早期的vm，大页充足</div><div class="line">$sudo cat /proc/87369/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">8275968</div><div class="line"></div><div class="line">近期的vm，大页不够</div><div class="line">$sudo cat /proc/22081/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">251904</div><div class="line"></div><div class="line">$sudo cat /proc/44073/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</div><div class="line">10240</div></pre></td></tr></table></figure>
<h2 id="内存使用分析"><a href="#内存使用分析" class="headerlink" title="内存使用分析"></a>内存使用分析</h2><h3 id="pmap"><a href="#pmap" class="headerlink" title="pmap"></a>pmap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">pmap -x 24282 | less</div><div class="line">24282:   /usr/sbin/rsyslogd -n</div><div class="line">Address           Kbytes     RSS   Dirty Mode  Mapping</div><div class="line">000055ce1a99f000     596     580       0 r-x-- rsyslogd</div><div class="line">000055ce1ac34000      12      12      12 r---- rsyslogd</div><div class="line">000055ce1ac37000      28      28      28 rw--- rsyslogd</div><div class="line">000055ce1ac3e000       4       4       4 rw---   [ anon ]</div><div class="line">000055ce1c1f1000     364     204     204 rw---   [ anon ]</div><div class="line">00007fff8b5a4000     132      20      20 rw---   [ stack ]</div><div class="line">00007fff8b5e6000      12       0       0 r----   [ anon ]</div><div class="line">00007fff8b5e9000       8       4       0 r-x--   [ anon ]</div><div class="line">---------------- ------- ------- -------</div><div class="line">total kB          620060   17252    3304</div></pre></td></tr></table></figure>
<ul>
<li>Address：占用内存的文件的内存起始地址。</li>
<li>Kbytes：占用内存的字节数。</li>
<li>RSS：实际占用内存大小。</li>
<li>Dirty：脏页大小。</li>
<li>Mapping：占用内存的文件，[anon] 为已分配的内存，[stack] 为程序堆栈</li>
</ul>
<h3 id="proc-pid"><a href="#proc-pid" class="headerlink" title="/proc/pid/"></a>/proc/pid/</h3><p><code>/proc/[pid]/</code> 下面与进程内存相关的文件主要有<code>maps , smaps, status</code>。<br>maps： 文件可以查看某个进程的代码段、栈区、堆区、动态库、内核区对应的虚拟地址<br>smaps: 显示每个分区更详细的内存占用数据，能看到一个动态库被共享了几次<br>status: 包含了所有CPU活跃的信息，该文件中的所有值都是从系统启动开始累计到当前时刻</p>
<h2 id="Java内存使用分析"><a href="#Java内存使用分析" class="headerlink" title="Java内存使用分析"></a><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr007.html" target="_blank" rel="external">Java内存使用分析</a></h2><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">创建1000个线程，ss为2M</div><div class="line">java -XX:NativeMemoryTracking=detail -Xms10g -Xmx10g -Xmn5g -XX:ReservedCodeCacheSize=512m -XX:MetaspaceSize=512m -XX:MaxMetaspaceSize=512m -XX:MaxDirectMemorySize=1g -Xss2048K ThreadPoolExample</div><div class="line"></div><div class="line">分析结果：</div><div class="line">#jcmd 81849 VM.native_memory summary</div><div class="line">81849:</div><div class="line"></div><div class="line">Native Memory Tracking:</div><div class="line"></div><div class="line">Total: reserved=14737064KB, committed=13157168KB</div><div class="line">-                 Java Heap (reserved=10485760KB, committed=10485760KB)</div><div class="line">                            (mmap: reserved=10485760KB, committed=10485760KB)</div><div class="line"></div><div class="line">-                     Class (reserved=1102016KB, committed=50112KB)</div><div class="line">                            (classes #416)</div><div class="line">                            (malloc=45248KB #1420)</div><div class="line">                            (mmap: reserved=1056768KB, committed=4864KB)</div><div class="line"></div><div class="line">//committed 已向OS提请分配，实际要到使用时page fault才会实际分配物理内存并在RSS中反应出来</div><div class="line">-                    Thread (reserved=2134883KB, committed=2134883KB)//reserved还没分配,不能访问</div><div class="line">                            (thread #1070) //1000个应用线程，加70个JVM native线程</div><div class="line">                            (stack: reserved=2128820KB, committed=2128820KB) //需要2G多点</div><div class="line">                            (malloc=3500KB #5390)</div><div class="line">                            (arena=2563KB #2138)</div><div class="line"></div><div class="line">-                      Code (reserved=532612KB, committed=4620KB)</div><div class="line">                            (malloc=132KB #528)</div><div class="line">                            (mmap: reserved=532480KB, committed=4488KB)</div><div class="line"></div><div class="line">-                        GC (reserved=430421KB, committed=430421KB)</div><div class="line">                            (malloc=50737KB #235)</div><div class="line">                            (mmap: reserved=379684KB, committed=379684KB)</div><div class="line"></div><div class="line">-                  Compiler (reserved=137KB, committed=137KB)</div><div class="line">                            (malloc=6KB #53)</div><div class="line">                            (arena=131KB #3)</div><div class="line"></div><div class="line">-                  Internal (reserved=48901KB, committed=48901KB)</div><div class="line">                            (malloc=48869KB #14030)</div><div class="line">                            (mmap: reserved=32KB, committed=32KB)</div><div class="line"></div><div class="line">-                    Symbol (reserved=1479KB, committed=1479KB)</div><div class="line">                            (malloc=959KB #110)</div><div class="line">                            (arena=520KB #1)</div><div class="line"></div><div class="line">-    Native Memory Tracking (reserved=608KB, committed=608KB)</div><div class="line">                            (malloc=193KB #2556)</div><div class="line">                            (tracking overhead=415KB)</div><div class="line"></div><div class="line">-               Arena Chunk (reserved=248KB, committed=248KB)</div><div class="line">                            (malloc=248KB)</div></pre></td></tr></table></figure>
<p>We can see two types of memory:</p>
<ul>
<li><strong>Reserved</strong> — the size which is guaranteed to be available by a host’s OS (but still not allocated and cannot be accessed by JVM) — it’s just a promise</li>
<li><strong>Committed</strong> — already taken, accessible, and allocated by JVM</li>
</ul>
<h3 id="page-fault"><a href="#page-fault" class="headerlink" title="page fault"></a>page fault</h3><p>内核给用户态申请的内存，默认都只是一段虚拟地址空间而已，并没有分配真正的物理内存。在第一次读写的时候才触发物理内存的分配，这个过程叫做page fault。那么，为了访问到真正的物理内存，page fault的时候，就需要更新对应的page table了。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>
<p><a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="external">rsyslog占用内存高</a></p>
<p><a href="https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html" target="_blank" rel="external">https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html</a></p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html" target="_blank" rel="external">鸟哥 journald 介绍</a></p>
<p><a href="https://mp.weixin.qq.com/s/OR2XB4J76haGc1THeq7WQg" target="_blank" rel="external">说出来你可能不信，内核这家伙在内存的使用上给自己开了个小灶！</a></p>
<p><a href="https://zhengheng.me/2018/08/27/socket-use-slab-dentry/" target="_blank" rel="external">socket 与 slab dentry</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/11/如何在工作中学习--V2.0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/11/如何在工作中学习--V2.0/" itemprop="url">如何在工作中学习--V2.0</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-11T12:30:03+08:00">
                2020-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何在工作中学习–V2-0"><a href="#如何在工作中学习–V2-0" class="headerlink" title="如何在工作中学习–V2.0"></a>如何在工作中学习–V2.0</h1><p>本文被网友翻译的<a href="https://medium.com/@cai.eason/learn-and-improve-the-right-technical-skills-7a0bc5123e1" target="_blank" rel="external">英文版</a> （medium 需要梯子）</p>
<blockquote>
<p>先说一件值得思考的事情：高考的时候大家都是一样的教科书，同一个教室，同样的老师辅导，时间精力基本差不多，可是最后别人考的是清华北大或者一本，而你的实力只能考个三本，为什么？ 当然这里主要是智商的影响，那么其他因素呢？智商解决的问题能不能后天用其他方式来补位一下？</p>
</blockquote>
<p>大家平时都看过很多方法论的文章，看的时候很爽觉得非常有用，但是一两周后基本还是老样子了。其中有很大一部分原因那些方法对脑力有要求、或者方法论比较空缺少落地的步骤。 下文中描述的方式方法是不需要智商也能学会的，非常具体的。</p>
<h2 id="关键问题点"><a href="#关键问题点" class="headerlink" title="关键问题点"></a>关键问题点</h2><h3 id="为什么你的知识积累不了？"><a href="#为什么你的知识积累不了？" class="headerlink" title="为什么你的知识积累不了？"></a>为什么你的知识积累不了？</h3><p>有些知识看过就忘、忘了再看，实际碰到问题还是联系不上这个知识，这其实是知识的积累出了问题，没有深入的理解自然就不能灵活运用，也就谈不上解决问题了。这跟大家一起看相同的高考教科书但是高考结果不一样是一个原因。问题出在了理解上，每个人的理解能力不一样（智商），绝大多数人对知识的理解要靠不断地实践（做题）来巩固。</p>
<h3 id="同样实践效果不一样？"><a href="#同样实践效果不一样？" class="headerlink" title="同样实践效果不一样？"></a>同样实践效果不一样？</h3><p>同样工作一年碰到了10个问题（或者说做了10套高考模拟试卷），但是结果不一样，那是因为在实践过程中方法不够好。或者说你对你为什么做对了、为什么做错了没有去复盘</p>
<p>假如碰到一个问题，身边的同事解决了，而我解决不了。那么我就去想这个问题他是怎么解决的，他看到这个问题后的逻辑和思考是怎么样的，有哪些知识指导了他这么逻辑推理，这些知识哪些我也知道但是我没有想到这么去运用推理（说明我对这个知识理解的不到位导致灵活运用缺乏）；这些知识中又有哪些是我不知道的（知识缺乏，没什么好说的快去Google什么学习下–有场景案例和目的加持，学习理解起来更快）。</p>
<p>等你把这个问题基本按照你同事掌握的知识和逻辑推理想明白后，需要再去琢磨一下他的逻辑推理解题思路中有没有不对的，有没有啰嗦的地方，有没有更直接的方式（对知识更好地运用）。</p>
<p>我相信每个问题都这么去实践的话就不应该再抱怨灵活运用、举一反三，同时知识也积累下来了，这种场景下积累到的知识是不会那么容易忘记的。</p>
<p>这就是向身边的牛人学习，同时很快超过他的办法。这就是为什么高考前你做了10套模拟题还不如其他人做一套的效果好</p>
<p><strong>知识+逻辑 基本等于你的能力</strong>，知识让你知道那个东西，逻辑让你把东西和问题联系起来</p>
<p><strong>这里的问题你可以理解成方案、架构、设计等</strong></p>
<h3 id="系统化的知识哪里来？"><a href="#系统化的知识哪里来？" class="headerlink" title="系统化的知识哪里来？"></a>系统化的知识哪里来？</h3><p>知识之间是可以联系起来的并且像一颗大树一样自我生长，但是当你都没理解透彻，自然没法产生联系，也就不能够自我生长了。</p>
<p>真正掌握好的知识点会慢慢生长连接最终组成一张大网</p>
<p>但是我们最容易陷入的就是掌握的深度、系统化（工作中碎片时间过多，学校里缺少时间）不够，所以一个知识点每次碰到花半个小时学习下来觉得掌握了，但是3个月后就又没印象了。总是感觉自己在懵懵懂懂中，或者一个领域学起来总是不得要领，根本的原因还是在于：宏观整体大图了解不够（缺乏体系，每次都是盲人摸象）；关键知识点深度不够，理解不透彻，这些关键点就是这个领域的骨架、支点、抓手。缺了抓手自然不能生长，缺了宏观大图容易误入歧途。</p>
<p>我们有时候发现自己在某个领域学起来特别快，但是换个领域就总是不得要领，问题出在了上面，即使花再多时间也是徒然。这也就是为什么学霸看两个小时的课本比你看两天效果还好，感受下来还觉得别人好聪明，是不是智商比我高啊。</p>
<p>所以新进入一个领域的时候要去找他的大图和抓手。</p>
<p>好的同事总是能很轻易地把这个大图交给你，再顺便给你几个抓手，你就基本入门了，这就是培训的魅力，这种情况肯定比自学效率高多了。但是目前绝大部分的培训都做不到这点</p>
<h3 id="好的逻辑又怎么来？"><a href="#好的逻辑又怎么来？" class="headerlink" title="好的逻辑又怎么来？"></a>好的逻辑又怎么来？</h3><p>实践、复盘</p>
<p><img src="/images/951413iMgBlog/webp-5540564.jpg" alt="img"></p>
<h2 id="讲个前同事的故事"><a href="#讲个前同事的故事" class="headerlink" title="讲个前同事的故事"></a>讲个前同事的故事</h2><p>有一个前同事是5Q过来的，负责技术（所有解决不了的问题都找他），这位同学从chinaren出道，跟着王兴一块创业5Q，5Q在学校靠鸡腿打下大片市场，最后被陈一舟的校内收购（据说被收购后5Q的好多技术都走了，最后王兴硬是呆在校内网把合约上的所有钱都拿到了）。这位同学让我最佩服的解决问题的能力，好多问题其实他也不一定就擅长，但是他就是有本事通过Help、Google不停地验证尝试就把一个不熟悉的问题给解决了，这是我最羡慕的能力，在后面的职业生涯中一直不停地往这个方面尝试。</p>
<h3 id="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"><a href="#应用刚启动连接到数据库的时候比较慢，但又不是慢查询" class="headerlink" title="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"></a>应用刚启动连接到数据库的时候比较慢，但又不是慢查询</h3><ol>
<li>这位同学的解决办法是通过tcpdump来分析网络包，看网络包的时间戳和网络包的内容，然后找到了具体卡在了哪里。</li>
<li>如果是专业的DBA可能会通过show processlist 看具体连接在做什么，比如看到这些连接状态是 <strong>authentication</strong> 状态，然后再通过Google或者对这个状态的理解知道创建连接的时候MySQL需要反查IP、域名这里比较耗时，通过配置参数 <strong>skip-name-resolve</strong> 跳过去就好了。</li>
<li><p>如果是MySQL的老司机，一上来就知道连接慢的话跟 <strong>skip-name-resolve</strong> 关系最大。</p>
<p> 在我眼里这三种方式都解决了问题，最后一种最快但是纯靠积累和经验，换个问题也许就不灵了；第一种方式是最牛逼和通用的，只需要最少的知识就把问题解决了。</p>
</li>
</ol>
<p>我当时跟着他从sudo、ls等linux命令开始学起。当然我不会轻易去打搅他问他，每次碰到问题我尽量让他在我的电脑上来操作，解决后我再自己复盘，通过history调出他的所有操作记录，看他在我的电脑上用Google搜啥了，然后一个个去学习分析他每个动作，去想他为什么搜这个关键字，复盘完还有不懂的再到他面前跟他面对面的讨论他为什么要这么做，指导他这么做的知识和逻辑又是什么。</p>
<h2 id="有哪些好的行为帮你更好地掌握知识"><a href="#有哪些好的行为帮你更好地掌握知识" class="headerlink" title="有哪些好的行为帮你更好地掌握知识"></a>有哪些好的行为帮你更好地掌握知识</h2><h2 id="笔记-写博客"><a href="#笔记-写博客" class="headerlink" title="笔记+写博客"></a>笔记+写博客</h2><p>看东西的时候要做笔记，要不当时看得再爽也很容易忘记，我们需要反复复习来加深印象和理解，复习的根据就是笔记（不可能再完整又看一次），笔记整理出里面的要点和你的盲点。</p>
<p>一段时间后把相关的笔记整理成一篇体系性的博客文章，这样既加深了理解又系统化了相关知识。以后再看到跟这篇博客相关的案例、知识点时不断地更新博客（完善你的知识点）</p>
<h2 id="场景式学习、体感的来源、面对问题学习"><a href="#场景式学习、体感的来源、面对问题学习" class="headerlink" title="场景式学习、体感的来源、面对问题学习"></a>场景式学习、体感的来源、面对问题学习</h2><p>前面提到的对知识的深入理解这有点空，如何才能做到深入理解？</p>
<h3 id="举个学习TCP三次握手例子"><a href="#举个学习TCP三次握手例子" class="headerlink" title="举个学习TCP三次握手例子"></a>举个学习TCP三次握手例子</h3><p>经历稍微丰富点的工程师都觉得TCP三次握手看过很多次、很多篇文章了，但是文章写得再好似乎当时理解了，但是总是过几个月就忘了或者一看就懂，过一阵子被人一问就模模糊糊了，或者两个为什么就答不上了，自己都觉得自己的回答是在猜或者不确定</p>
<p>为什么会这样呢？而学其它知识就好通畅多了，我觉得这里最主要的是我们对TCP缺乏体感，比如没有几个工程师去看过TCP握手的代码，也没法想象真正的TCP握手是如何在电脑里运作的（打电话能给你一些类似的体感，但是细节覆盖面不够）。</p>
<p>如果这个时候你一边学习的时候一边再用wireshark抓包看看三次握手具体在干什么，比抽象的描述实在多了，你能看到具体握手的一来一回，并且看到一来一回带了哪些内容，这些内容又是用来做什么、为什么要带，这个时候你再去看别人讲解的理论顿时会觉得好理解多了，以后也很难忘记。</p>
<p>但是这里很多人执行能力不强，想去抓包，但是觉得要下载安装wireshark，要学习wireshark就放弃了。只看不动手当然是最舒适的，但是这个最舒适给了你在学习的假象，没有结果。</p>
<p>这是不是跟你要解决一个难题非常像，这个难题需要你去做很多事，比如下载源代码（翻不了墙，放弃）；比如要编译（还要去学习那些编译参数，放弃）；比如要搭建环境（太琐屑，放弃）。你看这中间九九八十一难你放弃了一难都取不了真经。这也是为什么同样学习、同样的问题，他能学会，他能解决，你不可以。</p>
<p><img src="/images/951413iMgBlog/167211888bc4f2a368df3d16c68e6d51.png" alt="167211888bc4f2a368df3d16c68e6d51.png"></p>
<h3 id="再来看一个解决问题的例子"><a href="#再来看一个解决问题的例子" class="headerlink" title="再来看一个解决问题的例子"></a>再来看一个解决问题的例子</h3><p><a href="https://www.atatech.org/articles/73174" target="_blank" rel="external">会员系统双11优化这个问题</a>对我来说，我是个外来者，完全不懂这里面的部署架构、业务逻辑。但是在问题的关键地方（会员认为自己没问题–压力测试正常的；淘宝API更是认为自己没问题，alimonitor监控显示正常），结果就是会员的同学说我们没有问题，淘宝API肯定有问题，然后就不去思考自己这边可能出问题的环节了。思想上已经甩包了，那么即使再去review流程、环节也就不会那么仔细，自然更是发现不了问题了。</p>
<p>但是我的经验告诉我要有证据地甩包，或者说拿着证据优雅地甩包，这迫使我去找更多的细节证据（证据要给力哦，不能让人家拍回来）。如果我是这么说的，这个问题在淘宝API这里，你看理由是…………，我做了这些实验，看到了这些东东。那么淘宝API那边想要证明我的理由错了就会更积极地去找一些数据。</p>
<p>事实上我就是做这些实验找证据过程中发现了会员的问题，这就是态度、执行力、知识、逻辑能力综合下来拿到的一个结果。我最不喜欢的一句话就是我的程序没问题，因为我的逻辑是这样的，不会错的。你当然不会写你知道的错误逻辑，程序之所以有错误都是在你的逻辑、意料之外的东西。有很多次一堆人电话会议中扯皮的时候，我一般把电话静音了，直接上去人肉一个个过对方的逻辑，一般来说电话会议还没有结束我就给出来对方逻辑之外的东西。</p>
<h2 id="钉子式学习方法和系统性学习方法"><a href="#钉子式学习方法和系统性学习方法" class="headerlink" title="钉子式学习方法和系统性学习方法"></a>钉子式学习方法和系统性学习方法</h2><p>系统性学习方法就是想掌握MySQL，那么搞几本MySQL专著和MySQL 官方DOC看下来，一般课程设计的好的话还是比较容易掌握下来，绝大部分时候都是这种学习方法，可是在种学习方法的问题在于学完后当时看着似乎理解了，但是很容易忘记，一片一片地系统性的忘记，并且缺少应用能力（理解不深）。这是因为一般人对知识的理解没那么容易真正<strong>理解（掌握或者说应用）</strong>。</p>
<p>钉子式的学习方式，就是在一大片知识中打入几个桩，反复演练将这个桩不停地夯实，夯稳，做到在这个知识点上用通俗的语言跟小白都能讲明白，然后再这几个桩中间发散像星星之火燎原一样把整个一片知识都掌握下来。这种学习方法的缺点就是很难找到一片知识点的这个点，然后没有很好整合的话知识过于零散。</p>
<p>钉子式学习方法看着慢但是因为这样掌握的更透彻和牢固实际最终反而快。</p>
<p>我们常说的一个人很聪明，就是指系统性的看看书就都理解了，是真的理解那种，还能灵活运用，但是大多数普通人就不是这样的，看完书似乎理解了，实际几周后基本都忘记了，真正实践需要用的时候还是用不好。</p>
<h3 id="举个Open-SSH的例子"><a href="#举个Open-SSH的例子" class="headerlink" title="举个Open-SSH的例子"></a>举个Open-SSH的例子</h3><p>为了做通 SSH 的免密登陆，大家都需要用到 ssh-keygen/ssh-copy-id， 如果我们把这两个命令当一个小的钉子的话，会去了解ssh-keygen做了啥（生成了密钥对），或者ssh-copy-id 的时候报错了（原来是需要秘钥对），然后将 ssh-keygen 生成的pub key复制到server的~/.ssh/authorized_keys 中。</p>
<p>然后你应该会对这个原理要有一些理解（更大的钉子），于是理解了密钥对，和ssh验证的流程，顺便学会怎么看ssh debug信息，那么接下来网络上各种ssh攻略、各种ssh卡顿的解决都是很简单的事情了。</p>
<p>比如你通过SSH可以解决这些问题：</p>
<ul>
<li>免密登陆</li>
<li>ssh卡顿</li>
<li>怎么去掉ssh的时候需要手工多输入yes</li>
<li>我的ssh怎么很快就断掉了</li>
<li>我怎么样才能一次通过跳板机ssh到目标机器</li>
<li>我怎么样通过ssh科学上网</li>
<li>我的ansible（底层批量命令都是基于ssh）怎么这么多问题，到底是为什么</li>
<li>我的git怎么报网络错误了</li>
<li>X11 forward我怎么配置不好</li>
<li>https为什么需要随机数加密，还需要签名</li>
<li>…………</li>
</ul>
<p>这些问题都是一步步在扩大ssh的外延，让这个钉子变成一个巨大的桩。</p>
<p>然后就会学习到一些<a href="/2018/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82SSH--SSH%E8%8A%B1%E5%BC%8F%E7%8E%A9%E6%B3%95/">高级一些的ssh配置</a>，比如干掉经常ssh的时候要yes一下(StrictHostKeyChecking=no), 或者怎么配置一下ssh就不会断线了（ServerAliveInterval=15），或者将 ssh跳板机-&gt;ssh server的过程做成 ssh server一步就可以了(ProxyCommand)，进而发现用 ssh的ProxyCommand很容易科学上网了，或者git有问题的时候轻而易举地把ssh debug打开，对git进行debug了……</p>
<p>这基本都还是ssh的本质范围，像ansible、git在底层都是依赖ssh来通讯的，你会发现学、调试X11、ansible和git简直太容易了。</p>
<p>另外理解了ssh的秘钥对，也就理解了非对称加密，同时也很容易理解https流程（SSL），同时知道对称和非对称加密各自的优缺点，SSL为什么需要用到这两种加密算法了。</p>
<p>你看一个简单日常的知识我们只要沿着它用钉子精神，深挖细挖你就会发现知识之间的连接，这个小小的知识点成为你知识体系的一根结实的柱子。</p>
<p>我见过太多的老的工程师、年轻的工程师，天天在那里ssh 密码，ssh 跳板机，ssh 目标机，一小会ssh断了，重来一遍；或者ssh后卡住了，等吧……</p>
<p>在这个问题上表现得没有求知欲、没有探索精神、没有一次把问题搞定的魄力，所以就习惯了</p>
<h2 id="空洞的口号"><a href="#空洞的口号" class="headerlink" title="空洞的口号"></a>空洞的口号</h2><p>很多老师和文章都会教大家：举一反三、灵活运用、活学活用、多做多练。但是只有这些口号是没法落地的，落地的基本步骤就是前面提到的，却总是被忽视了。</p>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举一反三，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>肯定知识效率最牛逼，但是拥有这种技能的人毕竟非常少（天生的高智商吧）。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快的掌握一个新知识，非常气人。剩下的绝大部分只能拼时间+方法+总结等也能掌握一些知识</p>
<p>非常遗憾我就是工程效率型，只能羡慕那些知识效率型的学霸。但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，两者之间当然没有明显的界限，知识积累多了逻辑训练好了在别人看来你的智商就高了</p>
<h2 id="知识分两种"><a href="#知识分两种" class="headerlink" title="知识分两种"></a>知识分两种</h2><p>一种是通用知识（不是说对所有人通用，而是说在一个专业领域去到哪个公司都能通用）；另外一种是跟业务公司绑定的特定知识</p>
<p>通用知识没有任何疑问碰到后要非常饥渴地扑上去掌握他们（受益终生，这还有什么疑问吗？）。对于特定知识就要看你对业务需要掌握的深度了，肯定也是需要掌握一些的，特定知识掌握好的一般在公司里混的也会比较好。</p>
<p>一个具体知识体系里面又有一些核心知识点（抓手、essential knowledge），也就是掌握可以快速帮你膨胀、延伸到其他相关知识的知识点。</p>
<p>还有一些知识、工具一旦掌握就能帮你贯穿、具象、理解别的知识点，比如网络知识体系中的wireshark；理工科中的数学；知识体系中的学习方法、行为方式。我们要多去发现这些知识、工具（how？）</p>
<h2 id="哪些品质能够决定一个人学习好坏"><a href="#哪些品质能够决定一个人学习好坏" class="headerlink" title="哪些品质能够决定一个人学习好坏"></a>哪些品质能够决定一个人学习好坏</h2><h3 id="排在第一名的品质是复盘、总结能力"><a href="#排在第一名的品质是复盘、总结能力" class="headerlink" title="排在第一名的品质是复盘、总结能力"></a>排在第一名的品质是复盘、总结能力</h3><p>简单的说，这个能力就是这个孩子心里是否有个“小教练”，能够每次跳脱出当前任务，帮助自己分析，失败在哪里，成功在哪里，如何进阶，如何训练等等。</p>
<p>举几个例子：</p>
<ol>
<li><p>如果写不出作文，这个“小教练”能告诉孩子，是没有素材，还是文字能力不强。</p>
<p>如果是文字能力不强，应该如何训练（是造句，还是拆段落）</p>
</li>
<li><p>如果数学题做不出来，这个“小教练”能告诉孩子，我的弱点在哪里，哪个类型题我有重大问题，是因为哪里没有理解和打通。</p>
</li>
</ol>
<p>有内化的“自我教练”，这个能力系数是1.67。也就是其他能力相当，学习效果可以翻1.67倍。</p>
<h3 id="排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等"><a href="#排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等" class="headerlink" title="排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等"></a>排在第二名的是建构能力。简单的说是逻辑推理，做事顺序等等</h3><p>在内心“小教练”能把问题进行拆解之后，建构能力能把这些问题进行排序，应该怎么做更合理。</p>
<p>最终怎么把训练步骤整合。遇见一个问题，先干什么，再干什么等等。</p>
<p>有很强的顺序能力，系数为1.44。也就是其他能力相当，学习效果可以翻1.44倍。</p>
<h3 id="排在第三名的能力是智商和过去成绩"><a href="#排在第三名的能力是智商和过去成绩" class="headerlink" title="排在第三名的能力是智商和过去成绩"></a>排在第三名的能力是智商和过去成绩</h3><p>这个毋庸置疑，聪明做事就会简单一些。平均效应系数为0.67。</p>
<p>也就是说，其他能力相当，智商高和过去成绩好，学习效果提升67%</p>
<p>智商重要程度应该比这里更高，但是实际高智商的太少，<strong>大多都是因为基础知识好给人产生了智商高的误解！</strong></p>
<h3 id="排在第四名的能力是自我驱动力"><a href="#排在第四名的能力是自我驱动力" class="headerlink" title="排在第四名的能力是自我驱动力"></a>排在第四名的能力是自我驱动力</h3><p>简单的说，知道自己为什么学习，能够自我鼓励，遇见失败能抗挫，有很强的心理驱动力。</p>
<p>平均效应系数为0.48。也就是说，其他能力相当，有自我驱动力的人，学习效果提升48%。</p>
<h3 id="排在第五名的才是集中注意力"><a href="#排在第五名的才是集中注意力" class="headerlink" title="排在第五名的才是集中注意力"></a>排在第五名的才是集中注意力</h3><p>也就是说，注意力强。注意力对学习影响，并没有很多家长想象的那么大。</p>
<p>注意力的平均效应系数为0.44</p>
<p>也就是说，其他能力相当，注意力好的孩子，效果能提升44%</p>
<p>———总结一下——-</p>
<p>学习提升的个人因素：</p>
<p>自我分析，自我教练的元认知能力 》 逻辑排序与制定计划的建构能力》 智商和过去成绩 》自我驱动力》 集中注意力。</p>
<p>很多家长痴迷于“专注力”。当然专注力是一个效应量很强的学习力，但是从整体数据看，对学习的提升效果，仅仅排到第五名。</p>
<p>帮助孩子建立元认知能力和建构能力的培训，才能给他们对终身学习有帮助的技能包</p>
<p>以上缺少了对方法的落地执行能力的评估，实际这是影响最大的</p>
<h2 id="为什么一直努力却没有结果"><a href="#为什么一直努力却没有结果" class="headerlink" title="为什么一直努力却没有结果"></a><strong>为什么一直努力却没有结果</strong></h2><p>人生不是走斜坡，你持续走就可以走到巅峰；人生像走阶梯（有时候需要跳上台阶），每一阶有每一阶的难点，学物理有物理的难点，学漫画有漫画的难点，你没有克服难点，再怎么努力都是原地跳。所以当你克服难点，你跳上去就不会下来了。</p>
<p>这里的克服难点可以理解成真正掌握知识点，大多时候只是似是而非，所以一直在假忙碌、假学习，只有真正掌握后才像是上了个台阶。</p>
<p>这句话本身就是对知识的具象化理解，本来要描述的是有些人很容易达到目的、有些人则很难，我们很容易归结到天赋等原因，但是还是不能完全理解这个事情，知道天赋差异也不会帮助到我们跳楼梯，而这句话给出了一个形象的描述，一下子就会记住并在日常中尽量尝试跳上去和脱离假忙碌的状态。</p>
<p><img src="/images/951413iMgBlog/6856fb6af3fa3b940c38c71edbc3bb73.png" alt="image.png"></p>
<p>在上述截图中，温伯格拿他自己玩弹子球游戏的经验，来描述”跃迁模式”。<br>　 为啥在跃迁之前会出现一个【低谷】捏？温伯格认为：要想提升到一个新的 level，你需要放弃某些你擅长的技能，然后尝试你所【不】擅长的技能。<br>温伯格以他的弹子球游戏举例说：<br>    为了得到更高的分数，他必须放弃以前熟悉的【低】难度技巧，转而尝试某种【高】难度技巧。在练习高难度技巧的过程中，他的分数会跌得比原先更低（也就是截图中下凹的低谷）。经过了一段时间的练习，当他掌握了高难度技巧，他的游戏得分就突然飞跃到一个新的 level。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/02/举三反一--从理论知识到实际问题的推导/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/02/举三反一--从理论知识到实际问题的推导/" itemprop="url">举三反一--从理论知识到实际问题的推导</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-02T10:30:03+08:00">
                2020-11-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="举三反一–从理论知识到实际问题的推导"><a href="#举三反一–从理论知识到实际问题的推导" class="headerlink" title="举三反一–从理论知识到实际问题的推导"></a>举三反一–从理论知识到实际问题的推导</h1><blockquote>
<p>怎么样才能获取举三反一的秘籍， 普通人为什么要案例来深化对理论知识的理解。</p>
</blockquote>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举三反一，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理解理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>对于费曼（参考费曼学习法）这样的聪明人就是很容易看到一个理论知识就能理解这个理论知识背后的本质。</p>
<p>肯定知识效率最牛逼，但是拥有这种能力的人毕竟非常少。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快地掌握一个新知识。剩下的绝大部分只能拼时间(刷题)+方法+总结等也能掌握一些知识</p>
<p>但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，即使灰色地带也行啊。</p>
<p>接下来看看TCP状态中的CLOSE_WAIT状态的含义</p>
<h2 id="先看TCP连接状态图"><a href="#先看TCP连接状态图" class="headerlink" title="先看TCP连接状态图"></a>先看TCP连接状态图</h2><p>这是网络、书本上凡是描述TCP状态一定会出现的状态图，理论上看这个图能解决任何TCP状态问题。</p>
<p><img src="/images/oss/b3d075782450b0c8d2615c5d2b75d923.png" alt="image.png"></p>
<p>反复看这个图的右下部分的CLOSE_WAIT ，从这个图里可以得到如下结论：</p>
<p><strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong></p>
<p>基本上这一结论要能帮助解决所有CLOSE_WAIT相关的问题，如果不能说明对这个知识点理解的不够。</p>
<h2 id="server端大量close-wait案例"><a href="#server端大量close-wait案例" class="headerlink" title="server端大量close_wait案例"></a>server端大量close_wait案例</h2><p>用实际案例来检查自己对CLOSE_WAIT 理论（<strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong>）的掌握 – 能不能用这个结论来解决实际问题。同时也可以看看自己从知识到问题的推理能力（跟前面的知识效率呼应一下）。</p>
<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><blockquote>
<p>服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
</blockquote>
<p>根据这个描述先不要往下看，自己推理分析下可能的原因。</p>
<p>我的推理如下：</p>
<p>从这里看起来，client跟server成功建立了somaxconn个连接（somaxconn小于backlog，所以accept queue只有这么大），但是应用没有accept这个连接，导致这些连接一直在accept queue中。但是这些连接的状态已经是ESTABLISHED了，也就是client可以发送数据了，数据发送到server后OS ack了，并放在os的tcp buffer中，应用一直没有accept也就没法读取数据。client于是发送fin（可能是超时、也可能是简单发送数据任务完成了得结束连接），这时Server上这个连接变成了CLOSE_WAIT .</p>
<p>也就是从开始到结束这些连接都在accept queue中，没有被应用accept，很快他们又因为client 发送 fin 包变成了CLOSE_WAIT ，所以始终看到的是服务端出现大量CLOSE_WAIT 并且个数正好等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）。</p>
<p>如下图所示，在连接进入accept queue后状态就是ESTABLISED了，也就是可以正常收发数据和fin了。client是感知不到server是否accept()了，只是发了数据后server的os代为保存在OS的TCP buffer中，因为应用没来取自然在CLOSE_WAIT 后应用也没有close()，所以一直维持CLOSE_WAIT 。</p>
<p>得检查server 应用为什么没有accept。</p>
<p><img src="/images/951413iMgBlog/20190706093602331.png" alt="Recv-Q和Send-Q"></p>
<p>如上是老司机的思路靠经验缺省了一些理论推理，缺省还是对理论理解不够， 这个分析抓住了 大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）但是没有抓住 CLOSE_WAIT 背后的核心原因</p>
<h3 id="更简单的推理"><a href="#更简单的推理" class="headerlink" title="更简单的推理"></a>更简单的推理</h3><p>如果没有任何实战经验，只看上面的状态图的学霸应该是这样推理的：</p>
<p>看到server上有大量的CLOSE_WAIT说明client主动断开了连接，server的OS收到client 发的fin，并回复了ack，这个过程不需要应用感知，进而连接从ESTABLISHED进入CLOSE_WAIT，此时在等待server上的应用调用close连关闭连接（处理完所有收发数据后才会调close()） —- 结论：server上的应用一直卡着没有调close().</p>
<p>同时这里很奇怪的现象： 服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn，进而可以猜测是不是连接建立后很快accept队列满了（应用也没有accept() ), 导致 大量CLOSE_WAIT 个数正好 等于somaxconn —- 结论： server 上的应用不但没有调close(), 连close() 前面必须调用 accept() 都一直卡着没调 （这个结论需要有accept()队列的理论知识)</p>
<p><strong>从上面两个结论可以清楚地看到 server的应用卡住了</strong></p>
<h3 id="实际结论："><a href="#实际结论：" class="headerlink" title="实际结论："></a>实际结论：</h3><blockquote>
<p>这个case的最终原因是因为<strong>OS的open files设置的是1024,达到了上限</strong>，进而导致server不能accept，但这个时候的tcp连接状态已经是ESTABLISHED了（这个状态变换是取决于内核收发包，跟应用是否accept()无关）。</p>
<p>同时从这里可以推断 netstat 即使看到一个tcp连接状态是ESTABLISHED也不能代表占用了 open files句柄。此时client可以正常发送数据了，只是应用服务在accept之前没法receive数据和close连接。</p>
</blockquote>
<p>这个结论的图解如下：</p>
<p><img src="/images/oss/bcf463efeb677d5749d8d7571274ee79.png" alt="image.png"></p>
<p>假如全连接队列满了，握手第三步后对于client端来说是无法感知的，client端只需要回复ack后这个连接对于client端就是ESTABLISHED了，这时client是可以发送数据的。但是Server会扔掉收到的ack，回复syn+ack给client。</p>
<p>如果全连接队列没满，但是fd不够，那么在Server端这个Socket也是ESTABLISHED，但是只是暂存在全连接队列中，等待应用来accept，这个时候client端同样无法感知这个连接没有被accept，client是可以发送数据的，这个数据会保存在tcp receive memory buffer中，等到accept后再给应用。</p>
<p>如果自己无法得到上面的分析，那么再来看看如果把 CLOSE_WAIT 状态更细化地分析下(类似有老师帮你把知识点揉开跟实际案例联系下—-未必是上面的案例)，看完后再来分析下上面的案例。</p>
<h2 id="CLOSE-WAIT-状态拆解"><a href="#CLOSE-WAIT-状态拆解" class="headerlink" title="CLOSE_WAIT 状态拆解"></a>CLOSE_WAIT 状态拆解</h2><p>通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：</p>
<ul>
<li><strong>程序问题</strong>：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。</li>
<li>响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。</li>
<li>BACKLOG 太大：此处的 backlog 不是 syn backlog，而是 accept 的 backlog，如果 backlog 太大的话，设想突然遭遇大访问量的话，即便响应速度不慢，也可能出现来不及消费的情况，导致多余的请求还在<a href="http://jaseywang.me/2014/07/20/tcp-queue-的一些问题/" target="_blank" rel="external">队列</a>里就被对方关闭了。</li>
</ul>
<p>如果你通过「netstat -ant」或者「ss -ant」命令发现了很多 CLOSE_WAIT 连接，请注意结果中的「Recv-Q」和「Local Address」字段，通常「Recv-Q」会不为空，它表示应用还没来得及接收数据，而「Local Address」表示哪个地址和端口有问题，我们可以通过「lsof -i:<port>」来确认端口对应运行的是什么程序以及它的进程号是多少。</port></p>
<p>如果是我们自己写的一些程序，比如用 HttpClient 自定义的蜘蛛，那么八九不离十是程序问题，如果是一些使用广泛的程序，比如 Tomcat 之类的，那么更可能是响应速度太慢或者 timeout 设置太小或者 BACKLOG 设置过大导致的故障。</p>
<p>看完这段 CLOSE_WAIT 更具体深入点的分析后再来分析上面的案例看看，能否推导得到正确的结论。</p>
<h2 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h2><h3 id="连接都没有被accept-client端就能发送数据了？"><a href="#连接都没有被accept-client端就能发送数据了？" class="headerlink" title="连接都没有被accept(), client端就能发送数据了？"></a>连接都没有被accept(), client端就能发送数据了？</h3><p>答：是的。只要这个连接在OS看来是ESTABLISHED的了就可以，因为握手、接收数据都是由内核完成的，内核收到数据后会先将数据放在内核的tcp buffer中，然后os回复ack。另外三次握手之后client端是没法知道server端是否accept()了。</p>
<h3 id="CLOSE-WAIT与accept-queue有关系吗？"><a href="#CLOSE-WAIT与accept-queue有关系吗？" class="headerlink" title="CLOSE_WAIT与accept queue有关系吗？"></a>CLOSE_WAIT与accept queue有关系吗？</h3><p>答：没有关系。只是本案例中因为open files不够了，影响了应用accept(), 导致accept queue满了，同时因为即使应用不accept（三次握手后，server端是否accept client端无法感知），client也能发送数据和发 fin断连接，这些响应都是os来负责，跟上层应用没关系，连接从握手到ESTABLISHED再到CLOSE_WAIT都不需要fd，也不需要应用参与。CLOSE_WAIT只跟应用不调 close() 有关系。 </p>
<h3 id="CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？"><a href="#CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？" class="headerlink" title="CLOSE_WAIT与accept queue为什么刚好一致并且联动了？"></a>CLOSE_WAIT与accept queue为什么刚好一致并且联动了？</h3><p>答：这里他们的数量刚好一致是因为所有新建连接都没有accept，堵在queue中。同时client发现问题后把所有连接都fin了，也就是所有queue中的连接从来没有被accept过，但是他们都是ESTABLISHED，过一阵子之后client端发了fin所以所有accept queue中的连接又变成了 CLOSE_WAIT, 所以二者刚好一致并且联动了</p>
<h3 id="openfiles和accept-的关系是？"><a href="#openfiles和accept-的关系是？" class="headerlink" title="openfiles和accept()的关系是？"></a>openfiles和accept()的关系是？</h3><p>答：accept()的时候才会创建文件句柄，消耗openfiles</p>
<h3 id="一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"><a href="#一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？" class="headerlink" title="一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"></a>一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？</h3><p>答：是</p>
<h3 id="如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？"><a href="#如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？" class="headerlink" title="如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？"></a>如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？</h3><p>答：对</p>
<h3 id="如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？"><a href="#如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？" class="headerlink" title="如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？"></a>如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？</h3><p>答： 能，这个时候在client和server两边看到的连接状态都是 ESTABLISHED，只是Server上的全连接队列占用加1。连接握手并切换到ESTABLISHED状态都是由OS来负责的，应用不参与，ESTABLISHED后应用才能accept，进而收发数据。也就是能放入到全连接队列里面的连接肯定都是 ESTABLISHED 状态的了</p>
<h3 id="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"><a href="#接着上面的问题，如果新连接继续连接进而全连接队列满了呢？" class="headerlink" title="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"></a>接着上面的问题，如果新连接继续连接进而全连接队列满了呢？</h3><p>答：那就连不上了，server端的OS因为全连接队列满了直接扔掉第一个syn握手包，这个时候连接在client端是SYN_SENT，Server端没有这个连接，这是因为syn到server端就直接被OS drop 了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">//如下图，本机测试，只有一个client端发起的syn_send, 3306的server端没有任何连接</div><div class="line">$netstat -antp  |grep -i 127.0.0.1:3306</div><div class="line">tcp     0   1 127.0.0.1:61106      127.0.0.1:3306    SYN_SENT    21352/telnet</div></pre></td></tr></table></figure>
<p>能进入到accept queue中的连接都是 ESTABLISHED，不管用户态有没有accept，用户态accept后队列大小减1</p>
<h3 id="如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？"><a href="#如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？" class="headerlink" title="如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？"></a>如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？</h3><p>答： 如果此时收到对方的RESET了，那么OS会释放这个连接。但是内核认为所有 listen 到的连接, 必须要 accept 走, 因为用户有权利知道有过这么一个连接存在过。所以OS不会到全连接队列拿掉这个连接，全连接队列数量也不会减1，直到应用accept这个连接，然后read/write才发现这个连接断开了，报communication failure异常</p>
<h3 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h3><p>三次握手成功就变成 ESTABLISHED 了，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>这是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</div><div class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </div><div class="line">LISTEN      73     1024            *:terabase                 *:*</div></pre></td></tr></table></figure>
<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -at |grep &quot;:12346 &quot;  </div><div class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</div><div class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</div><div class="line"># ss -lt</div><div class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </div><div class="line">LISTEN      1025   1024             *:terabase              *:*</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/09/22/kubernetes service 和 kube-proxy详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/22/kubernetes service 和 kube-proxy详解/" itemprop="url">kubernetes service 和 kube-proxy详解</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-22T17:30:03+08:00">
                2020-09-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-service-和-kube-proxy详解"><a href="#kubernetes-service-和-kube-proxy详解" class="headerlink" title="kubernetes service 和 kube-proxy详解"></a>kubernetes service 和 kube-proxy详解</h1><blockquote>
<p>service 是Kubernetes里面非常重要的一个功能，用以解决负载均衡、弹性伸缩、升级灰度等等 </p>
<p>本文先从概念介绍到实际负载均衡运转过程中追踪每个环节都做哪些处理，同时这些包会相应地怎么流转最终到达目标POD，以阐明service工作原理以及kube-proxy又在这个过程中充当了什么角色。</p>
</blockquote>
<h2 id="service-模式"><a href="#service-模式" class="headerlink" title="service 模式"></a>service 模式</h2><p>根据创建Service的<code>type</code>类型不同，可分成4种模式：</p>
<ul>
<li>ClusterIP： <strong>默认方式</strong>。根据是否生成ClusterIP又可分为普通Service和Headless Service两类：<ul>
<li><code>普通Service</code>：通过为Kubernetes的Service分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问。为最常见的方式。</li>
<li><code>Headless Service</code>：该服务不会分配Cluster IP，也不通过kube-proxy做反向代理和负载均衡。而是通过DNS提供稳定的网络ID来访问，DNS会将headless service的后端直接解析为podIP列表。主要供StatefulSet中对应POD的序列用。</li>
</ul>
</li>
<li><code>NodePort</code>：除了使用Cluster IP之外，还通过将service的port映射到集群内每个节点的相同一个端口，实现通过nodeIP:nodePort从集群外访问服务。NodePort会RR转发给后端的任意一个POD，跟ClusterIP类似</li>
<li><code>LoadBalancer</code>：和nodePort类似，不过除了使用一个Cluster IP和nodePort之外，还会向所使用的公有云申请一个负载均衡器，实现从集群外通过LB访问服务。在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。</li>
<li><code>ExternalName</code>：是 Service 的特例。此模式主要面向运行在集群外部的服务，通过它可以将外部服务映射进k8s集群，且具备k8s内服务的一些特征（如具备namespace等属性），来为集群内部提供服务。此模式要求kube-dns的版本为1.7或以上。这种模式和前三种模式（除headless service）最大的不同是重定向依赖的是dns层次，而不是通过kube-proxy。</li>
</ul>
<p>service yaml案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: nginx-ren</div><div class="line">  labels:</div><div class="line">    app: web</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line"># clusterIP: None  </div><div class="line">  ports:</div><div class="line">  - port: 8080</div><div class="line">    targetPort: 80</div><div class="line">    nodePort: 30080</div><div class="line">  selector:</div><div class="line">    app: ren</div></pre></td></tr></table></figure>
<p><code>ports</code> 字段指定服务的端口信息：</p>
<ul>
<li><code>port</code>：虚拟 ip 要绑定的 port，每个 service 会创建出来一个虚拟 ip，通过访问 <code>vip:port</code> 就能获取服务的内容。这个 port 可以用户随机选取，因为每个服务都有自己的 vip，也不用担心冲突的情况</li>
<li><code>targetPort</code>：pod 中暴露出来的 port，这是运行的容器中具体暴露出来的端口，一定不能写错–一般用name来代替具体的port</li>
<li><code>protocol</code>：提供服务的协议类型，可以是 <code>TCP</code> 或者 <code>UDP</code></li>
<li><code>nodePort</code>： 仅在type为nodePort模式下有用，宿主机暴露端口</li>
</ul>
<p>nodePort和loadbalancer可以被外部访问，loadbalancer需要一个外部ip，流量走外部ip进出</p>
<p>NodePort向外部暴露了多个宿主机的端口，外部可以部署负载均衡将这些地址配置进去。</p>
<p>默认情况下，服务会rr转发到可用的后端。如果希望保持会话（同一个 client 永远都转发到相同的 pod），可以把 <code>service.spec.sessionAffinity</code> 设置为 <code>ClientIP</code>。</p>
<h2 id="Service和kube-proxy的工作原理"><a href="#Service和kube-proxy的工作原理" class="headerlink" title="Service和kube-proxy的工作原理"></a>Service和kube-proxy的工作原理</h2><p>kube-proxy有两种主要的实现（userspace基本没有使用了）：</p>
<ul>
<li>iptables来做NAT以及负载均衡（默认方案）</li>
<li>ipvs来做NAT以及负载均衡</li>
</ul>
<p>Service 是由 kube-proxy 组件通过监听 Pod 的变化事件，在宿主机上维护iptables规则或者ipvs规则。</p>
<p>Kube-proxy 主要监听两个对象，一个是 Service，一个是 Endpoint，监听他们启停。以及通过selector将他们绑定。</p>
<p>IPVS 是专门为LB设计的。它用hash table管理service，对service的增删查找都是<em>O(1)</em>的时间复杂度。不过IPVS内核模块没有SNAT功能，因此借用了iptables的SNAT功能。IPVS 针对报文做DNAT后，将连接信息保存在nf_conntrack中，iptables据此接力做SNAT。该模式是目前Kubernetes网络性能最好的选择。但是由于nf_conntrack的复杂性，带来了很大的性能损耗。</p>
<h3 id="iptables-实现负载均衡的工作流程"><a href="#iptables-实现负载均衡的工作流程" class="headerlink" title="iptables 实现负载均衡的工作流程"></a>iptables 实现负载均衡的工作流程</h3><p>如果kube-proxy不是用的ipvs模式，那么主要靠iptables来做DNAT和SNAT以及负载均衡</p>
<p>iptables+clusterIP工作流程：</p>
<ol>
<li>集群内访问svc 10.10.35.224:3306 命中 kube-services iptables（两条规则，宿主机、以及pod内）</li>
<li>iptables 规则：KUBE-SEP-F4QDAAVSZYZMFXZQ 对应到  KUBE-SEP-F4QDAAVSZYZMFXZQ</li>
<li>KUBE-SEP-F4QDAAVSZYZMFXZQ 指示 DNAT到 宿主机：192.168.0.83:10379（在内核中将包改写了ip port）</li>
<li>从svc description中可以看到这个endpoint的地址 192.168.0.83:10379（pod使用Host network）</li>
</ol>
<p><img src="/images/oss/52e050ebb7841d70b7e3ea62e18d5b30.png" alt="image.png"></p>
<p>iptables规则解析如下（case不一样，所以看到的端口、ip都不一样）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">-t nat -A &#123;PREROUTING, OUTPUT&#125; -m conntrack --ctstate NEW -j KUBE-SERVICES</div><div class="line"></div><div class="line"># 宿主机访问 nginx Service 的流量，同时满足 4 个条件：</div><div class="line"># 1. src_ip 不是 Pod 网段</div><div class="line"># 2. dst_ip=3.3.3.3/32 (ClusterIP)</div><div class="line"># 3. proto=TCP</div><div class="line"># 4. dport=80</div><div class="line"># 如果匹配成功，直接跳转到 KUBE-MARK-MASQ；否则，继续匹配下面一条（iptables 是链式规则，高优先级在前）</div><div class="line"># 跳转到 KUBE-MARK-MASQ 是为了保证这些包出宿主机时，src_ip 用的是宿主机 IP。</div><div class="line">-A KUBE-SERVICES ! -s 1.1.0.0/16 -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-MARK-MASQ</div><div class="line"># Pod 访问 nginx Service 的流量：同时满足 4 个条件：</div><div class="line"># 1. 没有匹配到前一条的，（说明 src_ip 是 Pod 网段）</div><div class="line"># 2. dst_ip=3.3.3.3/32 (ClusterIP)</div><div class="line"># 3. proto=TCP</div><div class="line"># 4. dport=80</div><div class="line">-A KUBE-SERVICES -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-NGINX</div><div class="line"></div><div class="line"># 以 50% 的概率跳转到 KUBE-SEP-NGINX1</div><div class="line">-A KUBE-SVC-NGINX -m statistic --mode random --probability 0.50 -j KUBE-SEP-NGINX1</div><div class="line"># 如果没有命中上面一条，则以 100% 的概率跳转到 KUBE-SEP-NGINX2</div><div class="line">-A KUBE-SVC-NGINX -j KUBE-SEP-NGINX2</div><div class="line"></div><div class="line"># 如果 src_ip=1.1.1.1/32，说明是 Service-&gt;client 流量，则</div><div class="line"># 需要做 SNAT（MASQ 是动态版的 SNAT），替换 src_ip -&gt; svc_ip，这样客户端收到包时，</div><div class="line"># 看到就是从 svc_ip 回的包，跟它期望的是一致的。</div><div class="line">-A KUBE-SEP-NGINX1 -s 1.1.1.1/32 -j KUBE-MARK-MASQ</div><div class="line"># 如果没有命令上面一条，说明 src_ip != 1.1.1.1/32，则说明是 client-&gt; Service 流量，</div><div class="line"># 需要做 DNAT，将 svc_ip -&gt; pod1_ip，</div><div class="line">-A KUBE-SEP-NGINX1 -p tcp -m tcp -j DNAT --to-destination 1.1.1.1:80</div><div class="line"># 同理，见上面两条的注释</div><div class="line">-A KUBE-SEP-NGINX2 -s 1.1.1.2/32 -j KUBE-MARK-MASQ</div><div class="line">-A KUBE-SEP-NGINX2 -p tcp -m tcp -j DNAT --to-destination 1.1.1.2:80</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/dc0aa14d0eedf9f8f6f8bca1eee34cf8.png" alt="image.png"></p>
<p>在对应的宿主机上可以清楚地看到容器中的mysqld进程正好监听着 10379端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@az1-drds-83 ~]# ss -lntp |grep 10379</div><div class="line">LISTEN     0      128         :::10379                   :::*                   users:((&quot;mysqld&quot;,pid=17707,fd=18))</div><div class="line">[root@az1-drds-83 ~]# ps auxff | grep 17707 -B2</div><div class="line">root     13606  0.0  0.0  10720  3764 ?        Sl   17:09   0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc</div><div class="line"></div><div class="line">root     13624  0.0  0.0 103044 10424 ?        Ss   17:09   0:00  |   \_ python /entrypoint.py</div><div class="line">root     14835  0.0  0.0  11768  1636 ?        S    17:10   0:00  |   \_ /bin/sh /u01/xcluster/bin/mysqld_safe --defaults-file=/home/mysql/my10379.cnf</div><div class="line">alidb    17707  0.6  0.0 1269128 67452 ?       Sl   17:10   0:25  |       \_ /u01/xcluster_20200303/bin/mysqld --defaults-file=/home/mysql/my10379.cnf --basedir=/u01/xcluster_20200303 --datadir=/home/mysql/data10379/dbs10379 --plugin-dir=/u01/xcluster_20200303/lib/plugin --user=mysql --log-error=/home/mysql/data10379/mysql/master-error.log --open-files-limit=8192 --pid-file=/home/mysql/data10379/dbs10379/az1-drds-83.pid --socket=/home/mysql/data10379/tmp/mysql.sock --port=10379</div></pre></td></tr></table></figure>
<p>对应的这个pod的description：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">#kubectl describe pod apsaradbcluster010-cv6w</div><div class="line">Name:         apsaradbcluster010-cv6w</div><div class="line">Namespace:    default</div><div class="line">Priority:     0</div><div class="line">Node:         az1-drds-83/192.168.0.83</div><div class="line">Start Time:   Thu, 10 Sep 2020 17:09:33 +0800</div><div class="line">Labels:       alisql.clusterName=apsaradbcluster010</div><div class="line">              alisql.pod_name=apsaradbcluster010-cv6w</div><div class="line">              alisql.pod_role=leader</div><div class="line">Annotations:  apsara.metric.pod_name: apsaradbcluster010-cv6w</div><div class="line">Status:       Running</div><div class="line">IP:           192.168.0.83</div><div class="line">IPs:</div><div class="line">  IP:           192.168.0.83</div><div class="line">Controlled By:  ApsaradbCluster/apsaradbcluster010</div><div class="line">Containers:</div><div class="line">  engine:</div><div class="line">    Container ID:   docker://ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97</div><div class="line">    Image:          reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-engine:develop-20200910140415</div><div class="line">    Image ID:       docker://sha256:7ad5cc53c87b34806eefec829d70f5f0192f4127c7ee4e867cb3da3bb6c2d709</div><div class="line">    Ports:          10379/TCP, 20383/TCP, 46846/TCP</div><div class="line">    Host Ports:     10379/TCP, 20383/TCP, 46846/TCP</div><div class="line">    State:          Running</div><div class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</div><div class="line">    Ready:          True</div><div class="line">    Restart Count:  0</div><div class="line">    Environment:</div><div class="line">      ALISQL_POD_NAME:  apsaradbcluster010-cv6w (v1:metadata.name)</div><div class="line">      ALISQL_POD_PORT:  10379</div><div class="line">    Mounts:</div><div class="line">      /dev/shm from devshm (rw)</div><div class="line">      /etc/localtime from etclocaltime (rw)</div><div class="line">      /home/mysql/data from data-dir (rw)</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</div><div class="line">  exporter:</div><div class="line">    Container ID:  docker://b49865b7798f9036b431203d54994ac8fdfcadacb01a2ab4494b13b2681c482d</div><div class="line">    Image:         reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-exporter:latest</div><div class="line">    Image ID:      docker://sha256:432cdd0a0e7c74c6eb66551b6f6af9e4013f60fb07a871445755f6577b44da19</div><div class="line">    Port:          47272/TCP</div><div class="line">    Host Port:     47272/TCP</div><div class="line">    Args:</div><div class="line">      --web.listen-address=:47272</div><div class="line">      --collect.binlog_size</div><div class="line">      --collect.engine_innodb_status</div><div class="line">      --collect.info_schema.innodb_metrics</div><div class="line">      --collect.info_schema.processlist</div><div class="line">      --collect.info_schema.tables</div><div class="line">      --collect.info_schema.tablestats</div><div class="line">      --collect.slave_hosts</div><div class="line">    State:          Running</div><div class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</div><div class="line">    Ready:          True</div><div class="line">    Restart Count:  0</div><div class="line">    Environment:</div><div class="line">      ALISQL_POD_NAME:   apsaradbcluster010-cv6w (v1:metadata.name)</div><div class="line">      DATA_SOURCE_NAME:  root:@(127.0.0.1:10379)/</div><div class="line">    Mounts:</div><div class="line">      /dev/shm from devshm (rw)</div><div class="line">      /etc/localtime from etclocaltime (rw)</div><div class="line">      /home/mysql/data from data-dir (rw)</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</div></pre></td></tr></table></figure>
<p>DNAT 规则的作用，就是<strong>在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口</strong>。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。</p>
<h4 id="如下是一个iptables来实现service的案例中的iptables流量分配规则："><a href="#如下是一个iptables来实现service的案例中的iptables流量分配规则：" class="headerlink" title="如下是一个iptables来实现service的案例中的iptables流量分配规则："></a>如下是一个iptables来实现service的案例中的iptables流量分配规则：</h4><p>三个pod，每个pod承担三分之一的流量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">iptables-save | grep 3306</div><div class="line"></div><div class="line">iptables-save | grep KUBE-SERVICES</div><div class="line"></div><div class="line">#iptables-save |grep KUBE-SVC-RVEVH2XMONK6VC5O</div><div class="line">:KUBE-SVC-RVEVH2XMONK6VC5O - [0:0]</div><div class="line">-A KUBE-SERVICES -d 10.10.70.95/32 -p tcp -m comment --comment &quot;drds/mysql-read:mysql cluster IP&quot; -m tcp --dport 3306 -j KUBE-SVC-RVEVH2XMONK6VC5O</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-XC4TZYIZFYB653VI</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-MK4XPBZUIJGFXKED</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -j KUBE-SEP-AAYXWGQJBDHUJUQ3</div></pre></td></tr></table></figure>
<p>到这里我们基本可以看到，利用iptables规则，宿主机内核把发到宿主机上的流量按照iptables规则做dnat后发给service后端的pod，同时iptables规则可以配置每个pod的流量大小。再辅助kube-proxy监听pod的起停和健康状态并相应地更新iptables规则，这样整个service实现逻辑就很清晰了。</p>
<p>看起来 service 是个完美的方案，可以解决服务访问的所有问题，但是 service 这个方案（iptables 模式）也有自己的缺点。</p>
<p>首先，<strong>如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod</strong>，当然这个可以通过 <code>readiness probes</code> 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。</p>
<p>另外，<code>nodePort</code> 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。因为只做了NAT</p>
<h3 id="ipvs-实现负载均衡的原理"><a href="#ipvs-实现负载均衡的原理" class="headerlink" title="ipvs 实现负载均衡的原理"></a>ipvs 实现负载均衡的原理</h3><p>ipvs模式下，kube-proxy会先创建虚拟网卡，kube-ipvs0下面的每个ip都对应着svc的一个clusterIP：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># ip addr</div><div class="line">  ...</div><div class="line">5: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default </div><div class="line">    link/ether de:29:17:2a:8d:79 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 10.68.70.130/32 scope global kube-ipvs0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure>
<p>kube-ipvs0下面绑的这些ip就是在发包的时候让内核知道如果目标ip是这些地址的话，这些地址是自身的所以包不会发出去，而是给INPUT链，这样ipvs内核模块有机会改写包做完NAT后再发走。</p>
<p>ipvs会放置DNAT钩子在INPUT链上，因此必须要让内核识别 VIP 是本机的 IP。这样才会过INPUT 链，要不然就通过OUTPUT链出去了。k8s 通过kube-proxy将service cluster ip 绑定到虚拟网卡kube-ipvs0。</p>
<p>同时在路由表中增加一些ipvs 的路由条目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"># ip route show table local  //等于ip route list table local</div><div class="line">local 10.68.0.1 dev kube-ipvs0 proto kernel scope host src 10.68.0.1 </div><div class="line">local 10.68.0.2 dev kube-ipvs0 proto kernel scope host src 10.68.0.2 </div><div class="line">local 10.68.70.130 dev kube-ipvs0 proto kernel scope host src 10.68.70.130 -- ipvs</div><div class="line">broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1 </div><div class="line">local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 </div><div class="line">local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 </div><div class="line">broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 </div><div class="line">broadcast 172.17.0.0 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">local 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 </div><div class="line">broadcast 172.17.255.255 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">local 172.20.185.192 dev tunl0 proto kernel scope host src 172.20.185.192 </div><div class="line">broadcast 172.20.185.192 dev tunl0 proto kernel scope link src 172.20.185.192 </div><div class="line">broadcast 172.26.128.0 dev eth0 proto kernel scope link src 172.26.137.117 </div><div class="line">local 172.26.137.117 dev eth0 proto kernel scope host src 172.26.137.117 </div><div class="line">broadcast 172.26.143.255 dev eth0 proto kernel scope link src 172.26.137.117 </div><div class="line"></div><div class="line">//访问本机IP（不是 127.0.0.1），内核在路由项查找的时候判断类型是 RTN_LOCAL，仍然会使用 net-&gt;loopback_dev。也就是 lo 虚拟网卡。</div></pre></td></tr></table></figure>
<p>而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ipvsadm -ln |grep 10.68.114.131 -A5</div><div class="line">TCP  10.68.114.131:3306 rr</div><div class="line">  -&gt; 172.20.120.143:3306          Masq    1      0          0         </div><div class="line">  -&gt; 172.20.185.209:3306          Masq    1      0          0         </div><div class="line">  -&gt; 172.20.248.143:3306          Masq    1      0          0</div></pre></td></tr></table></figure>
<p>172.20.<em>.</em> 是后端真正pod的ip， 10.68.114.131 是cluster-ip.</p>
<p>完整的工作流程如下：</p>
<ol>
<li>因为service cluster ip 绑定到虚拟网卡kube-ipvs0上，内核可以识别访问的 VIP 是本机的 IP.</li>
<li>数据包到达INPUT链.</li>
<li>ipvs监听到达input链的数据包，比对数据包请求的服务是为集群服务，修改数据包的目标IP地址为对应pod的IP，然后将数据包发至POSTROUTING链.</li>
<li>数据包经过POSTROUTING链选路由后，将数据包通过tunl0网卡(calico网络模型)发送出去。从tunl0虚拟网卡获得源IP.</li>
<li>经过tunl0后进行ipip封包，丢到物理网络，路由到目标node（目标pod所在的node）</li>
<li>目标node进行ipip解包后给pod对应的网卡</li>
<li>pod接收到请求之后，构建响应报文，改变源地址和目的地址，返回给客户端。</li>
</ol>
<p><img src="/images/oss/51695ebb1c6b30d95f8ac8d5dcb8dd7f.png" alt="image.png"></p>
<h4 id="ipvs实际案例"><a href="#ipvs实际案例" class="headerlink" title="ipvs实际案例"></a>ipvs实际案例</h4><p>ipvs负载均衡下一次完整的syn握手抓包。</p>
<p>宿主机上访问 curl clusterip+port 后因为这个ip绑定在kube-ipvs0上，本来是应该发出去的包（prerouting）但是内核认为这个包是访问自己，于是给INPUT链，接着被ipvs放置在INPUT中的DNAT钩子勾住，将dest ip根据负载均衡逻辑改成pod-ip，然后将数据包再发至POSTROUTING链。这时因为目标ip是POD-IP了，根据ip route 选择到出口网卡是tunl0。</p>
<p>可以看下内核中的路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># ip route get 10.68.70.130</div><div class="line">local 10.68.70.130 dev lo src 10.68.70.130  //这条规则指示了clusterIP是发给自己的</div><div class="line">    cache &lt;local&gt; </div><div class="line"># ip route get 172.20.185.217</div><div class="line">172.20.185.217 via 172.26.137.117 dev tunl0 src 172.20.22.192  //这条规则指示clusterIP替换成POD IP后发给本地tunl0做ipip封包</div></pre></td></tr></table></figure>
<p>于是cip变成了tunl0的IP，这个tunl0是ipip模式，于是将这个包打包成ipip，也就是外层sip、dip都是宿主机ip，再将这个包丢入到物理网络</p>
<p><img src="/images/oss/84bbd3f10de9e7ec2266a82520876c8c.png" alt=""></p>
<p>网络收包到达内核后的处理流程如下，核心都是查路由表，出包也会查路由表（判断是否本机内部通信，或者外部通信的话需要选用哪个网卡）</p>
<h4 id="ipvs的一些分析"><a href="#ipvs的一些分析" class="headerlink" title="ipvs的一些分析"></a>ipvs的一些分析</h4><p>ipvs是一个内核态的四层负载均衡，支持NAT以及IPIP隧道模式，但LB和RS不能跨子网，IPIP性能次之，通过ipip隧道解决跨网段传输问题，因此能够支持跨子网。而NAT模式没有限制，这也是唯一一种支持端口映射的模式。</p>
<p>但是ipvs只有NAT（也就是DNAT），NAT也俗称三角模式，要求RS和LVS 在一个二层网络，并且LVS是RS的网关，这样回包一定会到网关，网关再次做SNAT，这样client看到SNAT后的src ip是LVS ip而不是RS-ip。默认实现不支持ful-NAT，所以像公有云厂商为了适应公有云场景基本都会定制实现ful-NAT模式的lvs。</p>
<p>我们不难猜想，由于Kubernetes Service需要使用端口映射功能，因此kube-proxy必然只能使用ipvs的NAT模式。</p>
<p>如下Masq表示MASQUERADE（也就是SNAT），跟iptables里面的 MASQUERADE 是一个意思</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># ipvsadm -L -n  |grep 70.130 -A12</div><div class="line">TCP  10.68.70.130:12380 rr</div><div class="line">  -&gt; 172.20.185.217:9376          Masq    1      0          0</div></pre></td></tr></table></figure>
<h2 id="为什么clusterIP不能ping通"><a href="#为什么clusterIP不能ping通" class="headerlink" title="为什么clusterIP不能ping通"></a>为什么clusterIP不能ping通</h2><p><a href="https://cizixs.com/2017/03/30/kubernetes-introduction-service-and-kube-proxy/" target="_blank" rel="external">集群内访问cluster ip（不能ping，只能cluster ip+port）就是在到达网卡之前被内核iptalbes做了dnat/snat</a>, cluster IP是一个虚拟ip，可以针对具体的服务固定下来，这样服务后面的pod可以随便变化。</p>
<p>iptables模式的svc会ping不通clusterIP，可以看如下iptables和route（留意：–reject-with icmp-port-unreachable）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#ping 10.96.229.40</div><div class="line">PING 10.96.229.40 (10.96.229.40) 56(84) bytes of data.</div><div class="line">^C</div><div class="line">--- 10.96.229.40 ping statistics ---</div><div class="line">2 packets transmitted, 0 received, 100% packet loss, time 999ms</div><div class="line"></div><div class="line">#iptables-save |grep 10.96.229.40</div><div class="line">-A KUBE-SERVICES -d 10.96.229.40/32 -p tcp -m comment --comment &quot;***-service:https has no endpoints&quot; -m tcp --dport 8443 -j REJECT --reject-with icmp-port-unreachable</div><div class="line"></div><div class="line">#ip route get 10.96.229.40</div><div class="line">10.96.229.40 via 11.164.219.253 dev eth0  src 11.164.219.119 </div><div class="line">    cache</div></pre></td></tr></table></figure>
<p>如果用ipvs实现的clusterIP是可以ping通的：</p>
<ul>
<li>如果用iptables 来做转发是ping不通的，因为iptables里面这条规则只处理tcp包，reject了icmp</li>
<li>ipvs实现的clusterIP都能ping通</li>
<li>ipvs下的clusterIP ping通了也不是转发到pod，ipvs负载均衡只转发tcp协议的包</li>
<li>ipvs 的clusterIP在本地配置了route路由到回环网卡，这个包是lo网卡回复的</li>
</ul>
<p>ipvs实现的clusterIP，在本地有添加路由到lo网卡</p>
<p><img src="/images/oss/1f5539eb4c5fa16b2f66f44056d80d7a.png" alt="image.png"></p>
<p>然后在本机抓包（到ipvs后端的pod上抓不到icmp包）：</p>
<p><img src="/images/oss/1caea5b0eb23a47241191d1b5d8c5001.png" alt="image.png"></p>
<p>从上面可以看出显然ipvs只会转发tcp包到后端pod，所以icmp包不会通过ipvs转发到pod上，同时在本地回环网卡lo上抓到了进去的icmp包。因为本地添加了一条路由规则，目标clusterIP被指示发到lo网卡上，lo网卡回复了这个ping包，所以通了。</p>
<h2 id="NodePort-Service"><a href="#NodePort-Service" class="headerlink" title="NodePort Service"></a>NodePort Service</h2><p>这种类型的 Service 也能被宿主机和 pod 访问，但与 ClusterIP 不同的是，<strong>它还能被 集群外的服务访问</strong>。</p>
<ul>
<li>External node IP + port in NodePort range to any endpoint (pod), e.g. 10.0.0.1:31000</li>
<li>Enables access from outside</li>
</ul>
<p>实现上，kube-apiserver 会<strong>从预留的端口范围内分配一个端口给 Service</strong>，然后 <strong>每个宿主机上的 kube-proxy 都会创建以下规则</strong>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-t nat -A &#123;PREROUTING, OUTPUT&#125; -m conntrack --ctstate NEW -j KUBE-SERVICES</div><div class="line"></div><div class="line">-A KUBE-SERVICES ! -s 1.1.0.0/16 -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-MARK-MASQ</div><div class="line">-A KUBE-SERVICES -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-NGINX</div><div class="line"># 如果前面两条都没匹配到（说明不是 ClusterIP service 流量），并且 dst 是 LOCAL，跳转到 KUBE-NODEPORTS</div><div class="line">-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</div><div class="line"></div><div class="line">-A KUBE-NODEPORTS -p tcp -m tcp --dport 31000 -j KUBE-MARK-MASQ</div><div class="line">-A KUBE-NODEPORTS -p tcp -m tcp --dport 31000 -j KUBE-SVC-NGINX</div><div class="line"></div><div class="line">-A KUBE-SVC-NGINX -m statistic --mode random --probability 0.50 -j KUBE-SEP-NGINX1</div><div class="line">-A KUBE-SVC-NGINX -j KUBE-SEP-NGINX2</div></pre></td></tr></table></figure>
<ol>
<li>前面几步和 ClusterIP Service 一样；如果没匹配到 ClusterIP 规则，则跳转到 <code>KUBE-NODEPORTS</code> chain。</li>
<li><code>KUBE-NODEPORTS</code> chain 里做 Service 匹配，但<strong>这次只匹配协议类型和目的端口号</strong>。</li>
<li>匹配成功后，转到对应的 <code>KUBE-SVC-</code> chain，后面的过程跟 ClusterIP 是一样的。</li>
</ol>
<h3 id="NodePort-的一些问题"><a href="#NodePort-的一些问题" class="headerlink" title="NodePort 的一些问题"></a>NodePort 的一些问题</h3><ul>
<li>首先endpoint回复不能走node 1给client，因为会被client reset（如果在node1上将src ip替换成node2的ip可能会路由不通）。回复包在 node1上要snat给node2</li>
<li>经过snat后endpoint没法拿到client ip（slb之类是通过option带过来）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">          client</div><div class="line">            \ ^</div><div class="line">             \ \</div><div class="line">              v \</div><div class="line">  node 1 &lt;--- node 2</div><div class="line">   | ^   SNAT</div><div class="line">   | |   ---&gt;</div><div class="line">   v |</div><div class="line">endpoint</div></pre></td></tr></table></figure>
<p>可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。</p>
<p>而这个机制的实现原理也非常简单：这时候，<strong>一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod</strong>。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">      client</div><div class="line">      ^ /   \</div><div class="line">     / /     \</div><div class="line">    / v       X</div><div class="line">  node 1     node 2</div><div class="line">   ^ |</div><div class="line">   | |</div><div class="line">   | v</div><div class="line">endpoint</div></pre></td></tr></table></figure>
<p>当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。</p>
<h2 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h2><p>在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#iptables-代理模式" target="_blank" rel="external">iptables 代理模式</a>，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#ipvs-代理模式" target="_blank" rel="external">ipvs 代理模式</a></p>
<p>kube-proxy相当于service的管理方，业务流量不会走到kube-proxy，业务流量的负载均衡都是由内核层面的iptables或者ipvs来分发。</p>
<p>kube-proxy的三种模式：</p>
<p><img src="/images/oss/075e2955c5fbd08986bd34afaa5034ba.png" alt="image.png"></p>
<p><strong>一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。</strong></p>
<p>ipvs 就是用于解决在大量 Service 时，iptables 规则同步变得不可用的性能问题。与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，这也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能。</p>
<p>除了能够提升性能之外，ipvs 也提供了多种类型的负载均衡算法，除了最常见的 Round-Robin 之外，还支持最小连接、目标哈希、最小延迟等算法，能够很好地提升负载均衡的效率。</p>
<p>而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，“将重要操作放入内核态”是提高性能的重要手段。</p>
<p><strong>IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。</strong></p>
<p>ipvs 和 iptables 都是基于 Netfilter 实现的。</p>
<p>Kubernetes 中已经使用 ipvs 作为 kube-proxy 的默认代理模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/opt/kube/bin/kube-proxy --bind-address=172.26.137.117 --cluster-cidr=172.20.0.0/16 --hostname-override=172.26.137.117 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --logtostderr=true --proxy-mode=ipvs</div></pre></td></tr></table></figure>
<p><img src="/images/oss/c44c8b3fbb1b2e0910872a6aecef790c.png" alt="image.png"></p>
<h2 id="port-forward"><a href="#port-forward" class="headerlink" title="port-forward"></a>port-forward</h2><p>port-forward后外部也能够像nodePort一样访问到，但是port-forward不适合大流量，一般用于管理端口，启动的时候port-forward会固定转发到一个具体的Pod上，也没有负载均衡的能力。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#在本机监听1080端口，并转发给后端的svc/nginx-ren(总是给发给svc中的一个pod)</div><div class="line">kubectl port-forward --address 0.0.0.0 svc/nginx-ren 1080:80</div></pre></td></tr></table></figure>
<p><code>kubectl</code> looks up a Pod from the service information provided on the command line and forwards directly to a Pod rather than forwarding to the ClusterIP/Service port and allowing the cluster to load balance the service like regular service traffic.</p>
<p>The <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L225" target="_blank" rel="external">portforward.go <code>Complete</code> function</a> is where <code>kubectl portforward</code> does the first look up for a pod from options via <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L254" target="_blank" rel="external"><code>AttachablePodForObjectFn</code></a>:</p>
<p>The <code>AttachablePodForObjectFn</code> is defined as <code>attachablePodForObject</code> in <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/interface.go#L39-L40" target="_blank" rel="external">this interface</a>, then here is the <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go" target="_blank" rel="external"><code>attachablePodForObject</code> function</a>.</p>
<p>To my (inexperienced) Go eyes, it appears the <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go" target="_blank" rel="external"><code>attachablePodForObject</code></a> is the thing <code>kubectl</code> uses to look up a Pod to from a Service defined on the command line.</p>
<p>Then from there on everything deals with filling in the Pod specific <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L46-L58" target="_blank" rel="external"><code>PortForwardOptions</code></a> (which doesn’t include a service) and is passed to the kubernetes API.</p>
<h2 id="Service-和-DNS-的关系"><a href="#Service-和-DNS-的关系" class="headerlink" title="Service 和 DNS 的关系"></a>Service 和 DNS 的关系</h2><p>Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。</p>
<p>对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。</p>
<p>而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">#kubectl get pod -l app=mysql-r -o wide</div><div class="line">NAME        READY   STATUS    RESTARTS IP               NODE          </div><div class="line">mysql-r-0   2/2     Running   0        172.20.120.143   172.26.137.118</div><div class="line">mysql-r-1   2/2     Running   4        172.20.248.143   172.26.137.116</div><div class="line">mysql-r-2   2/2     Running   0        172.20.185.209   172.26.137.117</div><div class="line"></div><div class="line">/ # nslookup mysql-r-1.mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r-1.mysql-r</div><div class="line">Address 1: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</div><div class="line">/ # </div><div class="line">/ # nslookup mysql-r-2.mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r-2.mysql-r</div><div class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</div><div class="line"></div><div class="line">#如果service是headless(也就是明确指定了 clusterIP: None)</div><div class="line">/ # nslookup mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r</div><div class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</div><div class="line">Address 2: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</div><div class="line">Address 3: 172.20.120.143 mysql-r-0.mysql-r.default.svc.cluster.local</div><div class="line"></div><div class="line">#如果service 没有指定 clusterIP: None，也就是会分配一个clusterIP给集群</div><div class="line">/ # nslookup mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r</div><div class="line">Address 1: 10.68.90.172 mysql-r.default.svc.cluster.local</div></pre></td></tr></table></figure>
<p>不是每个pod都会向DNS注册，只有：</p>
<ul>
<li>StatefulSet中的POD会向dns注册，因为他们要保证顺序行</li>
<li>POD显式指定了hostname和subdomain，说明要靠hostname/subdomain来解析</li>
<li>Headless Service代理的POD也会注册</li>
</ul>
<h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p> <code>kube-proxy</code> 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 <a href="https://jimmysong.io/kubernetes-handbook/concepts/cni.html" target="_blank" rel="external">CNI</a> 创建的外网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/ingress.html" target="_blank" rel="external">ingress</a> 这个资源对象，它由位于 Kubernetes <a href="https://jimmysong.io/kubernetes-handbook/practice/edge-node-configuration.html" target="_blank" rel="external">边缘节点</a>（这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理<strong>南北向流量</strong>，Ingress 必须对接各种 Ingress Controller 才能使用，比如 <a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="external">nginx ingress controller</a>、<a href="https://traefik.io/" target="_blank" rel="external">traefik</a>。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 <a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/" target="_blank" rel="external">nginx ingress controller</a>，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。</p>
<p>Ingress是授权入站连接到达集群服务的规则集合。 你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。 用户通过POST Ingress资源到API server的方式来请求ingress。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> internet</div><div class="line">     |</div><div class="line">[ Ingress ]</div><div class="line">--|-----|--</div><div class="line">[ Services ]</div></pre></td></tr></table></figure>
<p>可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers" target="_blank" rel="external">Ingress 控制器</a> 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。</p>
<p>Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#nodeport" target="_blank" rel="external">Service.Type=NodePort</a> 或 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#loadbalancer" target="_blank" rel="external">Service.Type=LoadBalancer</a> 类型的服务。</p>
<p>Ingress 其实不是Service的一个类型，但是它可以作用于多个Service，作为集群内部服务的入口。Ingress 能做许多不同的事，比如根据不同的路由，将请求转发到不同的Service上等等。</p>
<p><img src="/images/oss/0e100056910df8cfc45403a05838dd34.png" alt="image.png"></p>
<p> Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Ingress</div><div class="line">metadata:</div><div class="line">  name: cafe-ingress</div><div class="line">spec:</div><div class="line">  tls:</div><div class="line">  - hosts:</div><div class="line">    - cafe.example.com</div><div class="line">    secretName: cafe-secret</div><div class="line">  rules:</div><div class="line">  - host: cafe.example.com</div><div class="line">    http:</div><div class="line">      paths:</div><div class="line">      - path: /tea              --入口url路径</div><div class="line">        backend:</div><div class="line">          serviceName: tea-svc  --对应的service</div><div class="line">          servicePort: 80</div><div class="line">      - path: /coffee</div><div class="line">        backend:</div><div class="line">          serviceName: coffee-svc</div><div class="line">          servicePort: 80</div></pre></td></tr></table></figure>
<p>在实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可。然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。</p>
<p>目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。</p>
<p>一个 Ingress Controller 可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。</p>
<h2 id="eBPF（extended-Berkeley-Packet-Filter）和网络"><a href="#eBPF（extended-Berkeley-Packet-Filter）和网络" class="headerlink" title="eBPF（extended Berkeley Packet Filter）和网络"></a>eBPF（extended Berkeley Packet Filter）和网络</h2><p>eBPF允许程序<strong>对内核本身进行编程</strong>（即 通过程序动态修改内核的行为。传统方式要么是<strong>给内核打补丁</strong>，要么是<strong>修改内核源码 重新编译</strong>）。一句话来概括：<strong>编写代码监听内核事件，当事件发生时，BPF 代码就会在内核执行</strong>。</p>
<p>eBPF 最早出现在 3.18 内核中，此后原来的 BPF 就被称为 <strong>“经典” BPF</strong>（classic BPF, cBPF），cBPF 现在基本已经废弃了。很多人知道 cBPF 是因为它是 <code>tcpdump</code> 的包过滤语言。<strong>现在，Linux 内核只运行 eBPF，内核会将加载的 cBPF 字节码 透明地转换成 eBPF 再执行</strong>。如无特殊说明，本文中所说的 BPF 都是泛指 BPF 技术。</p>
<p>2015年<strong>eBPF 添加了一个新 fast path：XDP</strong>，XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码（在软件中最早可以处理包的位置），而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。</p>
<p>BPF 当时用于 tcpdump，在内核中尽量前面的位置抓包，它不会 crash 内核；</p>
<p>bcc 是 tracing frontend for eBPF。</p>
<p>内核添加了一个新 socket 类型 AF_XDP。它提供的能力是：在零拷贝（ zero-copy）的前提下将包从网卡驱动送到用户空间。</p>
<p>AF_XDP 提供的能力与 DPDK 有点类似，不过：</p>
<ul>
<li>DPDK 需要重写网卡驱动，需要额外维护用户空间的驱动代码。</li>
<li>AF_XDP 在复用内核网卡驱动的情况下，能达到与 DPDK 一样的性能。</li>
</ul>
<p>而且由于复用了内核基础设施，所有的网络管理工具还都是可以用的，因此非常方便， 而 DPDK 这种 bypass 内核的方案导致绝大大部分现有工具都用不了了。</p>
<p>由于所有这些操作都是发生在 XDP 层的，因此它称为 AF_XDP。插入到这里的 BPF 代码 能直接将包送到 socket。</p>
<p>Facebook 公布了生产环境 XDP+eBPF 使用案例（DDoS &amp; LB）</p>
<ul>
<li>用 XDP/eBPF 重写了原来基于 IPVS 的 L4LB，性能 10x。</li>
<li>eBPF 经受住了严苛的考验：从 2017 开始，每个进入 facebook.com 的包，都是经过了 XDP &amp; eBPF 处理的。</li>
</ul>
<p><strong>Cilium 1.6 发布</strong> 第一次支持完全干掉基于 iptables 的 kube-proxy，全部功能基于 eBPF。Cilium 1.8 支持基于 XDP 的 Service 负载均衡和 host network policies。</p>
<p>传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：</p>
<p><img src="/images/oss/67851ecb88fca18b9745dae4948947a5.png" alt="image.png"></p>
<p>步骤：</p>
<ol>
<li>网卡收到一个包（通过 DMA 放到 ring-buffer）。</li>
<li>包经过 XDP hook 点。</li>
<li>内核给包分配内存，此时才有了大家熟悉的 skb（包的内核结构体表示），然后 送到内核协议栈。</li>
<li>包经过 GRO 处理，对分片包进行重组。</li>
<li>包进入 tc（traffic control）的 ingress hook。接下来，所有橙色的框都是 Netfilter 处理点。</li>
<li>Netfilter：在 PREROUTING hook 点处理 raw table 里的 iptables 规则。</li>
<li>包经过内核的连接跟踪（conntrack）模块。</li>
<li>Netfilter：在 PREROUTING hook 点处理 mangle table 的 iptables 规则。</li>
<li>Netfilter：在 PREROUTING hook 点处理 nat table 的 iptables 规则。</li>
<li>进行路由判断（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。</li>
<li>Netfilter：在 FORWARD hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 FORWARD hook 点处理 filter table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 nat table 里的iptables 规则。</li>
<li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。</li>
<li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：发送到一个本机 veth 设备，或者一个本机 service endpoint， 或者，如果目的 IP 是主机外，就通过网卡发出去。</li>
</ol>
<h3 id="Cilium-如何处理POD之间的流量（东西向流量）"><a href="#Cilium-如何处理POD之间的流量（东西向流量）" class="headerlink" title="Cilium 如何处理POD之间的流量（东西向流量）"></a>Cilium 如何处理POD之间的流量（东西向流量）</h3><p><img src="/images/oss/f6efb2e51abbd2c88a099ee9dc942d37.png" alt="image.png"></p>
<p>如上图所示，Socket 层的 BPF 程序主要处理 Cilium 节点的东西向流量（E-W）。</p>
<ul>
<li>将 Service 的 IP:Port 映射到具体的 backend pods，并做负载均衡。</li>
<li>当应用发起 connect、sendmsg、recvmsg 等请求（系统调用）时，拦截这些请求， 并根据请求的IP:Port 映射到后端 pod，直接发送过去。反向进行相反的变换。</li>
</ul>
<p>这里实现的好处：性能更高。</p>
<ul>
<li>不需要包级别（packet leve）的地址转换（NAT）。在系统调用时，还没有创建包，因此性能更高。</li>
<li>省去了 kube-proxy 路径中的很多中间节点（intermediate node hops） 可以看出，应用对这种拦截和重定向是无感知的（符合 Kubernetes Service 的设计）。</li>
</ul>
<h3 id="Cilium处理外部流量（南北向流量）"><a href="#Cilium处理外部流量（南北向流量）" class="headerlink" title="Cilium处理外部流量（南北向流量）"></a>Cilium处理外部流量（南北向流量）</h3><p><img src="/images/oss/e013d356145d1be6d6a69e2f1b32bdc8.png" alt="image.png"></p>
<p>集群外来的流量到达 node 时，由 XDP 和 tc 层的 BPF 程序进行处理， 它们做的事情与 socket 层的差不多，将 Service 的 IP:Port 映射到后端的 PodIP:Port，如果 backend pod 不在本 node，就通过网络再发出去。发出去的流程我们 在前面 Cilium eBPF 包转发路径 讲过了。</p>
<p>这里 BPF 做的事情：执行 DNAT。这个功能可以在 XDP 层做，也可以在 TC 层做，但 在XDP 层代价更小，性能也更高。</p>
<p>总结起来，Cilium的核心理念就是：</p>
<ul>
<li>将东西向流量放在离 socket 层尽量近的地方做。</li>
<li>将南北向流量放在离驱动（XDP 和 tc）层尽量近的地方做。</li>
</ul>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p>测试环境：两台物理节点，一个发包，一个收包，收到的包做 Service loadbalancing 转发给后端 Pods。</p>
<p><img src="/images/oss/1b69dfd206a91dc4007781163fd55f41.png" alt="image.png"></p>
<p>可以看出：</p>
<ul>
<li>Cilium XDP eBPF 模式能处理接收到的全部 10Mpps（packets per second）。</li>
<li>Cilium tc eBPF 模式能处理 3.5Mpps。</li>
<li>kube-proxy iptables 只能处理 2.3Mpps，因为它的 hook 点在收发包路径上更后面的位置。</li>
<li>kube-proxy ipvs 模式这里表现更差，它相比 iptables 的优势要在 backend 数量很多的时候才能体现出来。</li>
</ul>
<p>cpu：</p>
<ul>
<li>XDP 性能最好，是因为 XDP BPF 在驱动层执行，不需要将包 push 到内核协议栈。</li>
<li>kube-proxy 不管是 iptables 还是 ipvs 模式，都在处理软中断（softirq）上消耗了大量 CPU。</li>
</ul>
<h2 id="利用-ebpf-sockmap-redirection-提升-socket-性能"><a href="#利用-ebpf-sockmap-redirection-提升-socket-性能" class="headerlink" title="利用 ebpf sockmap/redirection 提升 socket 性能"></a><a href="http://arthurchiao.art/blog/socket-acceleration-with-ebpf-zh/?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io#11-bpf-%E5%9F%BA%E7%A1%80" target="_blank" rel="external">利用 ebpf sockmap/redirection 提升 socket 性能</a></h2><p>通过bpf监听socket来拦截所有sendmsg事件，如果是发送到本地另一个socket那么bpf就绕过TCP/IP协议栈，直接将msg送给对方socket。依赖用cgroups来指定监听的sockets事件</p>
<p><img src="/images/951413iMgBlog/sock-redir.png" alt="img"></p>
<p>实现这个功能依赖两个东西：</p>
<ol>
<li><p>sockmap：这是一个存储 socket 信息的映射表。作用：</p>
<ul>
<li>一段 BPF 程序<strong>监听所有的内核 socket 事件</strong>，并将新建的 socket 记录到这个 map；</li>
<li>另一段 BPF 程序<strong>拦截所有 <code>sendmsg</code> 系统调用</strong>，然后去 map 里查找 socket 对端，之后 调用 BPF 函数绕过 TCP/IP 协议栈，直接将数据发送到对端的 socket queue。</li>
</ul>
</li>
<li><p>cgroups：绑定cgroup下的进程，从而将这些进程下的所有 socket 加入到sockmap。</p>
</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/" target="_blank" rel="external">https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/</a> Kubernetes 网络疑难杂症排查方法</p>
<p><a href="https://blog.csdn.net/qq_36183935/article/details/90734936" target="_blank" rel="external">https://blog.csdn.net/qq_36183935/article/details/90734936</a>  kube-proxy ipvs模式详解</p>
<p><a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/ebpf-and-k8s-zh/</a>  大规模微服务利器：eBPF 与 Kubernetes</p>
<p><a href="http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/</a>  Life of a Packet in Cilium：实地探索 Pod-to-Service 转发路径及 BPF 处理逻辑</p>
<p><a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/</a>  深入理解 Cilium 的 eBPF 收发包路径（datapath）（KubeCon, 2019）</p>
<p><a href="http://arthurchiao.art/blog/socket-acceleration-with-ebpf-zh/?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io#11-bpf-%E5%9F%BA%E7%A1%80" target="_blank" rel="external">利用 ebpf sockmap/redirection 提升 socket 性能</a> </p>
<p><a href="http://arthurchiao.art/blog/cilium-scale-k8s-service-with-bpf-zh/" target="_blank" rel="external">利用 eBPF 支撑大规模 K8s Service (LPC, 2019)</a></p>
<p><a href="https://jiayu0x.com/2014/12/02/iptables-essential-summary/" target="_blank" rel="external">https://jiayu0x.com/2014/12/02/iptables-essential-summary/</a></p>
<p><a href="https://k8s.imroc.io/" target="_blank" rel="external">imroc 电子书</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/09/16/RT都去哪了/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/16/RT都去哪了/" itemprop="url">delay ack拉高实际rt的case</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-16T17:30:03+08:00">
                2020-09-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="delay-ack拉高实际rt的case"><a href="#delay-ack拉高实际rt的case" class="headerlink" title="delay ack拉高实际rt的case"></a>delay ack拉高实际rt的case</h2><h2 id="案例描述"><a href="#案例描述" class="headerlink" title="案例描述"></a>案例描述</h2><blockquote>
<p>开发人员发现client到server的rtt是2.5ms，每个请求1ms server就能处理完毕，但是监控发现的rt不是3.5（1+2.5），而是6ms，想知道这个6ms怎么来的？</p>
</blockquote>
<p>如下业务监控图：实际处理时间（逻辑服务时间1ms，rtt2.4ms，加起来3.5ms），但是系统监控到的rt（蓝线）是6ms，如果一个请求分很多响应包串行发给client，这个6ms是正常的（1+2.4*N），但实际上如果send buffer足够的话，按我们前面的理解多个响应包会并发发出去，所以如果整个rt是3.5ms才是正常的。</p>
<p><img src="/images/oss/d56f87a19a10b0ac9a3b7009641247a0.png" alt="image.png"></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>抓包来分析原因：</p>
<p><img src="/images/oss/d5e2e358dd1a24e104f54815c84875c9.png" alt="image.png"></p>
<p>实际看到大量的response都是3.5ms左右，符合我们的预期，但是有少量rt被delay ack严重影响了</p>
<p>从下图也可以看到有很多rtt超过3ms的，这些超长时间的rtt会最终影响到整个服务rt</p>
<p><img src="/images/oss/48eae3dcd7c78a68b0afd5c66f783f23.png" alt="image.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/08/31/kubernetes容器网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/31/kubernetes容器网络/" itemprop="url">kubernetes容器网络</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-31T11:30:03+08:00">
                2020-08-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-容器网络"><a href="#kubernetes-容器网络" class="headerlink" title="kubernetes 容器网络"></a>kubernetes 容器网络</h1><h2 id="cni-网络"><a href="#cni-网络" class="headerlink" title="cni 网络"></a>cni 网络</h2><blockquote>
<p> <strong>cni0</strong> is a Linux network bridge device, all <strong>veth</strong> devices will connect to this bridge, so all Pods on the same node can communicate with each other, as explained in <strong>Kubernetes Network Model</strong> and the hotel analogy above.</p>
</blockquote>
<h3 id="cni（Container-Network-Interface）"><a href="#cni（Container-Network-Interface）" class="headerlink" title="cni（Container Network Interface）"></a>cni（Container Network Interface）</h3><p>CNI 全称为 Container Network Interface，是用来定义容器网络的一个 <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md" target="_blank" rel="external">规范</a>。<a href="https://github.com/containernetworking/cni" target="_blank" rel="external">containernetworking/cni</a> 是一个 CNCF 的 CNI 实现项目，包括基本额 bridge,macvlan等基本网络插件。</p>
<p>一般将cni各种网络插件的可执行文件二进制放到 <code>/opt/cni/bin</code> ，在 <code>/etc/cni/net.d/</code> 下创建配置文件，剩下的就交给 K8s 或者 containerd 了，我们不关心也不了解其实现。</p>
<p>比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#ls -lh /opt/cni/bin/</div><div class="line">总用量 90M</div><div class="line">-rwxr-x--- 1 root root 4.0M 12月 23 09:39 bandwidth</div><div class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico</div><div class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico-ipam</div><div class="line">-rwxr-x--- 1 root root 3.0M 12月 23 09:39 flannel</div><div class="line">-rwxr-x--- 1 root root 3.5M 12月 23 09:39 host-local</div><div class="line">-rwxr-x--- 1 root root 3.1M 12月 23 09:39 loopback</div><div class="line">-rwxr-x--- 1 root root 3.8M 12月 23 09:39 portmap</div><div class="line">-rwxr-x--- 1 root root 3.3M 12月 23 09:39 tuning</div><div class="line"></div><div class="line">[root@hygon3 15:55 /root]</div><div class="line">#ls -lh /etc/cni/net.d/</div><div class="line">总用量 12K</div><div class="line">-rw-r--r-- 1 root root  607 12月 23 09:39 10-calico.conflist</div><div class="line">-rw-r----- 1 root root  292 12月 23 09:47 10-flannel.conflist</div><div class="line">-rw------- 1 root root 2.6K 12月 23 09:39 calico-kubeconfig</div></pre></td></tr></table></figure>
<p>CNI 插件都是直接通过 exec 的方式调用，而不是通过 socket 这样 C/S 方式，所有参数都是通过环境变量、标准输入输出来实现的。</p>
<p>Step-by-step communication from <strong>Pod 1</strong> to <strong>Pod 6</strong>:</p>
<ol>
<li><em>Package leaves</em> <strong><em>Pod 1 netns\</em></strong> <em>through the</em> <strong><em>eth1\</em></strong> <em>interface and reaches the</em> <strong><em>root netns\</em></strong> <em>through the virtual interface</em> <strong><em>veth1*</em></strong>;*</li>
<li><em>Package leaves</em> <strong><em>veth1\</em></strong> <em>and reaches</em> <strong><em>cni0*</em></strong>, looking for<em> **</em>Pod 6*<em>**’s</em> <em>address;</em></li>
<li><em>Package leaves</em> <strong><em>cni0\</em></strong> <em>and is redirected to</em> <strong><em>eth0*</em></strong>;*</li>
<li><em>Package leaves</em> <strong><em>eth0\</em></strong> <em>from</em> <strong><em>Master 1\</em></strong> <em>and reaches the</em> <strong><em>gateway*</em></strong>;*</li>
<li><em>Package leaves the</em> <strong><em>gateway\</em></strong> <em>and reaches the</em> <strong><em>root netns\</em></strong> <em>through the</em> <strong><em>eth0\</em></strong> <em>interface on</em> <strong><em>Worker 1*</em></strong>;*</li>
<li><em>Package leaves</em> <strong><em>eth0\</em></strong> <em>and reaches</em> <strong><em>cni0*</em></strong>, looking for<em> **</em>Pod 6*<em>**’s</em> <em>address;</em></li>
<li><em>Package leaves</em> <strong><em>cni0\</em></strong> <em>and is redirected to the</em> <strong><em>veth6\</em></strong> <em>virtual interface;</em></li>
<li><em>Package leaves the</em> <strong><em>root netns\</em></strong> <em>through</em> <strong><em>veth6\</em></strong> <em>and reaches the</em> <strong><em>Pod 6 netns\</em></strong> <em>though the</em> <strong><em>eth6\</em></strong> <em>interface;</em></li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115124747936.png" alt="image-20220115124747936"></p>
<h2 id="flannel-网络"><a href="#flannel-网络" class="headerlink" title="flannel 网络"></a>flannel 网络</h2><p>假如POD1访问POD4：</p>
<ol>
<li>从POD1中出来的包先到Bridge cni0上（因为POD1对应的veth挂在了cni0上）</li>
<li>然后进入到宿主机网络，宿主机有路由 10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink ，也就是目标ip 10.244.2.3的包交由 flannel.1 来处理</li>
<li>flanneld 进程将包封装成vxlan 丢到eth0从宿主机1离开（封装后的目标ip是192.168.2.91）</li>
<li>这个封装后的vxlan udp包正确路由到宿主机2</li>
<li>然后经由 flanneld 解包成 10.244.2.3 ，命中宿主机2上的路由：10.244.2.0/24 dev cni0 proto kernel scope link src 10.244.2.1 ，交给cni0（<strong>这里会过宿主机iptables</strong>）</li>
<li>cni0将包送给POD4</li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115132938290.png" alt="image-20220115132938290"></p>
<p>对应宿主机查询到的ip、路由信息（和上图不是对应的）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#ip -d -4 addr show cni0</div><div class="line">475: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</div><div class="line">    link/ether 8e:34:ba:e2:a4:c6 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</div><div class="line">    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q bridge_id 8000.8e:34:ba:e2:a4:c6 designated_root 8000.8e:34:ba:e2:a4:c6 root_port 0 root_path_cost 0 topology_change 0 topology_change_detected 0 hello_timer    0.00 tcn_timer    0.00 topology_change_timer    0.00 gc_timer  161.46 vlan_default_pvid 1 vlan_stats_enabled 0 group_fwd_mask 0 group_address 01:80:c2:00:00:00 mcast_snooping 1 mcast_router 1 mcast_query_use_ifaddr 0 mcast_querier 0 mcast_hash_elasticity 4 mcast_hash_max 512 mcast_last_member_count 2 mcast_startup_query_count 2 mcast_last_member_interval 100 mcast_membership_interval 26000 mcast_querier_interval 25500 mcast_query_interval 12500 mcast_query_response_interval 1000 mcast_startup_query_interval 3124 mcast_stats_enabled 0 mcast_igmp_version 2 mcast_mld_version 1 nf_call_iptables 0 nf_call_ip6tables 0 nf_call_arptables 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</div><div class="line">    inet 192.168.3.1/24 brd 192.168.3.255 scope global cni0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line"></div><div class="line">#ip -d -4 addr show flannel.1</div><div class="line">474: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</div><div class="line">    link/ether fe:49:64:ae:36:af brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</div><div class="line">    vxlan id 1 local 10.133.2.252 dev bond0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</div><div class="line">    inet 192.168.3.0/32 brd 192.168.3.0 scope global flannel.1</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">       </div><div class="line">[root@hygon239 20:06 /root]</div><div class="line">#kubectl describe node hygon252 | grep -C5 -i vtep  //可以看到VetpMAC 以及对应的宿主机IP（vxlan封包后的IP）</div><div class="line">Labels:             beta.kubernetes.io/arch=amd64</div><div class="line">                    beta.kubernetes.io/os=linux</div><div class="line">                    kubernetes.io/arch=amd64</div><div class="line">                    kubernetes.io/hostname=hygon252</div><div class="line">                    kubernetes.io/os=linux</div><div class="line">Annotations:        flannel.alpha.coreos.com/backend-data: &#123;&quot;VNI&quot;:1,&quot;VtepMAC&quot;:&quot;fe:49:64:ae:36:af&quot;&#125;</div><div class="line">                    flannel.alpha.coreos.com/backend-type: vxlan</div><div class="line">                    flannel.alpha.coreos.com/kube-subnet-manager: true</div><div class="line">                    flannel.alpha.coreos.com/public-ip: 10.133.2.252</div><div class="line">                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock</div><div class="line">                    node.alpha.kubernetes.io/ttl: 0</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20220115133500854.png" alt="image-20220115133500854"></p>
<h2 id="kubernetes-calico-网络"><a href="#kubernetes-calico-网络" class="headerlink" title="kubernetes calico 网络"></a>kubernetes calico 网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</div><div class="line"></div><div class="line">#或者老版本的calico</div><div class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</div></pre></td></tr></table></figure>
<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>跨宿主机的两个容器之间的流量链路是：</p>
<blockquote>
<p>cali-容器eth0-&gt;宿主机cali27dce37c0e8-&gt;tunl0-&gt;内核ipip模块封包-&gt;物理网卡（ipip封包后）—远程–&gt; 物理网卡-&gt;内核ipip模块解包-&gt;tunl0-&gt;cali-容器</p>
</blockquote>
<p><img src="/images/oss/a1767a5f2cbc2c48c1a35da9f3232a2c.png" alt="image.png"></p>
<p>Calico IPIP模式对物理网络无侵入，符合云原生容器网络要求；使用IPIP封包，性能略低于Calico BGP模式；无法使用传统防火墙管理、也无法和存量网络直接打通。Pod在Node做SNAT访问外部，Pod流量不易被监控。</p>
<h2 id="calico-ipip网络不通"><a href="#calico-ipip网络不通" class="headerlink" title="calico ipip网络不通"></a>calico ipip网络不通</h2><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，部分节点之间不通。每台机器部署好calico网络后，会分配一个 /26 CIRD 子网（64个ip）。</p>
<h3 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h3><p>目标机是10.122.127.128（宿主机ip 192.168.3.112），如果从10.122.17.64（宿主机ip 192.168.3.110） ping 10.122.127.128不通，查看10.122.127.128路由表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</div><div class="line">10.122.17.64/26 via 10.122.127.128 dev tunl0  //这条路由不通</div><div class="line">[root@az3-k8s-13 ~]# ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</div><div class="line"></div><div class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</div><div class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink //这样就通了</div></pre></td></tr></table></figure>
<p>在10.122.127.128抓包如下，明显可以看到icmp request到了 tunl0网卡，tunl0网卡也回复了，但是回复包没有经过kernel ipip模块封装后发到eth1上：</p>
<p><img src="/images/oss/d3111417ce646ca1475def5bea01e6b9.png" alt="image.png"></p>
<p>正常机器应该是这样，上图不正常的时候缺少红框中的reply：</p>
<p><img src="/images/oss/9ea9041af1211b2a5b8de4e216044465.png" alt="image.png"></p>
<p>解决：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; </div><div class="line">ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</div></pre></td></tr></table></figure>
<p>删除错误路由增加新的路由就可以了，新增路由的意思是从tunl0发给10.122.17.64/26的包下一跳是 192.168.3.110。</p>
<p> via 192.168.3.110 表示下一跳的ip</p>
<p>onlink参数的作用：<br>使用这个参数将会告诉内核，不必检查网关是否可达。因为在linux内核中，网关与本地的网段不同是被认为不可达的，从而拒绝执行添加路由的操作。</p>
<p>因为tunl0网卡ip的 CIDR 是32，也就是不属于任何子网，那么这个网卡上的路由没有网关，配置路由的话必须是onlink, 内核存也没法根据子网来选择到这块网卡，所以还会加上 dev 指定网卡。</p>
<h3 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h3><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，只有node2没有192.168.3.111这个ip，结果node2跟其他节点都不通：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#calicoctl node status</div><div class="line">Calico process is running.</div><div class="line"></div><div class="line">IPv4 BGP status</div><div class="line">+---------------+-------------------+-------+------------+-------------+</div><div class="line">| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |</div><div class="line">+---------------+-------------------+-------+------------+-------------+</div><div class="line">| 192.168.0.111 | node-to-node mesh | up    | 2020-08-29 | Established |</div><div class="line">| 192.168.3.112 | node-to-node mesh | up    | 2020-08-29 | Established |</div><div class="line">| 192.168.3.113 | node-to-node mesh | up    | 2020-08-29 | Established |</div><div class="line">| 192.168.3.114 | node-to-node mesh | up    | 2020-08-29 | Established |</div><div class="line">+---------------+-------------------+-------+------------+-------------+</div></pre></td></tr></table></figure>
<p>从node4 ping node2，然后在node2上抓包，可以看到 icmp request都发到了node2上，但是node2收到后没有发给tunl0：</p>
<p><img src="/images/oss/16fda9322e9a59c37c11629acc611bf3.png" alt="image.png"></p>
<p>所以icmp没有回复，这里的问题在于<strong>kernel收到包后为什么不给tunl0</strong></p>
<p>同样，在node2上ping node4，同时在node2上抓包，可以看到发给node4的request包和reply包：</p>
<p><img src="/images/oss/c6d1706b6f8162cfac528ddf5319c8e2.png" alt="image.png"></p>
<p>从request包可以看到src ip 是0.111， dest ip是 3.113，<strong>因为 node2 没有192.168.3.111这个ip</strong></p>
<p>非常关键的我们看到node4的回复包 src ip 不是3.113，而是0.113（根据node4的路由就应该是0.113）</p>
<p><img src="/images/oss/5c7172e2422579eb99c66e881d47bf99.png" alt="image.png"></p>
<p>这就是问题所在，从node4过来的ipip包src ip都是0.113，实际这里ipip能认识的只是3.113. </p>
<p>如果这个时候在3.113机器上把0.113网卡down掉，那么3.113上的：</p>
<p>10.122.124.128/26 via 192.168.0.111 dev tunl0 proto bird onlink 路由被自动删除，3.113将不再回复request。这是因为calico记录的node2的ip是192.168.0.111，所以会自动增加</p>
<p>解决办法，在node4上删除这条路由记录，也就是强制让回复包走3.113网卡，这样收发的ip就能对应上了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ip route del 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.113</div><div class="line">//同时将默认路由改到3.113</div><div class="line">ip route del default via 192.168.0.253 dev eth0; </div><div class="line">ip route add default via 192.168.3.253 dev eth1</div></pre></td></tr></table></figure>
<p>最终OK后，node4上的ip route是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[root@az3-k8s-14 ~]# ip route</div><div class="line">default via 192.168.3.253 dev eth1 </div><div class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink </div><div class="line">10.122.124.128/26 via 192.168.0.111 dev tunl0 proto bird onlink </div><div class="line">10.122.127.128/26 via 192.168.3.112 dev tunl0 proto bird onlink </div><div class="line">blackhole 10.122.157.128/26 proto bird </div><div class="line">10.122.157.129 dev cali19f6ea143e3 scope link </div><div class="line">10.122.157.130 dev cali09e016ead53 scope link </div><div class="line">10.122.157.131 dev cali0ad3225816d scope link </div><div class="line">10.122.157.132 dev cali55a5ff1a4aa scope link </div><div class="line">10.122.157.133 dev cali01cf8687c65 scope link </div><div class="line">10.122.157.134 dev cali65232d7ada6 scope link </div><div class="line">10.122.173.128/26 via 192.168.3.114 dev tunl0 proto bird onlink </div><div class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">192.168.3.0/24 dev eth1 proto kernel scope link src 192.168.3.113</div></pre></td></tr></table></figure>
<p>正常后的抓包, 注意这里drequest的est ip 和reply的 src ip终于一致了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">//request</div><div class="line">00:16:3e:02:06:1e &gt; ee:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 57971, offset 0, flags [DF], proto IPIP (4), length 104)</div><div class="line">    192.168.0.111 &gt; 192.168.3.110: (tos 0x0, ttl 64, id 18953, offset 0, flags [DF], proto ICMP (1), length 84)</div><div class="line">    10.122.124.128 &gt; 10.122.17.64: ICMP echo request, id 22001, seq 4, length 64</div><div class="line">    </div><div class="line">//reply    </div><div class="line">ee:ff:ff:ff:ff:ff &gt; 00:16:3e:02:06:1e, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 2565, offset 0, flags [none], proto IPIP (4), length 104)</div><div class="line">    192.168.3.110 &gt; 192.168.0.111: (tos 0x0, ttl 64, id 26374, offset 0, flags [none], proto ICMP (1), length 84)</div><div class="line">    10.122.17.64 &gt; 10.122.124.128: ICMP echo reply, id 22001, seq 4, length 64</div></pre></td></tr></table></figure>
<p>总结下来这两个案例都还是对路由不够了解，特别是案例2，因为有了多个网卡后导致路由更复杂。calico ipip的基本原理就是利用内核进行ipip封包，然后修改路由来保证网络的畅通。</p>
<h2 id="flannel网络不通"><a href="#flannel网络不通" class="headerlink" title="flannel网络不通"></a>flannel网络不通</h2><h3 id="firewalld"><a href="#firewalld" class="headerlink" title="firewalld"></a>firewalld</h3><p>在麒麟系统的物理机上通过kubeadm setup集群，发现有的环境flannel网络不通，在宿主机上ping 其它物理机flannel.0网卡的ip，通过在对端宿主机抓包发现icmp收到后被防火墙扔掉了，抓包中可以看到错误信息：icmp unreachable - admin prohibited</p>
<p>下图中正常的icmp是直接ping 物理机ip</p>
<p><img src="/images/951413iMgBlog/image-20211228203650921.png" alt="image-20211228203650921"></p>
<blockquote>
<p>The “admin prohibited filter” seen in the tcpdump output means there is a firewall blocking a connection. It does it by sending back an ICMP packet meaning precisely that: the admin of that firewall doesn’t want those packets to get through. It could be a firewall at the destination site. It could be a firewall in between. It could be iptables on the Linux system.</p>
</blockquote>
<p>发现有问题的环境中宿主机的防火墙设置报错了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">12月 28 23:35:08 hygon253 firewalld[10493]: WARNING: COMMAND_FAILED: &apos;/usr/sbin/iptables -w10 -t filter -X DOCKER-ISOLATION-STAGE-1&apos; failed: iptables: No chain/target/match by that name.</div><div class="line">12月 28 23:35:08 hygon253 firewalld[10493]: WARNING: COMMAND_FAILED: &apos;/usr/sbin/iptables -w10 -t filter -F DOCKER-ISOLATION-STAGE-2&apos; failed: iptables: No chain/target/match by that name.</div></pre></td></tr></table></figure>
<p>应该是因为启动docker的时候 firewalld 是运行着的</p>
<blockquote>
<p>Do you have firewalld enabled, and was it (re)started after docker was started? If so, then it’s likely that firewalld wiped docker’s IPTables rules. Restarting the docker daemon should re-create those rules.</p>
</blockquote>
<p>停掉 firewalld 服务可以解决这个问题</p>
<h3 id="掉电重启后flannel网络不通"><a href="#掉电重启后flannel网络不通" class="headerlink" title="掉电重启后flannel网络不通"></a><a href="https://github.com/flannel-io/flannel/issues/799" target="_blank" rel="external">掉电重启后flannel网络不通</a></h3><p>flannel能收到包，但是cni0收不到包，说明包进到了目标宿主机，但是从flannel解开udp转送到cni的时候出了问题，大概率是iptables 拦截了包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">It seems docker version &gt;=1.13 will add iptables rule like below,and it make this issue happen:</div><div class="line">iptables -P FORWARD DROP</div><div class="line"></div><div class="line">All you need to do is add a rule below:</div><div class="line">iptables -P FORWARD ACCEPT</div></pre></td></tr></table></figure>
<h2 id="清理"><a href="#清理" class="headerlink" title="清理"></a><a href="https://serverfault.com/questions/247767/cannot-delete-gre-tunnel" target="_blank" rel="external">清理</a></h2><p>cni信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/etc/cni/net.d/*</div><div class="line">/var/lib/cni/ 下存放有ip分配信息</div></pre></td></tr></table></figure>
<p>calico创建的tunl0网卡是个tunnel，可以通过 ip tunnel show来查看，清理不掉（重启可以清理掉tunl0）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ip link set dev tunl0 name tunl0_fallback</div><div class="line"></div><div class="line">或者</div><div class="line">/sbin/ip link set eth1 down</div><div class="line">/sbin/ip link set eth1 name eth123</div><div class="line">/sbin/ip link set eth123 up</div></pre></td></tr></table></figure>
<p>flannel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ip link delete flannel.1</div><div class="line">ip link delete cni0</div></pre></td></tr></table></figure>
<h2 id="netns"><a href="#netns" class="headerlink" title="netns"></a><a href="https://mp.weixin.qq.com/s/lscMpc5BWAEzjgYw6H0wBw" target="_blank" rel="external">netns</a></h2><p>以下case创建一个名为 ren 的netns，然后在里面增加一对虚拟网卡veth1 veth1_p,  veth1放置在ren里面，veth1_p 放在物理机上，给他们配置上ip并up就能通了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">1004  [2021-10-27 10:49:08] ip netns add ren</div><div class="line">1005  [2021-10-27 10:49:12] ip netns show</div><div class="line">1006  [2021-10-27 10:49:22] ip netns exec ren route   //为空</div><div class="line">1007  [2021-10-27 10:49:29] ip netns exec ren iptables -L</div><div class="line">1008  [2021-10-27 10:49:55] ip link add veth1 type veth peer name veth1_p //此时宿主机上能看到这两块网卡</div><div class="line">1009  [2021-10-27 10:50:07] ip link set veth1 netns ren //将veth1从宿主机默认网络空间挪到ren中，宿主机中看不到veth1了</div><div class="line">1010  [2021-10-27 10:50:18] ip netns exec ren route  </div><div class="line">1011  [2021-10-27 10:50:25] ip netns exec ren iptables -L</div><div class="line">1012  [2021-10-27 10:50:39] ifconfig</div><div class="line">1013  [2021-10-27 10:50:51] ip link list</div><div class="line">1014  [2021-10-27 10:51:29] ip netns exec ren ip link list</div><div class="line">1017  [2021-10-27 10:53:27] ip netns exec ren ip addr add 172.19.0.100/24 dev veth1 </div><div class="line">1018  [2021-10-27 10:53:31] ip netns exec ren ip link list</div><div class="line">1019  [2021-10-27 10:53:39] ip netns exec ren ifconfig</div><div class="line">1020  [2021-10-27 10:53:42] ip netns exec ren ifconfig -a</div><div class="line">1021  [2021-10-27 10:54:13] ip netns exec ren ip link set dev veth1 up</div><div class="line">1022  [2021-10-27 10:54:16] ip netns exec ren ifconfig</div><div class="line">1023  [2021-10-27 10:54:22] ping 172.19.0.100</div><div class="line">1024  [2021-10-27 10:54:35] ifconfig -a</div><div class="line">1025  [2021-10-27 10:55:03] ip netns exec ren ip addr add 172.19.0.101/24 dev veth1_p</div><div class="line">1026  [2021-10-27 10:55:10] ip addr add 172.19.0.101/24 dev veth1_p</div><div class="line">1027  [2021-10-27 10:55:16] ifconfig veth1_p</div><div class="line">1028  [2021-10-27 10:55:30] ip link set dev veth1_p up</div><div class="line">1029  [2021-10-27 10:55:32] ifconfig veth1_p</div><div class="line">1030  [2021-10-27 10:55:38] ping 172.19.0.101</div><div class="line">1031  [2021-10-27 10:55:43] ping 172.19.0.100</div><div class="line">1032  [2021-10-27 10:55:53] ip link set dev veth1_p down</div><div class="line">1033  [2021-10-27 10:55:54] ping 172.19.0.100</div><div class="line">1034  [2021-10-27 10:55:58] ping 172.19.0.101</div><div class="line">1035  [2021-10-27 10:56:08] ifconfig veth1_p</div><div class="line">1036  [2021-10-27 10:56:32] ping 172.19.0.101</div><div class="line">1037  [2021-10-27 10:57:04] ip netns exec ren route</div><div class="line">1038  [2021-10-27 10:57:52] ip netns exec ren ping 172.19.0.101</div><div class="line">1039  [2021-10-27 10:57:58] ip link set dev veth1_p up</div><div class="line">1040  [2021-10-27 10:57:59] ip netns exec ren ping 172.19.0.101</div><div class="line">1041  [2021-10-27 10:58:06] ip netns exec ren ping 172.19.0.100</div><div class="line">1042  [2021-10-27 10:58:14] ip netns exec ren ifconfig</div><div class="line">1043  [2021-10-27 10:58:19] ip netns exec ren route</div><div class="line">1044  [2021-10-27 10:58:26] ip netns exec ren ping 172.19.0.100 -I veth1</div><div class="line">1045  [2021-10-27 10:58:58] ifconfig veth1_p</div><div class="line">1046  [2021-10-27 10:59:10] ping 172.19.0.100</div><div class="line">1047  [2021-10-27 10:59:26] ip netns exec ren ping 172.19.0.101 -I veth1</div><div class="line"></div><div class="line">把网卡加入到docker0的bridge下</div><div class="line">1160  [2021-10-27 12:17:37] brctl show</div><div class="line">1161  [2021-10-27 12:18:05] ip link set dev veth3_p master docker0</div><div class="line">1162  [2021-10-27 12:18:09] ip link set dev veth1_p master docker0</div><div class="line">1163  [2021-10-27 12:18:13] ip link set dev veth2 master docker0</div><div class="line">1164  [2021-10-27 12:18:15] brctl show</div><div class="line"></div><div class="line">btctl showmacs br0</div></pre></td></tr></table></figure>
<p>Linux 上存在一个默认的网络命名空间，Linux 中的 1 号进程初始使用该默认空间。Linux 上其它所有进程都是由 1 号进程派生出来的，在派生 clone 的时候如果没有额外特别指定，所有的进程都将共享这个默认网络空间。</p>
<p>所有的网络设备刚创建出来都是在宿主机默认网络空间下的。可以通过 <code>ip link set 设备名 netns 网络空间名</code> 将设备移动到另外一个空间里去，socket也是归属在某一个网络命名空间下的，由创建socket进程所在的netns来决定socket所在的netns</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//file: net/socket.c</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">sock_create</span><span class="params">(<span class="keyword">int</span> family, <span class="keyword">int</span> type, <span class="keyword">int</span> protocol, struct socket **res)</span></span></div><div class="line">&#123;</div><div class="line"> <span class="keyword">return</span> __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, <span class="number">0</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//file: include/net/sock.h</span></div><div class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span></span></div><div class="line"><span class="keyword">void</span> <span class="title">sock_net_set</span><span class="params">(struct sock *sk, struct net *net)</span></div><div class="line">&#123;</div><div class="line"> write_pnet(&amp;sk-&gt;sk_net, net);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>内核提供了三种操作命名空间的方式，分别是 clone、setns 和 unshare。ip netns add 使用的是 unshare，原理和 clone 是类似的。</p>
<p><img src="/images/951413iMgBlog/640-5304524." alt="Image"></p>
<p>每个 net 下都包含了自己的路由表、iptable 以及内核参数配置等等</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://morven.life/notes/networking-3-ipip/" target="_blank" rel="external">https://morven.life/notes/networking-3-ipip/</a></p>
<p><a href="https://www.cnblogs.com/bakari/p/10564347.html" target="_blank" rel="external">https://www.cnblogs.com/bakari/p/10564347.html</a></p>
<p><a href="https://www.cnblogs.com/goldsunshine/p/10701242.html" target="_blank" rel="external">https://www.cnblogs.com/goldsunshine/p/10701242.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/07/03/MySQL JDBC StreamResult 和 net_write_timeout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/03/MySQL JDBC StreamResult 和 net_write_timeout/" itemprop="url">MySQL JDBC StreamResult 和 net_write_timeout</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-03T17:30:03+08:00">
                2020-07-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MySQL-JDBC-StreamResult-和-net-write-timeout"><a href="#MySQL-JDBC-StreamResult-和-net-write-timeout" class="headerlink" title="MySQL JDBC StreamResult 和 net_write_timeout"></a>MySQL JDBC StreamResult 和 net_write_timeout</h1><h2 id="MySQL-JDBC-拉取数据的三种方式"><a href="#MySQL-JDBC-拉取数据的三种方式" class="headerlink" title="MySQL JDBC 拉取数据的三种方式"></a>MySQL JDBC 拉取数据的三种方式</h2><p>MySQL JDBC 在从 MySQL 拉取数据的时候有<a href="https://segmentfault.com/a/1190000016724645" target="_blank" rel="external">三种方式</a>：</p>
<ol>
<li>简单模式，也就是默认模式，数据都先要从MySQL Server发到client的OS TCP buffer，然后JDBC把 OS buffer读取到JVM内存中，读取到JVM内存的过程中憋着不让client读取，全部读完再通知inputStream.read(). 数据大的话容易导致JVM OOM</li>
<li><strong>useCursorFetch=true</strong>，配合FetchSize，也就是MySQL Server把查到的数据先缓存到本地磁盘，然后按照FetchSize挨个发给client。这需要占用MySQL很高的IOPS（先写磁盘缓存），其次每次Fetch需要一个RTT，效率不高。</li>
<li>Stream读取，Stream读取是在执行SQL前设置FetchSize：statement.setFetchSize(Integer.MIN_VALUE)，同时确保游标是只读、向前滚动的（为游标的默认值），MySQL JDBC内置的操作方法是将Statement强制转换为：com.mysql.jdbc.StatementImpl，调用其方法：enableStreamingResults()，这2者达到的效果是一致的，都是启动Stream流方式读取数据。这个时候MySQL不停地发数据，inputStream.read()不停地读取。一般来说发数据更快些，很快client的OS TCP recv buffer就满了，这时MySQL停下来等buffer有空闲就继续发数据。等待过程中如果超过 net_write_timeout MySQL就会报错，中断这次查询。</li>
</ol>
<p>从这里的描述来看，数据小的时候第一种方式还能接受，但是数据大了容易OOM，方式三看起来不错，但是要特别注意 net_write_timeout。</p>
<p>1和3对MySQL Server来说处理上没有啥区别，也感知不到这两种方式的不同。只是对1来说从OS Buffer中的数据复制到JVM内存中速度快，JVM攒多了数据内存就容易爆掉；对3来说JDBC一条条将OS Buffer中的数据复制到JVM(内存复制速度快)同时返回给execute挨个处理（慢），一般来说挨个处理要慢一些，这就导致了从OS Buffer中复制数据较慢，容易导致 TCP Receive Buffer满了，那么MySQL Server感知到的就是TCP 传输窗口为0了，导致暂停传输数据。</p>
<p>在数据量很小的时候方式三没什么优势，因为总是多一次set net_write_tiemout，也就是多了一次RTT。</p>
<p><img src="/images/951413iMgBlog/70.png" alt="img"></p>
<h2 id="MySQL-timeout"><a href="#MySQL-timeout" class="headerlink" title="MySQL timeout"></a><a href="https://www.cubrid.org/blog/3826470" target="_blank" rel="external">MySQL timeout</a></h2><ol>
<li>Creates a statement by calling <code>Connection.createStatement()</code>.</li>
<li>Calls <code>Statement.executeQuery()</code>.</li>
<li>The statement transmits the Query to MySqlServer by using the internal connection.</li>
<li>The statement creates a new timeout-execution thread for timeout process.</li>
<li>For version 5.1.x, it changes to assign 1 thread for each connection.</li>
<li>Registers the timeout execution to the thread.</li>
<li>Timeout occurs.</li>
<li>The timeout-execution thread creates a connection that has the same configurations as the statement.</li>
<li>Transmits the cancel Query (KILL QUERY “connectionId“) by using the connection.</li>
</ol>
<p><img src="/images/951413iMgBlog/1f6df479e83fd2c14ecac4ee6be64a29.png" alt="Figure 6: QueryTimeout Execution Process for MySQL JDBC Statement (5.0.8)."></p>
<p>参考<a href="https://cloud.tencent.com/developer/article/1162225" target="_blank" rel="external">《揭秘JDBC超时机制》</a></p>
<h2 id="net-read-timeout"><a href="#net-read-timeout" class="headerlink" title="net_read_timeout"></a>net_read_timeout</h2><table>
<thead>
<tr>
<th style="text-align:left">Command-Line Format</th>
<th><code>--net-read-timeout=#</code></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">System Variable</td>
<td><code>net_read_timeout</code></td>
</tr>
<tr>
<td style="text-align:left">Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td style="text-align:left">Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td style="text-align:left">Type</td>
<td>Integer</td>
</tr>
<tr>
<td style="text-align:left">Default Value</td>
<td><code>30</code></td>
</tr>
<tr>
<td style="text-align:left">Minimum Value</td>
<td><code>1</code></td>
</tr>
<tr>
<td style="text-align:left">Maximum Value</td>
<td><code>31536000</code></td>
</tr>
<tr>
<td style="text-align:left">Unit</td>
<td>seconds</td>
</tr>
</tbody>
</table>
<p>The number of seconds to wait for more data from a connection before aborting the read. When the server is reading from the client, <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_net_read_timeout" target="_blank" rel="external"><code>net_read_timeout</code></a> is the timeout value controlling when to abort.</p>
<p>如下图，MySQL Server监听3017端口，195228号包 客户端发一个SQL 给 MySQL Server，但是似乎这个时候正好网络异常，30秒钟后（从 SQL 请求的前一个 ack 开始算，Server应该一直都没有收到），Server 端触发 net_read_timeout 超时异常（疑问：这里没有 net_read_timeout 描述的读取了一半的现象）</p>
<p><img src="/images/951413iMgBlog/image-20230209155545142.png" alt="image-20230209155545142"></p>
<p>解决方案：建议调大 net_read_timeout 以应对可能出现的网络丢包</p>
<h2 id="net-write-timeout"><a href="#net-write-timeout" class="headerlink" title="net_write_timeout"></a>net_write_timeout</h2><p>show processlist 看到的State的值一直处于<strong>“Sending to client”</strong>，说明SQL这个语句已经执行完毕，而此时由于请求的数据太多，MySQL不停写入net buffer，而net buffer又不停的将数据写入服务端的网络棧，服务器端的网络栈（socket send buffer）被写满了，又没有被客户端读取并消化，这时读数据的流程就被MySQL暂停了。直到客户端完全读取了服务端网络棧的数据，这个状态才会消失。</p>
<p>先看下 <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_net_write_timeout" target="_blank" rel="external"><code>net_write_timeout</code></a>的解释：The number of seconds to wait for a block to be written to a connection before aborting the write. 只针对执行查询中的等待超时，网络不好，tcp buffer满了（应用迟迟不读走数据）等容易导致mysql server端报net_write_timeout错误，指的是mysql server hang在那里长时间无法发送查询结果。</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command-Line Format</td>
<td><code>--net-write-timeout=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>net_write_timeout</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>60</code></td>
</tr>
<tr>
<td>Minimum Value</td>
<td><code>1</code></td>
</tr>
</tbody>
</table>
<p>报这个错就是RDS等了net_write_timeout这么久没写数据，可能是客户端卡死没有读走数据，也可能是从多个分片挨个拉取，还没开始拉第7片前面6片拉取耗时就超过了net_write_timeout。</p>
<blockquote>
<p><strong>案例</strong>：DRDS 到 MySQL 多个分片拉取数据生成了许多 cursor 并发执行,但拉数据的时候是串行拉取的,如果用户端拉取数据过慢会导致最后一个 cursor 执行完成之后要等待很久.会超过 MySQL 的 net_write_timeout 配置从而引发报错. 也就是最后一个cursor打开后一直没有去读取数据，直到MySQL  Server 触发 net_write_timeout 异常</p>
<p>首先可以尝试在 DRDS jdbcurl 配置 netTimeoutForStreamingResults 参数,设置为 0 可以使其一直等待,或设置一个合理的值(秒).</p>
</blockquote>
<p>从JDBC驱动中可以看到，当调用PreparedStatement的executeQuery() 方法的时候，如果我们是去获取流式resultset的话，就会默认执行SET net_write_timeout= ？ 这个命令去重新设置timeout时间。源代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">if (doStreaming &amp;&amp; this.connection.getNetTimeoutForStreamingResults() &gt; 0) &#123;  </div><div class="line">            java.sql.Statement stmt = null;  </div><div class="line">            try &#123;  </div><div class="line">                stmt = this.connection.createStatement();                    ((com.mysql.jdbc.StatementImpl)stmt).executeSimpleNonQuery(this.connection, &quot;SET net_write_timeout=&quot;   </div><div class="line">                        + this.connection.getNetTimeoutForStreamingResults());  </div><div class="line">            &#125; finally &#123;  </div><div class="line">                if (stmt != null) &#123;  </div><div class="line">                    stmt.close();  </div><div class="line">                &#125;  </div><div class="line">            &#125;  </div><div class="line">        &#125;  </div><div class="line">        </div><div class="line">//另外DRDS代码 AppLoader.java 中写死了net_write_timeout 8小时</div><div class="line">ds.putConnectionProperties(ConnectionProperties.NET_WRITE_TIMEOUT, 28800);</div></pre></td></tr></table></figure>
<p>而 this.connection.getNetTimeoutForStreamingResults() 默认是600秒，或者在JDBC连接串种通过属性 netTimeoutForStreamingResults 来指定。</p>
<p>netTimeoutForStreamingResults 默认值：</p>
<p>What value should the driver automatically set the server setting ‘net_write_timeout’ to when the streaming result sets feature is in use? Value has unit of seconds, the value “0” means the driver will not try and adjust this value.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Default Value</th>
<th>600</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Since Version</td>
<td>5.1.0</td>
</tr>
</tbody>
</table>
<p>一般在数据导出场景中容易出现 net_write_timeout 这个错误，比如这个错误堆栈：</p>
<p><img src="/images/oss/8fe715d3ebb6929afecd19aadbe53e5e.png" alt=""></p>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">ErrorMessage:</div><div class="line">Communications link failure</div><div class="line">The last packet successfully received from the server was 7 milliseconds ago.  The last packet sent successfully to the server was 709,806 milliseconds ago. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">The last packet successfully received from the server was 7 milliseconds ago.  The last packet sent successfully to the server was 709,806 milliseconds ago.</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1036)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3427)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3327)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3814)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:870)</div><div class="line">	at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1928)</div><div class="line">	at com.mysql.jdbc.RowDataDynamic.nextRecord(RowDataDynamic.java:378)</div><div class="line">	at com.mysql.jdbc.RowDataDynamic.next(RowDataDynamic.java:358)</div><div class="line">	at com.mysql.jdbc.ResultSetImpl.next(ResultSetImpl.java:6337)</div><div class="line">	at com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader$Task.startRead(CommonRdbmsReader.java:275)</div><div class="line">	at com.alibaba.datax.plugin.reader.drdsreader.DrdsReader$Task.startRead(DrdsReader.java:148)</div><div class="line">	at com.alibaba.datax.core.taskgroup.runner.ReaderRunner.run(ReaderRunner.java:62)</div><div class="line">	at java.lang.Thread.run(Thread.java:834)</div><div class="line">Caused by: java.io.EOFException: Can not read response from server. Expected to read 258 bytes, read 54 bytes before connection was unexpectedly lost.</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2914)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3387)</div><div class="line">	... 11 more</div></pre></td></tr></table></figure>
<h3 id="特别注意"><a href="#特别注意" class="headerlink" title="特别注意"></a>特别注意</h3><p>JDBC驱动报如下错误</p>
<blockquote>
<p>Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server.  </p>
</blockquote>
<p>不一定是 <code>net_write_timeout</code> 设置过小导致的，JDBC 在 streaming 流模式下只要连接异常就会报如上错误，比如：</p>
<ul>
<li>连接被 TCP reset</li>
<li>连接因为某种原因(比如 QueryTimeOut、比如用户监控kill 慢查询) 触发 kill Query导致连接中断</li>
</ul>
<p><a href="https://plantegg.github.io/2022/10/10/Linux%20BUG%E5%86%85%E6%A0%B8%E5%AF%BC%E8%87%B4%E7%9A%84%20TCP%E8%BF%9E%E6%8E%A5%E5%8D%A1%E6%AD%BB/">比如出现内核bug，内核卡死不发包的话，客户端同样报 net_write_timeout 错误</a></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_allowed_packet" target="_blank" rel="external"><code>max_allowed_packet</code></a>: 单个SQL或者单条记录的最大大小</p>
<h2 id="一些其他的-Timeout"><a href="#一些其他的-Timeout" class="headerlink" title="一些其他的 Timeout"></a>一些其他的 Timeout</h2><p>connectTimeout：表示等待和MySQL数据库建立socket链接的超时时间，默认值0，表示不设置超时，单位毫秒，建议30000。 JDBC驱动连接属性</p>
<p>queryTimeout：超时后jdbc驱动触发新建一个连接来发送一个 kill 给DB</p>
<p>socketTimeout：JDBC参数，表示客户端发送请求给MySQL数据库后block在read的等待数据的超时时间，linux系统默认的socketTimeout为30分钟，可以不设置。<strong>socketTimeout 超时不会触发发kill，只会断开连接</strong>。</p>
<p>要特别注意socketTimeout仅仅是指等待socket数据时间，如果在传输数据那么这个值就没有用了。<a href="https://docs.oracle.com/javase/7/docs/api/java/net/SocketOptions.html#SO_TIMEOUT" target="_blank" rel="external">socketTimeout通过mysql-connector中的NativeProtocol最终设置在socketOptions上</a></p>
<p><img src="/images/951413iMgBlog/image-20211024171459127.png" alt="image-20211024171459127"></p>
<blockquote>
<p>static final int SO_TIMEOUT。 <strong>Set a timeout on blocking Socket operations</strong>:</p>
<p> ServerSocket.accept();<br> SocketInputStream.read();<br> DatagramSocket.receive();</p>
<p>The option must be set prior to entering a blocking operation to take effect. If the timeout expires and the operation would continue to block, <strong>java.io.InterruptedIOException</strong> is raised. The Socket is not closed in this case.</p>
</blockquote>
<p>Statement Timeout：用来限制statement的执行时长，timeout的值通过调用JDBC的java.sql.Statement.setQueryTimeout(int timeout) API进行设置。不过现在开发者已经很少直接在代码中设置，而多是通过框架来进行设置。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time" target="_blank" rel="external"><code>max_execution_time</code></a>：The execution timeout for <a href="https://dev.mysql.com/doc/refman/5.7/en/select.html" target="_blank" rel="external"><code>SELECT</code></a> statements, in milliseconds. If the value is 0, timeouts are not enabled.  MySQL 属性，可以set修改，一般用来设置一个查询最长不超过多少秒，避免一个慢查询一直在跑，跟statement timeout对应。</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command-Line Format</td>
<td><code>--max-execution-time=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>max_execution_time</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>0</code></td>
</tr>
</tbody>
</table>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout" target="_blank" rel="external"><code>wait_timeout</code></a> The number of seconds the server waits for activity on a noninteractive connection before closing it. MySQL 属性，一般设置tcp keepalive后这个值基本不会超时（这句话存疑 202110）。</p>
<p>On thread startup, the session <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout" target="_blank" rel="external"><code>wait_timeout</code></a> value is initialized from the global <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout" target="_blank" rel="external"><code>wait_timeout</code></a> value or from the global <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_interactive_timeout" target="_blank" rel="external"><code>interactive_timeout</code></a> value, depending on the type of client (as defined by the <code>CLIENT_INTERACTIVE</code> connect option to <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-real-connect.html" target="_blank" rel="external"><code>mysql_real_connect()</code></a>). See also <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_interactive_timeout" target="_blank" rel="external"><code>interactive_timeout</code></a>.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Command-Line Format</td>
<td><code>--wait-timeout=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>wait_timeout</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>28800</code></td>
</tr>
<tr>
<td>Minimum Value</td>
<td><code>1</code></td>
</tr>
<tr>
<td>Maximum Value (Other)</td>
<td><code>31536000</code></td>
</tr>
<tr>
<td>Maximum Value (Windows)</td>
<td><code>2147483</code></td>
</tr>
</tbody>
</table>
<p>一般来说应该设置： max_execution_time/statement timeout &lt; Tranction Timeout &lt; socketTimeout</p>
<h3 id="SocketTimeout"><a href="#SocketTimeout" class="headerlink" title="SocketTimeout"></a>SocketTimeout</h3><p><a href="https://issues.apache.org/jira/browse/HTTPCLIENT-1478" target="_blank" rel="external">这个 httpclient 的bug</a> 就是在 TCP 连接握手成功(只受ConnectTimeout影响，SocketTimeout还不起作用)后，还需要进行 SSL的数据交换(HandShake)，但因为httpclient是在连接建立后(含 SSL HandShake)才设置的 SocketTimeout，导致在SSL HandShake的时候卡在了读数据，此时恰好还没设置SocketTimeout，导致连接永久卡死在SSL HandShake的读数据</p>
<p>所以代码的fix方案就是在建连接前就设置好 SocketTimeout。</p>
<h2 id="一次PreparedStatement-执行"><a href="#一次PreparedStatement-执行" class="headerlink" title="一次PreparedStatement 执行"></a>一次PreparedStatement 执行</h2><p>useServerPrepStmts=true&amp;cachePrepStmts=true</p>
<p>5.0.5版本后的驱动 useServerPrepStmts 默认值是false；另外跨Statement是没法重用PreparedStatement预编译的，还需要设置 <a href="https://stackoverflow.com/questions/32286518/whats-the-difference-between-cacheprepstmts-and-useserverprepstmts-in-mysql-jdb" target="_blank" rel="external">cachePrepStmts 才可以</a>。</p>
<p>对于打开预编译的URL（String url = “jdbc:<a href="mysql://localhost:3306/studb?useServerPrepStmts=true&amp;cachePrepStmts=true" target="_blank" rel="external">mysql://localhost:3306/studb?useServerPrepStmts=true&amp;cachePrepStmts=true</a>“）获取数据库连接之后，本质是获取预编译语句 <strong>pstmt = conn.prepareStatement(sql)</strong>时会向MySQL服务端发送一个RPC，发送一个预编译的SQL模板（驱动会拼接MySQL预编译语句prepare s1 from ‘select <em> from user where id = ?’），然会MySQL服务端会编译好收到的SQL模板，再会为此预编译模板语句分配一个 <strong>serverStatementId</strong>发送给JDBC驱动，这样以后PreparedStatement就会持有当前预编译语句的服务端的serverStatementId,并且会把此 PreparedStatement缓存在当前数据库连接中，以后对于相同SQL模板的操作 <strong>pstmt.executeUpdate()</strong>，都用相同的PreparedStatement，执行SQL时只需要发送 <strong>serverStatementId</strong> <em>*和参数</em></em>，节省一次SQL编译, 直接执行。并且对于每一个连接(驱动端及MySQL服务端)都有自己的prepare cache,具体的源码实现是在com.mysql.jdbc.ServerPreparedStatement中实现。</p>
<p>根据SQL模板和设置的参数，解析成一条完整的SQL语句，最后根据MySQL协议，序列化成字节流，RPC发送给MySQL服务端</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 解析封装需要发送的SQL语句,序列化成MySQL协议对应的字节流</span></div><div class="line">Buffer sendPacket = fillSendPacket();</div></pre></td></tr></table></figure>
<p>准备好需要发送的MySQL协议的字节流（sendPacket）后，就可以一路通过ConnectionImpl.execSQL –&gt; MysqlIO.sqlQueryDirect –&gt; MysqlIO.send – &gt; OutPutStram.write把字节流数据通过Socket发送给MySQL服务器，然后线程阻塞等待服务端返回结果数据，拿到数据后再根据MySQL协议反序列化成我们熟悉的ResultSet对象。</p>
<p><img src="/images/951413iMgBlog/image-20230802101859567.png" alt="image-20230802101859567"></p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>设置JDBC参数不合理（不设置的话默认值是：queryTimeout=10s，socketTimeout=10s），会导致在异常情况下，第二条get获得了第一条的结果，拿到了错误的数据，数据库则表现正常</p>
<p>socketTimeout触发后，连接抛CommunicationsException（严重异常，触发后连接应该断开）, 但JDBC会检查请求是否被cancle了，如果cancle就会抛出MySQLTimeoutException异常，这是一个普通异常，连接会被重新放回连接池重用（导致下一个获取这个连接的线程可能会得到前一个请求的response）。</p>
<p>queryTimeout（queryTimeoutKillsConnection=True–来强制关闭连接）会触发启动一个新的连接向server发送 kill id的命令，<strong>MySQL5.7增加了max_statement_time/max_execution_time来做到在server上直接检测到这种查询，然后结束掉</strong>。</p>
<h3 id="jdbc-和-RDS之间-socket-timeout"><a href="#jdbc-和-RDS之间-socket-timeout" class="headerlink" title="jdbc 和 RDS之间 socket_timeout"></a>jdbc 和 RDS之间 socket_timeout</h3><p>jdbc驱动设置socketTimeout=1459，如果是socketTimeout触发客户端断开后，server端的SQL会继续执行，如果是client被kill则server端的SQL会被终止</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"># java -cp /home/admin/drds-server/lib/*:. Test &quot;jdbc:mysql://172.16.40.215:3008/bank_000000?socketTimeout=1459&quot; &quot;user&quot; &quot;pass&quot; &quot;select sleep(2)&quot; &quot;1&quot;</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line"></div><div class="line">The last packet successfully received from the server was 1,461 milliseconds ago.  The last packet sent successfully to the server was 1,461 milliseconds ago.</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2811)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2806)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2764)</div><div class="line">	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1399)</div><div class="line">	at Test.main(Test.java:29)</div><div class="line">Caused by: java.net.SocketTimeoutException: Read timed out</div><div class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</div><div class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</div><div class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</div><div class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 8 more</div><div class="line">	</div><div class="line">	或者开协程后的错误堆栈</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line"></div><div class="line">The last packet successfully received from the server was 1,460 milliseconds ago.  The last packet sent successfully to the server was 1,459 milliseconds ago.</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2811)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2806)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2764)</div><div class="line">	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1399)</div><div class="line">	at Test.main(Test.java:29)</div><div class="line">Caused by: java.net.SocketTimeoutException: time out</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 8 more</div></pre></td></tr></table></figure>
<p>对应抓包，没有 kill动作</p>
<p><img src="/images/951413iMgBlog/image-20220601141709318.png" alt="image-20220601141709318" style="zoom:50%;"></p>
<h3 id="CN-和-DN-间socket-timeout案例"><a href="#CN-和-DN-间socket-timeout案例" class="headerlink" title="CN 和 DN 间socket_timeout案例"></a>CN 和 DN 间socket_timeout案例</h3><p>设置CN到DN的socket_timeout为2秒，然后执行一个sleep</p>
<p>CN上抓包分析(stream 5是客户端到CN、stream6是CN到DN）如下，首先CN会计时2秒钟后发送quit给DN，然后断开和DN的连接，并返回一个错误给client，client发送quit断开连接：</p>
<p><img src="/images/951413iMgBlog/image-20220601122556415.png" alt="image-20220601122556415" style="zoom:50%;"></p>
<p>CN完整报错堆栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div></pre></td><td class="code"><pre><div class="line">2022-06-01 12:10:00.178 [ServerExecutor-bucket-2-19-thread-181] ERROR com.alibaba.druid.pool.DruidPooledStatement - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank] CommunicationsException, druid version 1.1.24, jdbcUrl : jdbc:mysql://172.16.40.215:3008/bank_000000?maintainTimeStats=false&amp;rewriteBatchedStatements=false&amp;failOverReadOnly=false&amp;cacheResultSetMetadata=true&amp;allowMultiQueries=true&amp;clobberStreamingResults=true&amp;autoReconnect=false&amp;usePsMemOptimize=true&amp;useServerPrepStmts=true&amp;netTimeoutForStreamingResults=0&amp;useSSL=false&amp;metadataCacheSize=256&amp;readOnlyPropagatesToServer=false&amp;prepStmtCacheSqlLimit=4096&amp;connectTimeout=5000&amp;socketTimeout=9000000&amp;cachePrepStmts=true&amp;characterEncoding=utf8&amp;prepStmtCacheSize=256, testWhileIdle true, idle millis 11861, minIdle 5, poolingCount 4, timeBetweenEvictionRunsMillis 60000, lastValidIdleMillis 11861, driver com.mysql.jdbc.Driver, exceptionSorter com.alibaba.polardbx.common.jdbc.sorter.MySQLExceptionSorter</div><div class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] ERROR com.alibaba.druid.pool.DruidDataSource - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank] discard connection</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</div><div class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</div><div class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</div><div class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</div><div class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</div><div class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</div><div class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</div><div class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</div><div class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</div><div class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</div><div class="line">	at java.lang.Thread.run(Thread.java:874)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">Caused by: java.net.SocketTimeoutException: time out</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 53 common frames omitted</div><div class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] [1461cdf8b2809000]Execute ERROR on GROUP: BANK_000000_GROUP, ATOM: dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000, MERGE_UNION_SIZE:1, SQL: /*DRDS /10.101.32.6/1461cdf8b2809000/0// */SELECT SLEEP(?) AS `sleep(236)`, PARAM: [236], ERROR: Communications link failure, tddl version: 5.4.13-16522656</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</div><div class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</div><div class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</div><div class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</div><div class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</div><div class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</div><div class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</div><div class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</div><div class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</div><div class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</div><div class="line">	at java.lang.Thread.run(Thread.java:874)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">Caused by: java.net.SocketTimeoutException: time out</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 53 common frames omitted</div><div class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] Reset conn socketTimeout failed, lastSocketTimeout is 9000000, tddl version: 5.4.13-16522656</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.Util.getInstance(Util.java:408)</div><div class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:918)</div><div class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:897)</div><div class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:886)</div><div class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:860)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.throwConnectionClosedException(ConnectionImpl.java:1326)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.checkClosed(ConnectionImpl.java:1321)</div><div class="line">	at com.mysql.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:5888)</div><div class="line">	at com.alibaba.polardbx.atom.utils.NetworkUtils.setNetworkTimeout(NetworkUtils.java:18)</div><div class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectConnection.setNetworkTimeout(TGroupDirectConnection.java:433)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.resetPhyConnSocketTimeout(MyJdbcHandler.java:721)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1173)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</div><div class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</div><div class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</div><div class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</div><div class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</div><div class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</div><div class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</div><div class="line">	at java.lang.Thread.run(Thread.java:874)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.executor.ExecutorHelper - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] PhyQuery(node=&quot;BANK_000000_GROUP&quot;, sql=&quot;SELECT SLEEP(?) AS `sleep(236)`&quot;)</div><div class="line">, tddl version: 5.4.13-16522656</div><div class="line">2022-06-01 12:10:00.180 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] [ERROR-CODE: 3009][1461cdf8b2809000] SQL:  /*+TDDL:node(0)  and SOCKET_TIMEOUT=2000 */ select sleep(236), tddl version: 5.4.13-16522656</div><div class="line">com.alibaba.polardbx.common.exception.TddlRuntimeException: ERR-CODE: [TDDL-4614][ERR_EXECUTE_ON_MYSQL] Error occurs when execute on GROUP &apos;BANK_000000_GROUP&apos; ATOM &apos;dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000&apos;: Communications link failure</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.handleException(MyJdbcHandler.java:1935)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.generalHandlerException(MyJdbcHandler.java:1911)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1168)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</div><div class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</div><div class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</div><div class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</div><div class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</div><div class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</div><div class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</div><div class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</div><div class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</div><div class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</div><div class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</div><div class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</div><div class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</div><div class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</div><div class="line">	at java.lang.Thread.run(Thread.java:874)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</div><div class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</div><div class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</div><div class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</div><div class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</div><div class="line">	... 44 common frames omitted</div><div class="line">Caused by: java.net.SocketTimeoutException: time out</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 53 common frames omitted</div></pre></td></tr></table></figure>
<h3 id="应用和-DB-间丢包导致-keepalive-心跳失败"><a href="#应用和-DB-间丢包导致-keepalive-心跳失败" class="headerlink" title="应用和 DB 间丢包导致 keepalive 心跳失败"></a>应用和 DB 间丢包导致 keepalive 心跳失败</h3><p>应用使用了 Druid 连接池来维护到 DB 间的所有长连接</p>
<p>应用和 DB 间丢包导致 keepalive 心跳失败，进而 OS会断开这个连接</p>
<p><img src="/images/951413iMgBlog/image-20230322171621838.png" alt="image-20230322171621838"></p>
<p>一个连接归还给Druid连接池都要做清理动作，就是第一个红框的rollback/autocommit=1</p>
<p>归还后OS 层面会探活TCP 连接，DB(4381)多次后多次不响应后，OS 触发reset tcp断开连接，此时上次应用(比如Druid连接池、比如Tomcat)还不知道此连接在OS 层面已经断开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#sysctl -a |grep -i keepalive</div><div class="line">net.ipv4.tcp_keepalive_intvl = 3</div><div class="line">net.ipv4.tcp_keepalive_probes = 60</div><div class="line">net.ipv4.tcp_keepalive_time = 20</div></pre></td></tr></table></figure>
<p>继续过来一个新连接，业务取到这个连接执行查询就会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line"></div><div class="line">The last packet successfully received from the server was 162,776 milliseconds ago.  The last packet sent successfully to the server was 162,776 milliseconds ago.</div><div class="line"></div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line"></div><div class="line">The last packet successfully received from the server was 162,776 milliseconds ago.  The last packet sent successfully to the server was 162,776 milliseconds ago.</div></pre></td></tr></table></figure>
<p>这个错误就是因为OS层面连接断开了，并且断开了162秒(和截图时间戳能对应上)</p>
<p>对应的错误堆栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Caused by: java.net.SocketException: Connection timed out (Write failed)</div><div class="line">        at java.net.SocketOutputStream.socketWrite0(Native Method)</div><div class="line">        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)</div><div class="line">        at java.net.SocketOutputStream.write(SocketOutputStream.java:155)</div><div class="line">        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)</div><div class="line">        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)</div><div class="line">        at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3725)</div><div class="line">        ... 46 common frames omitted</div></pre></td></tr></table></figure>
<h3 id="kill-案例"><a href="#kill-案例" class="headerlink" title="kill 案例"></a>kill 案例</h3><h4 id="kill-mysql-client"><a href="#kill-mysql-client" class="headerlink" title="kill mysql client"></a>kill mysql client</h4><p>mysql client连cn执行一个很慢的SQL，然后kill掉mysql client</p>
<p>cn报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line">2022-06-01 11:45:59.063 [ServerExecutor-bucket-0-17-thread-158] ERROR com.alibaba.druid.pool.DruidPooledStatement - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank] CommunicationsException, druid version 1.1.24, jdbcUrl : jdbc:mysql://172.16.40.215:3008/bank_000000?maintainTimeStats=false&amp;rewriteBatchedStatements=false&amp;failOverReadOnly=false&amp;cacheResultSetMetadata=true&amp;allowMultiQueries=true&amp;clobberStreamingResults=true&amp;autoReconnect=false&amp;usePsMemOptimize=true&amp;useServerPrepStmts=true&amp;netTimeoutForStreamingResults=0&amp;useSSL=false&amp;metadataCacheSize=256&amp;readOnlyPropagatesToServer=false&amp;prepStmtCacheSqlLimit=4096&amp;connectTimeout=5000&amp;socketTimeout=9000000&amp;cachePrepStmts=true&amp;characterEncoding=utf8&amp;prepStmtCacheSize=256, testWhileIdle true, idle millis 72028, minIdle 5, poolingCount 4, timeBetweenEvictionRunsMillis 60000, lastValidIdleMillis 345734, driver com.mysql.jdbc.Driver, exceptionSorter com.alibaba.polardbx.common.jdbc.sorter.MySQLExceptionSorter</div><div class="line">2022-06-01 11:45:59.064 [ServerExecutor-bucket-0-17-thread-158] ERROR com.alibaba.druid.pool.DruidDataSource - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank] discard connection</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	…………</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">Caused by: java.net.SocketException: Socket is closed</div><div class="line">	at java.net.Socket.getSoTimeout(Socket.java:1291)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:249)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 53 common frames omitted</div><div class="line">2022-06-01 11:45:59.065 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] [1461c86bbe809001]Execute ERROR on GROUP: BANK_000000_GROUP, ATOM: dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000, MERGE_UNION_SIZE:1, SQL: /*DRDS /10.101.32.6/1461c86bbe809001/0// */SELECT SLEEP(?) AS `sleep(236)`, PARAM: [236], ERROR: Communications link failure, tddl version: 5.4.13-16522656</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">…………</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">Caused by: java.net.SocketException: Socket is closed</div><div class="line">	at java.net.Socket.getSoTimeout(Socket.java:1291)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:249)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 53 common frames omitted</div><div class="line">2022-06-01 11:45:59.065 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] Reset conn socketTimeout failed, lastSocketTimeout is 9000000, tddl version: 5.4.13-16522656</div><div class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">…………</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">2022-06-01 11:45:59.065 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.executor.ExecutorHelper - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] PhyQuery(node=&quot;BANK_000000_GROUP&quot;, sql=&quot;SELECT SLEEP(?) AS `sleep(236)`&quot;)</div><div class="line">, tddl version: 5.4.13-16522656</div><div class="line">2022-06-01 11:45:59.066 [ServerExecutor-bucket-0-17-thread-158] ERROR com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] Interrupted unexpectedly for 1461c86bbe809001, tddl version: 5.4.13-16522656</div><div class="line">java.lang.InterruptedException: null</div><div class="line">	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1310)</div><div class="line">	at com.alibaba.polardbx.common.utils.BooleanMutex$Sync.innerGet(BooleanMutex.java:136)</div><div class="line">	at com.alibaba.polardbx.common.utils.BooleanMutex.get(BooleanMutex.java:53)</div><div class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool.waitByTraceId(ServerThreadPool.java:445)</div><div class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1291)</div><div class="line">	……</div><div class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</div><div class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">2022-06-01 11:45:59.066 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] [ERROR-CODE: 3009][1461c86bbe809001] SQL:  /*+TDDL:node(0)  and SOCKET_TIMEOUT=40000 */ select sleep(236), tddl version: 5.4.13-16522656</div><div class="line">com.alibaba.polardbx.common.exception.TddlRuntimeException: ERR-CODE: [TDDL-4614][ERR_EXECUTE_ON_MYSQL] Error occurs when execute on GROUP &apos;BANK_000000_GROUP&apos; ATOM &apos;dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000&apos;: Communications link failure</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.handleException(MyJdbcHandler.java:1935)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.generalHandlerException(MyJdbcHandler.java:1911)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1168)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</div><div class="line">	…………</div><div class="line">		at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</div><div class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</div><div class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</div><div class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</div><div class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</div><div class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</div><div class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</div><div class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</div><div class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</div><div class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</div><div class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</div><div class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</div><div class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</div><div class="line">	... 44 common frames omitted</div><div class="line">Caused by: java.net.SocketException: Socket is closed</div><div class="line">	at java.net.Socket.getSoTimeout(Socket.java:1291)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:249)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</div><div class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</div><div class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</div><div class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</div><div class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</div><div class="line">	... 53 common frames omitted</div><div class="line">2022-06-01 11:45:59.071 [KillExecutor-15-thread-49] WARN  com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] Connection Killed, tddl version: 5.4.13-16522656</div></pre></td></tr></table></figure>
<p>mysqld报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2022-06-01T11:45:58.915371+08:00 8218735 [Note] Aborted connection 8218735 to db: &apos;bank_000000&apos; user: &apos;rds_polardb_x&apos; host: &apos;172.16.40.214&apos; (Got an error reading communication packets)</div></pre></td></tr></table></figure>
<p>172.16.40.214是客户端IP</p>
<p>抓包看到CN收到mysql client发过来的fin，CN回复fin断开连接</p>
<p>CN会给DN在新的连接上发Kill Query（stream 1596），同时会在原来的连接(stream 583)上发fin，然后原来的连接收到DN的response（被kill），然后CN发reset给DN</p>
<p><img src="/images/951413iMgBlog/image-20220601120626629.png" alt="image-20220601120626629" style="zoom:50%;"></p>
<p>下图是sleep 连接的收发包</p>
<p><img src="/images/951413iMgBlog/image-20220601120417026.png" alt="image-20220601120417026" style="zoom:50%;"></p>
<h4 id="Kill-jdbc-client"><a href="#Kill-jdbc-client" class="headerlink" title="Kill jdbc client"></a>Kill jdbc client</h4><p>Java jdbc client被kill后没有错误堆栈，kill后触发socket.close(对应client发送fin断开连接），kill后server端SQL也被立即中断</p>
<p>抓包：</p>
<p><img src="/images/951413iMgBlog/image-20220601143200253.png" alt="image-20220601143200253" style="zoom:50%;"></p>
<p>server端报错信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2022-06-01T14:33:52.204848+08:00 8288839 [Note] Aborted connection 8288839 to db: &apos;bank_000000&apos; user: &apos;user&apos; host: &apos;172.16.40.214&apos; (Got an error reading communication packets)</div></pre></td></tr></table></figure>
<h3 id="Statement-timeout"><a href="#Statement-timeout" class="headerlink" title="Statement timeout"></a>Statement timeout</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># java -cp /home/admin/drds-server/lib/*:. Test &quot;jdbc:mysql://172.16.40.215:3008/bank_000000?socketTimeout=5459&quot; &quot;user&quot; &quot;pass&quot; &quot;select sleep(180)&quot; &quot;1&quot; 3</div><div class="line">com.mysql.jdbc.exceptions.MySQLTimeoutException: Statement cancelled due to timeout or client request</div><div class="line">	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1419)</div><div class="line">	at Test.main(Test.java:31)</div></pre></td></tr></table></figure>
<p>statement会设置一个timer，到时间还没有返回结果就创建一个新连接发送kill query</p>
<p>server 端收到kill后终止SQL执行，抓包看到Server端主动提前返回了错误</p>
<p><img src="/images/951413iMgBlog/image-20220601152401387.png" alt="image-20220601152401387" style="zoom:50%;"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/xieyuooo/article/details/83109971" target="_blank" rel="external">MySQL JDBC StreamResult通信原理浅析</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/07/01/如何创建一个自己连自己的TCP连接/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/01/如何创建一个自己连自己的TCP连接/" itemprop="url">如何创建一个自己连自己的TCP连接</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-01T17:30:03+08:00">
                2020-07-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何创建一个自己连自己的TCP连接"><a href="#如何创建一个自己连自己的TCP连接" class="headerlink" title="如何创建一个自己连自己的TCP连接"></a>如何创建一个自己连自己的TCP连接</h1><blockquote>
<p>能不能建立一个tcp连接， src-ip:src-port 等于dest-ip:dest-port 呢？</p>
</blockquote>
<p>执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># nc 192.168.0.79 18082 -p 18082</div></pre></td></tr></table></figure>
<p>然后就能看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># netstat -ant |grep 18082</div><div class="line">tcp        0      0 192.168.0.79:18082      192.168.0.79:18082      ESTABLISHED</div></pre></td></tr></table></figure>
<p>比较神奇，这个连接的srcport等于destport，并且完全可以工作，也能收发数据。这有点颠覆大家的理解，端口能重复使用？</p>
<h2 id="port-range"><a href="#port-range" class="headerlink" title="port range"></a>port range</h2><p>我们都知道linux下本地端口范围由参数控制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># cat /proc/sys/net/ipv4/ip_local_port_range </div><div class="line">10000	65535</div></pre></td></tr></table></figure>
<p>所以也经常看到一个<strong>误解</strong>：一台机器上最多能创建65535个TCP连接</p>
<h2 id="到底一台机器上最多能创建多少个TCP连接"><a href="#到底一台机器上最多能创建多少个TCP连接" class="headerlink" title="到底一台机器上最多能创建多少个TCP连接"></a>到底一台机器上最多能创建多少个TCP连接</h2><p>在内存、文件句柄足够的话可以创建的连接是没有限制的，那么/proc/sys/net/ipv4/ip_local_port_range指定的端口范围到底是什么意思呢？</p>
<p>一个TCP连接只要保证四元组(src-ip src-port dest-ip dest-port)唯一就可以了，而不是要求src port唯一，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># netstat -ant |grep 18089</div><div class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:22         ESTABLISHED</div><div class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      ESTABLISHED</div><div class="line">tcp        0      0 192.168.0.79:18089      192.168.0.79:22         TIME_WAIT </div><div class="line">tcp        0      0 192.168.1.79:22         192.168.1.79:18089      ESTABLISHED</div><div class="line">tcp        0      0 192.168.1.79:18080      192.168.1.79:18089      ESTABLISHED</div></pre></td></tr></table></figure>
<p>从前三行可以清楚地看到18089被用了三次，第一第二行src-ip、dest-ip也是重复的，但是dest port不一样，第三行的src-port还是18089，但是src-ip变了。</p>
<p>所以一台机器能创建的TCP连接是没有限制的，而ip_local_port_range是指没有bind的时候OS随机分配端口的范围，但是分配到的端口要同时满足五元组唯一，这样 ip_local_port_range 限制的是连同一个目标（dest-ip和dest-port一样）的port的数量（请忽略本地多网卡的情况，因为dest-ip为以后route只会选用一个本地ip）。</p>
<p>但是如果程序调用的是bind函数(bind(ip,port=0))这个时候是让系统绑定到某个网卡和自动分配的端口，此时系统没有办法确定接下来这个socket是要去connect还是listen. 如果是listen的话，那么肯定是不能出现端口冲突的，如果是connect的话，只要满足4元组唯一即可。在这种情况下，系统只能尽可能满足更强的要求，就是先要求端口不能冲突，即使之后去connect的时候4元组是唯一的。</p>
<p>bind()的时候内核是还不知道四元组的，只知道src_ip、src_port，所以这个时候单网卡下src_port是没法重复的，但是connect()的时候已经知道了四元组的全部信息，所以只要保证四元组唯一就可以了，那么这里的src_port完全是可以重复使用的。</p>
<h2 id="自己连自己的连接"><a href="#自己连自己的连接" class="headerlink" title="自己连自己的连接"></a>自己连自己的连接</h2><p>我们来看自己连自己发生了什么</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># strace nc 192.168.0.79 18084 -p 18084</span></div><div class="line">execve(<span class="string">"/usr/bin/nc"</span>, [<span class="string">"nc"</span>, <span class="string">"192.168.0.79"</span>, <span class="string">"18084"</span>, <span class="string">"-p"</span>, <span class="string">"18084"</span>], [/* 31 vars */]) = 0</div><div class="line">brk(NULL)                               = 0x23d4000</div><div class="line">mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f213f394000</div><div class="line">access(<span class="string">"/etc/ld.so.preload"</span>, R_OK)      = -1 ENOENT (No such file or directory)</div><div class="line">open(<span class="string">"/etc/ld.so.cache"</span>, O_RDONLY|O_CLOEXEC) = 3</div><div class="line">fstat(3, &#123;st_mode=S_IFREG|0644, st_size=23295, ...&#125;) = 0</div><div class="line">mmap(NULL, 23295, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f213f38e000</div><div class="line">close(3)                                = 0</div><div class="line">open(<span class="string">"/lib64/libssl.so.10"</span>, O_RDONLY|O_CLOEXEC) = 3</div><div class="line">………………</div><div class="line">munmap(0x7f213f393000, 4096)            = 0</div><div class="line">open(<span class="string">"/usr/share/ncat/ca-bundle.crt"</span>, O_RDONLY) = -1 ENOENT (No such file or directory)</div><div class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</div><div class="line">fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)</div><div class="line">fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0</div><div class="line">setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</div><div class="line"><span class="built_in">bind</span>(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(<span class="string">"0.0.0.0"</span>)&#125;, 16) = 0</div><div class="line">//注意这里<span class="built_in">bind</span>后直接就是connect，没有listen</div><div class="line">connect(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(<span class="string">"192.168.0.79"</span>)&#125;, 16) = -1 EINPROGRESS (Operation now <span class="keyword">in</span> progress)</div><div class="line">select(4, [3], [3], [3], &#123;10, 0&#125;)       = 1 (out [3], left &#123;9, 999998&#125;)</div><div class="line">getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0</div><div class="line">select(4, [0 3], [], [], NULL</div></pre></td></tr></table></figure>
<p>抓包看看，正常三次握手，但是syn的seq和syn+ack的seq是一样的</p>
<p><img src="/images/oss/341f2891253baa4eebdaeaf34aa60c4b.png" alt="image.png"></p>
<p>这个连接算是常说的TCP simultaneous open，simultaneous open指的是两个不同port同时发syn建连接。而这里是先创建了一个socket，然后socket bind到18084端口上（作为local port，因为nc指定了local port），然后执行 connect, 连接到的目标也是192.168.0.79:18084，而这个目标正好是刚刚创建的socket，也就是自己连自己（连接双方总共只有一个socket）。因为一个socket充当了两个角色（client、server），握手的时候发syn，自己收到自己发的syn，就相当于两个角色simultaneous open了。</p>
<p>正常一个连接一定需要两个socket参与（这两个socket不一定要在两台机器上），而这个连接只用了一个socket就创建了，还能正常传输数据。但是仔细观察发数据的时候发放的seq增加（注意tcp_len 11那里的seq），收方的seq也增加了11，这是因为本来这就是用的同一个socket。正常两个socket通讯不是这样的。</p>
<p>那么这种情况为什么没有当做bug被处理呢？</p>
<h2 id="TCP-simultanous-open"><a href="#TCP-simultanous-open" class="headerlink" title="TCP simultanous open"></a>TCP simultanous open</h2><p>在tcp连接的定义中，通常都是一方先发起连接，假如两边同时发起连接，也就是两个socket同时给对方发 syn 呢？ 这在内核中是支持的，就叫同时打开（simultaneous open）。</p>
<p><img src="/images/oss/b9a0144a3835759c844f697bc45103fa.png" alt="image.png"></p>
<p>​                                                                       摘自《tcp/ip卷1》</p>
<p>可以清楚地看到这个连接建立用了四次握手，然后连接建立了，当然也有 simultanous close(3次挥手成功关闭连接)。如下内核代码 net/ipv4/tcp_input.c 的5924行中就说明了允许这种自己连自己的连接（当然也允许simultanous open). 也就是允许一个socket本来应该收到 syn+ack(发出syn后), 结果收到了syn的情况，而一个socket自己连自己又是这种情况的特例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">	static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,</div><div class="line">                     const struct tcphdr *th)</div><div class="line">	&#123;</div><div class="line">5916         /* PAWS check. */</div><div class="line">						 //PAWS机制全称Protect Againest Wrapped Sequence numbers，</div><div class="line">						 //目的是为了解决在高带宽下，TCP序号可能被重复使用而带来的问题。</div><div class="line">5917         if (tp-&gt;rx_opt.ts_recent_stamp &amp;&amp; tp-&gt;rx_opt.saw_tstamp &amp;&amp;</div><div class="line">5918             tcp_paws_reject(&amp;tp-&gt;rx_opt, 0))</div><div class="line">5919                 goto discard_and_undo;</div><div class="line">5920         //在socket发送syn后收到了一个syn(正常应该收到syn+ack),这里是允许的。</div><div class="line">5921         if (th-&gt;syn) &#123;</div><div class="line">5922                 /* We see SYN without ACK. It is attempt of</div><div class="line">5923                  * simultaneous connect with crossed SYNs.</div><div class="line">5924                  * Particularly, it can be connect to self.  //自己连自己</div><div class="line">5925                  */</div><div class="line">5926                 tcp_set_state(sk, TCP_SYN_RECV);</div><div class="line">5927 </div><div class="line">5928                 if (tp-&gt;rx_opt.saw_tstamp) &#123;</div><div class="line">5929                         tp-&gt;rx_opt.tstamp_ok = 1;</div><div class="line">5930                         tcp_store_ts_recent(tp);</div><div class="line">5931                         tp-&gt;tcp_header_len =</div><div class="line">5932                                 sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;</div><div class="line">5933                 &#125; else &#123;</div><div class="line">5934                         tp-&gt;tcp_header_len = sizeof(struct tcphdr);</div><div class="line">5935                 &#125;</div><div class="line">5936 </div><div class="line">5937                 tp-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;seq + 1;</div><div class="line">5938                 tp-&gt;copied_seq = tp-&gt;rcv_nxt;</div><div class="line">5939                 tp-&gt;rcv_wup = TCP_SKB_CB(skb)-&gt;seq + 1;</div><div class="line">5940 </div><div class="line">5941                 /* RFC1323: The window in SYN &amp; SYN/ACK segments is</div><div class="line">5942                  * never scaled.</div><div class="line">5943                  */</div></pre></td></tr></table></figure>
<p>也就是在发送syn进入SYN_SENT状态之后，收到对端发来的syn包后不会RST，而是处理流程如下，调用tcp_set_state(sk, TCP_SYN_RECV)进入SYN_RECV状态，以及调用tcp_send_synack(sk)向对端发送syn+ack。</p>
<h2 id="自己连自己的原理解释"><a href="#自己连自己的原理解释" class="headerlink" title="自己连自己的原理解释"></a>自己连自己的原理解释</h2><p>第一我们要理解Kernel是支持simultaneous open（同时打开）的，也就是说socket发走syn后，本来应该收到一个syn+ack的，但是实际收到了一个syn（没有ack），这是允许的。这叫TCP连接同时打开（同时给对方发syn），四次握手然后建立连接成功。</p>
<p>自己连自己又是simultaneous open的一个特例，特别在这个连接只有一个socket参与，发送、接收都是同一个socket，自然也会是发syn后收到了自己的syn（自己发给自己），然后依照simultaneous open连接也能创建成功。</p>
<p>这个bind到18084 local port的socket又要连接到 18084 port上，而这个18084 socket已经bind到了socket（也就是自己），就形成了两个socket 的simultaneous open一样，内核又允许这种simultaneous open，所以就形成了自己连自己，也就是一个socket在自己给自己收发数据，所以看到收方和发放的seq是一样的。</p>
<p>可以用python来重现这个连接连自己的过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">import socket</div><div class="line">import time</div><div class="line"></div><div class="line">connected=False</div><div class="line">while (not connected):</div><div class="line">        try:</div><div class="line">                sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)</div><div class="line">                sock.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)</div><div class="line">				sock.bind((&apos;&apos;, 18084))               //sock 先bind到18084</div><div class="line">                sock.connect((&apos;127.0.0.1&apos;,18084))    //然后同一个socket连自己</div><div class="line">                connected=True</div><div class="line">        except socket.error,(value,message):</div><div class="line">                print message</div><div class="line"></div><div class="line">        if not connected:</div><div class="line">                print &quot;reconnect&quot;</div><div class="line">               </div><div class="line">print &quot;tcp self connection occurs!&quot;</div><div class="line">print &quot;netstat -an|grep 18084&quot;</div><div class="line">time.sleep(1800)</div></pre></td></tr></table></figure>
<p>这里connect前如果没有bind那么系统就会从 local port range 分配一个可用port。</p>
<p>bind成功后会将ip+port放入hash表来判重，这就是我们常看到的 Bind to <em>*</em> failed (IOD #1): Address already in use 异常。所以一台机器上，如果有多个ip，是可以将同一个port bind多次的，但是bind的时候如果不指定ip，也就是bind(‘0’, port) 还是会冲突。</p>
<p>connect成功后会将四元组放入ehash来判定连接的重复性。如果connect四元组冲突了就会报如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># nc 192.168.0.82 8080 -p 29798 -s 192.168.0.79</div><div class="line">Ncat: Cannot assign requested address.</div></pre></td></tr></table></figure>
<h2 id="bind-和-connect、listen"><a href="#bind-和-connect、listen" class="headerlink" title="bind 和 connect、listen"></a>bind 和 connect、listen</h2><p>当对一个TCP socket调用connect函数时，如果这个socket没有bind指定的端口号，操作系统会为它选择一个当前未被使用的端口号，这个端口号被称为ephemeral port, 范围可以在/proc/sys/net/ipv4/ip_local_port_range里查看。假设30000这个端口被选为ephemeral port。</p>
<p>如果这个socket指定了local port那么socket创建后会执行bind将这个socket bind到这个port。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</div><div class="line">fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)</div><div class="line">fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0</div><div class="line">setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</div><div class="line">bind(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(&quot;0.0.0.0&quot;)&#125;, 16) = 0</div></pre></td></tr></table></figure>
<p><img src="/images/oss/5373ecfe0d4496d106c64d3f370c893c.png" alt="image.png"></p>
<h3 id="listen"><a href="#listen" class="headerlink" title="listen"></a>listen</h3><p><img src="/images/oss/4d188cab03e919f055bb9dbe3da0188c.png" alt="image-20200702131215819"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://segmentfault.com/a/1190000002396411" target="_blank" rel="external">https://segmentfault.com/a/1190000002396411</a></p>
<p><a href="https://blog.csdn.net/a364572/article/details/40628171" target="_blank" rel="external">linux中TCP的socket、bind、listen、connect和accept的实现</a></p>
<p><a href="https://ops.tips/blog/how-linux-tcp-introspection/" target="_blank" rel="external">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
<p><a href="https://idea.popcount.org/2014-04-03-bind-before-connect/" target="_blank" rel="external">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/05/24/程序员如何学习和构建网络知识体系/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/05/24/程序员如何学习和构建网络知识体系/" itemprop="url">程序员如何学习和构建网络知识体系</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-05-24T17:30:03+08:00">
                2020-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="程序员如何学习和构建网络知识体系"><a href="#程序员如何学习和构建网络知识体系" class="headerlink" title="程序员如何学习和构建网络知识体系"></a>程序员如何学习和构建网络知识体系</h1><p>大家学习网络知识的过程中经常发现当时看懂了，很快又忘了，最典型的比如TCP三次握手、为什么要握手，大家基本都看过，但是种感觉还差那么一点点。都要看是因为面试官总要问，所以不能不知道啊。</p>
<p>我们来看一个典型的面试问题：</p>
<blockquote>
<p>问：为什么TCP是可靠的？<br>答：因为TCP有连接（或者回答因为TCP有握手）</p>
<p>追问：为什么有连接就可靠了？（面试的人估计心里在骂，你这不是傻逼么，有连接就可靠啊）</p>
<p>追问：这个TCP连接的本质是什么？网络上给你保留了一个带宽所以能可靠？<br>答：……懵了（或者因为TCP有ack，所以可靠）</p>
<p>追问：握手的本质是什么？为什么握手就可靠了<br>答：因为握手需要ack<br>追问：那这个ack也只是保证握手可靠，握手是怎么保证后面可靠的？握手本质做了什么事情？</p>
<p>追问：有了ack可靠后还会带来什么问题（比如发一个包ack一下，肯定是可行的，但是效率不行，面试官想知道的是这里TCP怎么传输的，从而引出各个buffer、拥塞窗口的概念）</p>
</blockquote>
<p>基本上我发现99%的程序员会回答TCP相对UDP是可靠的，70%以上的程序员会告诉你可靠是因为有ack（其他的会告诉你可靠是因为握手或者有连接），再追问下次就开始王顾左右而言他、胡言乱语。</p>
<p>我的理解：</p>
<blockquote>
<p>物理上没有一个连接的东西在这里，udp也类似会占用端口、ip，但是大家都没说过udp的连接。而本质上我们说tcp的握手是指tcp是协商和维护一些状态信息的，这个状态信息就包含seq、ack、窗口/buffer，tcp握手就是协商出来这些初始值。这些状态才是我们平时所说的tcp连接的本质。</p>
</blockquote>
<p>这说明大部分程序员对问题的本质的理解上出了问题，或者教科书描述的过于教条不够接地气所以看完书本质没get到。</p>
<p>想想 <code>费曼学习方法</code> 中对<strong>事物本质</strong>的理解的重要性。</p>
<h2 id="重点掌握如下两篇文章"><a href="#重点掌握如下两篇文章" class="headerlink" title="## 重点掌握如下两篇文章"></a>## 重点掌握如下两篇文章</h2><p><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">一个网络包是如何到达目的地的 – </a>  这篇可以帮你掌握网络如何运转，在本机上从端口、ip、mac地址如何一层层封上去，链路上每一个点拆开mac看看，拆看ip看看，然后替换mac地址继续扔到链路的下一跳，这样一跳跳到达目的。</p>
<p><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/">对BDP、Buffer、各种窗口、rt的理解和运用 </a> 这一篇可以让你入门TCP</p>
<p>以上两篇都是站在程序员的角度来剖析关于网络我们应该掌握哪些，也许第一篇有点像网工要掌握的，实际我不这么认为，目前很流行的微服务化、云原生对网络的要求更高了，大多时候需要程序员去掌握这些，也就是在网络包从你的网卡离开你才有资格呼叫网工，否则成本很高！</p>
<p>我本周还碰到了网络不通的问题</p>
<blockquote>
<p>我的测试机器不能连外网(公司安全策略)</p>
<p>走流程申请开通，开通后会在测试机器安装客户端以及安全配置文件</p>
<p>但仍然不通，客户端自检都能通</p>
<p>我的排查就是第一篇文章：ping 公网ip；ip route get 公网-ip；ping 网关；</p>
<p>很快就发现是路由的问题，公网ip正好命中了docker 容器添加的某个路由，以及默认路由缺失</p>
<p>如果我自己不会那就开工单、描述问题、call各种人、申请权限……</p>
</blockquote>
<p>我碰到的程序员一看到网络连接异常就吓尿了，不关我的事，网络不通，但是在call人前你至少可以做：</p>
<ol>
<li>ping ip 通不通(也有个别禁掉了icmp)</li>
<li>telnet ip port通不通</li>
<li>网络包发出去没有(抓包)</li>
<li>是不是都不通还是只有你的机器不通</li>
</ol>
<h2 id="来看一个案例"><a href="#来看一个案例" class="headerlink" title="来看一个案例"></a>来看一个案例</h2><p>我第一次看<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="external">RFC1180</a>的时候是震惊的，觉得讲述的太好了，2000字就把一本教科书的知识阐述的无比清晰、透彻。但是实际上我发现很快就忘了，而且大部分程序员基本都是这样</p>
<blockquote>
<p>RFC1180写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于95%的程序员没有什么用，当时看的时候很爽、也觉得自己理解了、学会了，实际上看完几周后就忘得差不多了。问题出在这种RFC偏理论多一点看起来完全没有体感无法感同身受，所以即使似乎当时看懂了，但是忘得也快，需要一篇结合实践的文章来帮助理解</p>
</blockquote>
<p>在这个问题上，让我深刻地理解到：</p>
<blockquote>
<p>一流的人看RFC就够了，差一些的人看《TCP/IP卷1》，再差些的人要看一个个案例带出来的具体知识的书籍了，比如<a href="https://book.douban.com/subject/26268767/" target="_blank" rel="external">《wireshark抓包艺术》</a>，人和人的学习能力有差别必须要承认。</p>
</blockquote>
<p>也就是我们要认识到每个个人的<a href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA" target="_blank" rel="external">学习能力的差异</a>，我超级认同这篇文章中的一个评论</p>
<blockquote>
<p>看完深有感触，尤其是后面的知识效率和工程效率型的区别。以前总是很中二的觉得自己看一遍就理解记住了，结果一次次失败又怀疑自己的智商是不是有问题，其实就是把自己当作知识效率型来用了。一个不太恰当的形容就是，有颗公主心却没公主命！</p>
</blockquote>
<p>嗯，大部分时候我们都觉得自己看一遍就理解了记住了能实用解决问题了，实际上了是马上忘了，停下来想想自己是不是这样的？在网络的相关知识上大部分看RFC、TCP卷1等东西是很难实际理解的，还是要靠实践来建立对知识的具体的理解，而网络相关的东西基本离大家有点远（大家不回去读tcp、ip源码，纯粹是靠对书本的理解），所以很难建立具体的概念，所以这里有个必杀技就是学会抓包和用wireshark看包，同时针对实际碰到的文题来抓包、看包分析。</p>
<p>比如这篇《<a href="https://mp.weixin.qq.com/s/x-ScSwEm3uQ2SFv-nAzNaA" target="_blank" rel="external">从计算机知识到落地能力，你欠缺了什么？</a>》就对上述问题最好的阐述，程序员最常碰到的网络问题就是网络为啥不通？</p>
<p>这是最好建立对网络知识具体理解和实践的机会，你把《<a href="https://mp.weixin.qq.com/s/x-ScSwEm3uQ2SFv-nAzNaA" target="_blank" rel="external">从计算机知识到落地能力，你欠缺了什么？</a>》实践完再去看<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="external">RFC1180</a> 就明白了。通过案例把RFC1180抽象的描述给它具体化、场景化了，理解起来就很轻松不容易忘记了。</p>
<blockquote>
<p>经验一: 通过具体的东西(案例、抓包)来建立对网络基础的理解</p>
</blockquote>
<p><img src="/images/951413iMgBlog/image-20220221151815993.png" alt="image-20220221151815993"></p>
<h2 id="不要追求知识的广度"><a href="#不要追求知识的广度" class="headerlink" title="不要追求知识的广度"></a>不要追求知识的广度</h2><p>学习网络知识过程中，不建议每个知识点都去看，因为很快会忘记，我的方法是只看经常碰到的问题点，碰到一个点把他学透理解明白。</p>
<p>比如我曾经碰到过 <a href="/2019/01/09/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82ping--nslookup-OK-but-ping-fail/">nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的知识体系中扎根下来</a> , 这个问题Google上很多人在搜索，说明很普遍，但是没找到有资料能把这个问题说清楚，所以借着这个机会就把 Linux下的 NSS（name service switch）的原理搞懂了。要不然碰到问题老司机告诉你改下 /etc/hosts 或者  /etc/nsswitch 或者 /etc/resolv.conf 之类的问题就能解决，但是你一直不知道这三个文件怎么起作用的，也就是你碰到过这种问题也解决过但是下次碰到类似的问题你不一定能解决。</p>
<p>当然对我来说为了解决这个问题最后写了4篇跟域名解析相关的文章，从windows到linux，涉及到vpn、glibc、docker等各种场景，我把他叫做场景驱动。后来换来工作环境从windows换到mac后又补了一篇mac下的路由、dns文章。</p>
<p>关于<a href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA" target="_blank" rel="external">场景驱动学习的方法可以看这篇总结</a></p>
<h2 id="TCP是最复杂的，要从实用出发"><a href="#TCP是最复杂的，要从实用出发" class="headerlink" title="TCP是最复杂的，要从实用出发"></a>TCP是最复杂的，要从实用出发</h2><p>比如拥塞算法基本大家不会用到，了解下就行，你想想你有碰到过因为拥塞算法导致的问题吗？极少是吧。还有拥塞窗口、慢启动，这个实际中碰到的概率不高，面试要问你基本上是属于炫技类型。</p>
<p>实际碰到更多的是传输效率（<a href="https://mp.weixin.qq.com/s/fKWJrDNSAZjLsyobolIQKw" target="_blank" rel="external">对BDP、Buffer、各种窗口、rt的理解和运用</a>），还有为什么连不通、<a href="https://mp.weixin.qq.com/s/yH3PzGEFopbpA-jw4MythQ" target="_blank" rel="external">连接建立不起来</a>、为什么收到包不回复、为什么要reset、为什么丢包了之类的问题。</p>
<p>关于为什么连不通，我碰到了<a href="/2019/05/16/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%B8%8D%E9%80%9A%E6%98%AF%E4%B8%AA%E5%A4%A7%E9%97%AE%E9%A2%98--%E5%8D%8A%E5%A4%9C%E9%B8%A1%E5%8F%AB/">这个问题</a>，随后在这个问题的基础上进行了总结，得到客户端建立连接的时候抛异常，可能的原因（握手失败，建不上连接）：</p>
<ul>
<li>网络不通，诊断：ping ip</li>
<li>端口不通,  诊断：telnet ip port</li>
<li>rp_filter 命中(rp_filter=1, 多网卡环境）， 诊断:  netstat -s | grep -i filter </li>
<li>防火墙、命中iptables 被扔掉了，可以试试22端口起sshd 能否正常访问，能的话说明是端口被干了</li>
<li>snat/dnat的时候宿主机port冲突，内核会扔掉 syn包。诊断: sudo conntrack -S | grep  insert_failed //有不为0的</li>
<li>Firewalld 或者 iptables</li>
<li>全连接队列满的情况，诊断： netstat -s | egrep “listen|LISTEN” </li>
<li>syn flood攻击, 诊断：同上</li>
<li>若远端服务器的内核参数 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 的值都为 1，则远端服务器会检查每一个报文中的时间戳（Timestamp），若 Timestamp 不是递增的关系，不会响应这个报文。配置 NAT 后，远端服务器看到来自不同的客户端的源 IP 相同，但 NAT 前每一台客户端的时间可能会有偏差，报文中的 Timestamp 就不是递增的情况。nat后的连接，开启timestamp。因为快速回收time_wait的需要，会校验时间该ip上次tcp通讯的timestamp大于本次tcp(nat后的不同机器经过nat后ip一样，保证不了timestamp递增），诊断：是否有nat和是否开启了timestamps</li>
<li>NAT 哈希表满导致 ECS 实例丢包 nf_conntrack full， 诊断: dmesg |grep conntrack</li>
</ul>
<p><a href="https://perthcharles.github.io/2015/08/27/timestamp-intro/" target="_blank" rel="external">Linux 系统有PAWS机制</a>，全称Protect Againest Wrapped Sequence numbers，目的是为了解决在高带宽下，TCP序号可能被重复使用而带来的问题。<a href="https://perthcharles.github.io/2015/08/27/timestamp-NAT/" target="_blank" rel="external">PAWS会检查syn网络包的timestamps</a> 来判断这个syn包的发送时间是否早于上一个syn包，如果早就扔掉。原本 PAWS 是每个连接的维度，但同时开启tcp_timestamp和tcp_tw_recycle之后，PAWS就变成per host粒度了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">timestamp为TCP/IP协议栈提供了两个功能：  </div><div class="line">    a. 更加准确的RTT测量数据，尤其是有丢包时  -- RTTM  </div><div class="line">    b. 保证了在极端情况下，TCP的可靠性        -- PAWS</div></pre></td></tr></table></figure>
<p>tcp connect 的流程是这样的：</p>
<p>1、tcp发出SYN建链报文后，报文到ip层需要进行路由查询</p>
<p>2、路由查询完成后，报文到arp层查询下一跳mac地址</p>
<p>3、如果本地没有对应网关的arp缓存，就需要缓存住这个报文，发起arp请求</p>
<p>4、arp层收到arp回应报文之后，从缓存中取出SYN报文，完成mac头填写并发送给驱动。</p>
<p>问题在于，arp层报文缓存队列长度默认为3。如果你运气不好，刚好赶上缓存已满，这个报文就会被丢弃。</p>
<p>TCP层发现SYN报文发出去3s（1s+2s）还没有回应，就会重发一个SYN。这就是为什么少数连接会3s后才能建链。</p>
<p>幸运的是，arp层缓存队列长度是可配置的，用 sysctl -a | grep unres_qlen 就能看到，默认值为3。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>一定要会用tcpdump和wireshark（纯工具，没有任何门槛，用不好只有一个原因: 懒）</li>
<li>多实践（因为网络知识离我们有点远、有点抽象）,用好各种工具，工具能帮我们看到、摸到</li>
<li>不要追求知识面的广度，深抠几个具体的知识点然后让这些点建立体系</li>
<li>不要为那些基本用不到的偏门知识花太多精力，天天用的都学不过来对吧。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/04/07/就是要你懂TCP--半连接队列和全连接队列--阿里技术公众号版本/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/07/就是要你懂TCP--半连接队列和全连接队列--阿里技术公众号版本/" itemprop="url">就是要你懂TCP--半连接队列和全连接队列</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-07T17:30:03+08:00">
                2020-04-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="关于TCP-半连接队列和全连接队列"><a href="#关于TCP-半连接队列和全连接队列" class="headerlink" title="关于TCP 半连接队列和全连接队列"></a>关于TCP 半连接队列和全连接队列</h1><blockquote>
<p>最近碰到一个client端连接服务器总是抛异常的问题，然后定位分析并查阅各种资料文章，对TCP连接队列有个深入的理解</p>
<p>查资料过程中发现没有文章把这两个队列以及怎么观察他们的指标说清楚，希望通过这篇文章能把他们说清楚</p>
</blockquote>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><pre><code>场景：JAVA的client和server，使用socket通信。server使用NIO。

1.间歇性的出现client向server建立连接三次握手已经完成，但server的selector没有响应到这连接。
2.出问题的时间点，会同时有很多连接出现这个问题。
3.selector没有销毁重建，一直用的都是一个。
4.程序刚启动的时候必会出现一些，之后会间歇性出现。
</code></pre><h3 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h3><h4 id="正常TCP建连接三次握手过程："><a href="#正常TCP建连接三次握手过程：" class="headerlink" title="正常TCP建连接三次握手过程："></a>正常TCP建连接三次握手过程：</h4><p><img src="/images/oss/159a331ff8cdd4b8994dfe6a209d035f.png" alt="image.png"></p>
<ul>
<li>第一步：client 发送 syn 到server 发起握手；</li>
<li>第二步：server 收到 syn后回复syn+ack给client；</li>
<li>第三步：client 收到syn+ack后，回复server一个ack表示收到了server的syn+ack（此时client的56911端口的连接已经是established）</li>
</ul>
<p>从问题的描述来看，有点像TCP建连接的时候全连接队列（accept队列，后面具体讲）满了，尤其是症状2、4. 为了证明是这个原因，马上通过 netstat -s | egrep “listen” 去看队列的溢出统计数据：</p>
<pre><code>667399 times the listen queue of a socket overflowed
</code></pre><p>反复看了几次之后发现这个overflowed 一直在增加，那么可以明确的是server上全连接队列一定溢出了</p>
<p>接着查看溢出后，OS怎么处理：</p>
<pre><code># cat /proc/sys/net/ipv4/tcp_abort_on_overflow
0
</code></pre><p><strong>tcp_abort_on_overflow 为0表示如果三次握手第三步的时候全连接队列满了那么server扔掉client 发过来的ack（在server端认为连接还没建立起来）</strong></p>
<p>为了证明客户端应用代码的异常跟全连接队列满有关系，我先把tcp_abort_on_overflow修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）。</p>
<p>接着测试，这时在客户端异常中可以看到很多connection reset by peer的错误，<strong>到此证明客户端错误是这个原因导致的（逻辑严谨、快速证明问题的关键点所在）</strong>。</p>
<p>于是开发同学翻看java 源代码发现socket 默认的backlog（这个值控制全连接队列的大小，后面再详述）是50，于是改大重新跑，经过12个小时以上的压测，这个错误一次都没出现了，同时观察到 overflowed 也不再增加了。</p>
<p>到此问题解决，<strong>简单来说TCP三次握手后有个accept队列，进到这个队列才能从Listen变成accept，默认backlog 值是50，很容易就满了</strong>。满了之后握手第三步的时候server就忽略了client发过来的ack包（隔一段时间server重发握手第二步的syn+ack包给client），如果这个连接一直排不上队就异常了。</p>
<blockquote>
<p>但是不能只是满足问题的解决，而是要去复盘解决过程，中间涉及到了哪些知识点是我所缺失或者理解不到位的；这个问题除了上面的异常信息表现出来之外，还有没有更明确地指征来查看和确认这个问题。</p>
</blockquote>
<h3 id="深入理解TCP握手过程中建连接的流程和队列"><a href="#深入理解TCP握手过程中建连接的流程和队列" class="headerlink" title="深入理解TCP握手过程中建连接的流程和队列"></a>深入理解TCP握手过程中建连接的流程和队列</h3><p><img src="/images/oss/2703fc07dfc4dd5b6e1bb4c2ce620e59.png" alt="image.png"><br>（图片来源：<a href="http://www.cnxct.com/something-about-phpfpm-s-backlog/）" target="_blank" rel="external">http://www.cnxct.com/something-about-phpfpm-s-backlog/）</a></p>
<p>如上图所示，这里有两个队列：syns queue(半连接队列）；accept queue（全连接队列）</p>
<p>三次握手中，在第一步server收到client的syn后，把这个连接信息放到半连接队列中，同时回复syn+ack给client（第二步）；</p>
<pre><code>题外话，比如syn floods 攻击就是针对半连接队列的，攻击方不停地建连接，但是建连接的时候只做第一步，第二步中攻击方收到server的syn+ack后故意扔掉什么也不做，导致server上这个队列满其它正常请求无法进来
</code></pre><p>第三步的时候server收到client的ack，如果这时全连接队列没满，那么从半连接队列拿出这个连接的信息放入到全连接队列中，否则按tcp_abort_on_overflow指示的执行。</p>
<p>这时如果全连接队列满了并且tcp_abort_on_overflow是0的话，server过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短，client就很容易异常了。</p>
<p>在我们的os中retry 第二步的默认次数是2（centos默认是5次）：</p>
<pre><code>net.ipv4.tcp_synack_retries = 2
</code></pre><h3 id="如果TCP连接队列溢出，有哪些指标可以看呢？"><a href="#如果TCP连接队列溢出，有哪些指标可以看呢？" class="headerlink" title="如果TCP连接队列溢出，有哪些指标可以看呢？"></a>如果TCP连接队列溢出，有哪些指标可以看呢？</h3><p>上述解决过程有点绕，听起来蒙逼，那么下次再出现类似问题有什么更快更明确的手段来确认这个问题呢？</p>
<p>（<em>通过具体的、感性的东西来强化我们对知识点的理解和吸收</em>）</p>
<h4 id="netstat-s"><a href="#netstat-s" class="headerlink" title="netstat -s"></a>netstat -s</h4><pre><code>[root@server ~]#  netstat -s | egrep &quot;listen|LISTEN&quot; 
667399 times the listen queue of a socket overflowed
667399 SYNs to LISTEN sockets ignored
</code></pre><p>比如上面看到的 667399 times ，表示全连接队列溢出的次数，隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了。</p>
<h4 id="ss-命令"><a href="#ss-命令" class="headerlink" title="ss 命令"></a>ss 命令</h4><pre><code>[root@server ~]# ss -lnt
Recv-Q Send-Q Local Address:Port  Peer Address:Port 
0        50               *:3306             *:* 
</code></pre><p><strong>上面看到的第二列Send-Q 值是50，表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少</strong></p>
<p><strong>全连接队列的大小取决于：min(backlog, somaxconn) . backlog是在socket创建的时候传入的，somaxconn是一个os级别的系统参数</strong></p>
<p>这个时候可以跟我们的代码建立联系了，比如Java创建ServerSocket的时候会让你传入backlog的值：</p>
<pre><code>ServerSocket()
    Creates an unbound server socket.
ServerSocket(int port)
    Creates a server socket, bound to the specified port.
ServerSocket(int port, int backlog)
    Creates a server socket and binds it to the specified local port number, with the specified backlog.
ServerSocket(int port, int backlog, InetAddress bindAddr)
    Create a server with the specified port, listen backlog, and local IP address to bind to.
</code></pre><p>（来自JDK帮助文档：<a href="https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html）" target="_blank" rel="external">https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html）</a></p>
<p><strong>半连接队列的大小取决于：max(64,  /proc/sys/net/ipv4/tcp_max_syn_backlog)。 不同版本的os会有些差异</strong></p>
<blockquote>
<p>我们写代码的时候从来没有想过这个backlog或者说大多时候就没给他值（那么默认就是50），直接忽视了他，首先这是一个知识点的忙点；其次也许哪天你在哪篇文章中看到了这个参数，当时有点印象，但是过一阵子就忘了，这是知识之间没有建立连接，不是体系化的。但是如果你跟我一样首先经历了这个问题的痛苦，然后在压力和痛苦的驱动自己去找为什么，同时能够把为什么从代码层推理理解到OS层，那么这个知识点你才算是比较好地掌握了，也会成为你的知识体系在TCP或者性能方面成长自我生长的一个有力抓手</p>
</blockquote>
<h4 id="netstat-命令"><a href="#netstat-命令" class="headerlink" title="netstat 命令"></a>netstat 命令</h4><p>netstat跟ss命令一样也能看到Send-Q、Recv-Q这些状态信息，不过如果这个连接不是<strong>Listen状态</strong>的话，Recv-Q就是指收到的数据还在缓存中，还没被进程读取，这个值就是还没被进程读取的 bytes；而 Send 则是发送队列中没有被远程主机确认的 bytes 数</p>
<pre><code>$netstat -tn  
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address   Foreign Address State  
tcp0  0 server:8182  client-1:15260 SYN_RECV   
tcp0 28 server:22    client-1:51708  ESTABLISHED
tcp0  0 server:2376  client-1:60269 ESTABLISHED
</code></pre><p> <strong>netstat -tn 看到的 Recv-Q 跟全连接半连接没有关系，这里特意拿出来说一下是因为容易跟 ss -lnt 的 Recv-Q 搞混淆，顺便建立知识体系，巩固相关知识点 </strong>  </p>
<h5 id="Recv-Q-和-Send-Q-的说明"><a href="#Recv-Q-和-Send-Q-的说明" class="headerlink" title="Recv-Q 和 Send-Q 的说明"></a>Recv-Q 和 Send-Q 的说明</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Recv-Q</div><div class="line">Established: The count of bytes not copied by the user program connected to this socket.</div><div class="line">Listening: Since Kernel 2.6.18 this column contains the current syn backlog.</div><div class="line"></div><div class="line">Send-Q</div><div class="line">Established: The count of bytes not acknowledged by the remote host.</div><div class="line">Listening: Since Kernel 2.6.18 this column contains the maximum size of the syn backlog.</div></pre></td></tr></table></figure>
<h6 id="通过-netstat-发现问题的案例"><a href="#通过-netstat-发现问题的案例" class="headerlink" title="通过 netstat 发现问题的案例"></a>通过 netstat 发现问题的案例</h6><p>自身太慢，比如如下netstat -t 看到的Recv-Q有大量数据堆积，那么一般是CPU处理不过来导致的：</p>
<p><img src="/images/oss/77ed9ba81f70f7940546f0a22dabf010.png" alt="image.png"></p>
<p>下面的case是接收方太慢，从应用机器的netstat统计来看，也是压力端回复太慢（本机listen 9108端口)</p>
<p><img src="/images/oss/1579241362064-807d8378-6c54-4a2c-a888-ff2337df817c.png" alt="image.png" style="zoom:80%;"></p>
<p>send-q表示回复从9108发走了，没收到对方的ack，<strong>基本可以推断PTS到9108之间有瓶颈</strong></p>
<p>上面是通过一些具体的工具、指标来认识全连接队列（工程效率的手段）   </p>
<h3 id="实践验证一下上面的理解"><a href="#实践验证一下上面的理解" class="headerlink" title="实践验证一下上面的理解"></a>实践验证一下上面的理解</h3><p>把java中backlog改成10（越小越容易溢出），继续跑压力，这个时候client又开始报异常了，然后在server上通过 ss 命令观察到：</p>
<pre><code>Fri May  5 13:50:23 CST 2017
Recv-Q Send-QLocal Address:Port  Peer Address:Port
11         10         *:3306               *:*
</code></pre><p>按照前面的理解，这个时候我们能看到3306这个端口上的服务全连接队列最大是10，但是现在有11个在队列中和等待进队列的，肯定有一个连接进不去队列要overflow掉，同时也确实能看到overflow的值在不断地增大。</p>
<h4 id="Tomcat和Nginx中的Accept队列参数"><a href="#Tomcat和Nginx中的Accept队列参数" class="headerlink" title="Tomcat和Nginx中的Accept队列参数"></a>Tomcat和Nginx中的Accept队列参数</h4><p>Tomcat默认短连接，backlog（Tomcat里面的术语是Accept count）Ali-tomcat默认是200, Apache Tomcat默认100. </p>
<pre><code>#ss -lnt
Recv-Q Send-Q   Local Address:Port Peer Address:Port
0       100                 *:8080            *:*
</code></pre><p>Nginx默认是511</p>
<pre><code>$sudo ss -lnt
State  Recv-Q Send-Q Local Address:PortPeer Address:Port
LISTEN    0     511              *:8085           *:*
LISTEN    0     511              *:8085           *:*
</code></pre><p>因为Nginx是多进程模式，所以看到了多个8085，也就是多个进程都监听同一个端口以尽量避免上下文切换来提升性能   </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>全连接队列、半连接队列溢出这种问题很容易被忽视，但是又很关键，特别是对于一些短连接应用（比如Nginx、PHP，当然他们也是支持长连接的）更容易爆发。 一旦溢出，从cpu、线程状态看起来都比较正常，但是压力上不去，在client看来rt也比较高（rt=网络+排队+真正服务时间），但是从server日志记录的真正服务时间来看rt又很短。</p>
<p>jdk、netty等一些框架默认backlog比较小，可能有些情况下导致性能上不去，比如这个 <a href="https://www.atatech.org/articles/12919" target="_blank" rel="external">《netty新建连接并发数很小的case》 </a><br>都是类似原因</p>
<p>希望通过本文能够帮大家理解TCP连接过程中的半连接队列和全连接队列的概念、原理和作用，更关键的是有哪些指标可以明确看到这些问题（<strong>工程效率帮助强化对理论的理解</strong>）。</p>
<p>另外每个具体问题都是最好学习的机会，光看书理解肯定是不够深刻的，请珍惜每个具体问题，碰到后能够把来龙去脉弄清楚，每个问题都是你对具体知识点通关的好机会。</p>
<h3 id="最后提出相关问题给大家思考"><a href="#最后提出相关问题给大家思考" class="headerlink" title="最后提出相关问题给大家思考"></a>最后提出相关问题给大家思考</h3><ol>
<li>全连接队列满了会影响半连接队列吗？</li>
<li>netstat -s看到的overflowed和ignored的数值有什么联系吗？</li>
<li>如果client走完了TCP握手的第三步，在client看来连接已经建立好了，但是server上的对应连接实际没有准备好，这个时候如果client发数据给server，server会怎么处理呢？（有同学说会reset，你觉得呢？）</li>
</ol>
<blockquote>
<p>提出这些问题就是以这个知识点为抓手，让你的知识体系开始自我生长</p>
</blockquote>
<hr>
<p>参考文章：</p>
<p><a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank" rel="external">http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html</a></p>
<p><a href="http://www.cnblogs.com/zengkefu/p/5606696.html" target="_blank" rel="external">http://www.cnblogs.com/zengkefu/p/5606696.html</a></p>
<p><a href="http://www.cnxct.com/something-about-phpfpm-s-backlog/" target="_blank" rel="external">http://www.cnxct.com/something-about-phpfpm-s-backlog/</a></p>
<p><a href="http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/" target="_blank" rel="external">http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</a></p>
<p><a href="http://jin-yang.github.io/blog/network-synack-queue.html#" target="_blank" rel="external">http://jin-yang.github.io/blog/network-synack-queue.html#</a></p>
<p><a href="http://blog.chinaunix.net/uid-20662820-id-4154399.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-20662820-id-4154399.html</a></p>
<p><a href="https://www.atatech.org/articles/12919" target="_blank" rel="external">https://www.atatech.org/articles/12919</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/12995358.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/12995358.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/03/01/黄奇帆的复旦经济课--笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/01/黄奇帆的复旦经济课--笔记/" itemprop="url">分析与思考——黄奇帆的复旦经济课笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-01T17:30:03+08:00">
                2020-03-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/其它/" itemprop="url" rel="index">
                    <span itemprop="name">其它</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="分析与思考——黄奇帆的复旦经济课笔记"><a href="#分析与思考——黄奇帆的复旦经济课笔记" class="headerlink" title="分析与思考——黄奇帆的复旦经济课笔记"></a>分析与思考——黄奇帆的复旦经济课笔记</h1><p>这是一次奇特的读书经历，因为黄奇帆这本书的内容主要是17-19年的一些报告内容，里面给出了他的一些看法以及各种数据，所以在3年后的2021年来读的话，我能够搜索当前的数据来印证他的判断，这种穿越的感觉很好。</p>
<p>黄比较厉害的是思路、逻辑清晰，然后各种数据比较丰富，所以他对问题的判断和看法比较准确。另外一个准确的是任大炮。相较另外一些教授、专家就混乱无比了，比如<a href="https://baike.baidu.com/item/%E6%98%93%E5%AE%AA%E5%AE%B9/10443874" target="_blank" rel="external">易宪容</a></p>
<p>比如，去杠杆比他预估的要差多了，杠杆这几年不但没去成反而加大了；政府债务占比也没有按他的预期减少，稳中有增；房地产开工面积也在增加，当然增速很慢了，占GDP比重也在缓慢增加。</p>
<p>以下引用内容都是从网络搜索所得，其它内容为黄书中直接复制出来的，附录内容没有读。</p>
<hr>
<h2 id="去杠杆"><a href="#去杠杆" class="headerlink" title="去杠杆"></a>去杠杆</h2><ul>
<li>国家M2。2017年中国M2已经达到170万亿元，这几个月下来到5月底已经是176万亿元，我们的GDP 2017年是82万亿元，M2与GDP之比已经是2.1∶1。美国的M2跟它的GDP之比是0.9∶1，美国GDP是20万亿美元，他们M2统统加起来，尽管已经有了三次（Q1、Q2、Q3）的宽松，现在的M2其实也就18万亿美元，所以我们这个指标就显然是非常非常的高。</li>
<li>我们国家金融业的增加值。2017年年底占GDP总量的7.9%，2016年年底是8.4%，2017年五六月份到了8.8%，下半年开始努力地约束金融业夸张性的发展或者说太高速的发展，把这个约束了一下，所以到2017年年底是7.9%，2018年1—5月份还是在7.8%左右。这个指标也是世界最高，全世界金融增加值跟全世界GDP来比的话，平均是在4%左右。像日本尽管有泡沫危机，从20世纪80年代一直到现在，基本上在百分之五点几。美国从1980年到2000年也是百分之五点几，2000年以来，一直到次贷危机才逐渐增加。2008年崩盘之前占GDP的百分之八点几。这几年约束了以后，现在是在7%左右。它是世界金融的中心，全世界的金融资源集聚在华尔街，集聚在美国，产生的金融增加值也就是7%，我们并没有把世界的金融资源增加值、效益利润集聚到中国来，中国的金融业为何能够占中国80多万亿元GDP的百分之八点几？中国在十余年前，也就是2005年的时候，金融增加值占当时GDP的5%不到，百分之四点几，快速增长恰恰是这些年异常扩张、高速发展的结果。这说明我们金融发达吗？不对，其实是脱实就虚，许多金融GDP把实体经济的利润转移过来，使得实体经济异常辛苦，从这个意义上说，这个指标是泡沫化的表现。</li>
<li>我们国家宏观经济的杠杆率。非银行非金融的企业负债，政府部门的负债(40%多)，加上居民部门的负债(50%)，三方面加起来是GDP的2.5倍，250%，在世界100多个国家里我们是前5位，是偏高的，我们跟美国相当，美国也是250%，日本是最高的，现在是440%，英国也比较高，当然欧洲一些国家，比如意大利或者西班牙，以及像希腊等一些债务财政出问题的小的国家，他们也异常的高。即使这样，我们的债务杠杆率排在世界前5位，也是异常的高。</li>
<li>每年全社会新增的融资。我们的企业每年都要融资，除了存量借新还旧，存量之外，有个增量，我们在十年前每年全社会新增融资量是五六万亿元，五年前新增的量在10万亿—12万亿元，2017年新增融资18万亿元。每年新增的融资里面，股权资本金性质的融资只占总的融资量的10%不到一点，也就是说91%是债权，要么是银行贷款，要么是信托，要么是小贷公司，或者直接是金融的债券。大家可以想象，如果每年新增的融资总是90%以上是债权，10%是股权的话，这个数学模型推它十年，十年以后中国的债务不会缩小，只会越来越高。</li>
</ul>
<blockquote>
<p><strong>2021年4月末，广义货币(M2)余额226.21万亿元,同比增长8.1%，增速分别比上月末和上年同期低1.3个和3个百分点</strong>；狭义货币(M1)余额60.54万亿元,同比增长6.2%，增速比上月末低0.9个百分点，比上年同期高0.7个百分点；流通中货币(M0)余额8.58万亿元,同比增长5.3%。当月净回笼现金740亿元。</p>
</blockquote>
<p>杠杆率是250%，在全世界来说是排在前面，是比较高的。这个指标里面又分成三个方面，其中政府的债务占GDP不到50%，国家统计公布的数据是40%多，但是有些隐性债务没算进去，就算算进去也不到50%。第二个方面是老百姓的债务，十年前还只占10%，五年前到了20%，我印象中有一年中国人民银行也说了，中国居民部门的债务还可以放一点杠杆，这两年按揭贷款异常发展起来，居民债务两年就上升到50%。老百姓这一块的债务，主要是房产的债务，也包括信用卡和其他投资，总的也占GDP50%左右。两个方面加起来就等于GDP，剩下的160%是企业债务，美国企业负债是美国GDP的60%，而中国企业的负债是GDP的160%，这个指标是有问题的</p>
<p>中国政府的40多万亿元债务是中央政府的债务有十几万亿元，地方政府的债务有20多万亿元，加在一起40多万亿元，占GDP 50%左右，我们是把区县、地市、省级政府到国家级统算在一起的。所以，我们中国政府的债务算得是比较充分的。</p>
<blockquote>
<p>人民网北京2021年4月7日电 （记者王震）国务院新闻办公室4月7日就贯彻落实“十四五”规划纲要，加快建立现代财税体制有关情况举行发布会。财政部部长助理欧文汉介绍，截至2020年末，地方政府债务余额25.66万亿元，控制在全国人大批准的限额28.81万亿元之内，加上纳入预算管理的中央政府债务余额20.89万亿元，全国政府债务余额46.55万亿元，政府债务余额与GDP的比重为45.8%，低于国际普遍认同的60%警戒线，风险总体可控。</p>
<p><img src="/images/951413iMgBlog/1626778550752-653e12e4-aa1c-4048-b72f-5c253ab560c9.png" alt="image.png"></p>
</blockquote>
<p>去企业负债杠杆方法：第一是坚定不移地把没有任何前途的、过剩的企业，破产关闭，伤筋动骨、壮士断腕，该割肿瘤就要割掉一块(5%)。第二是通过收购兼并，资产重组去掉一部分坏账，去掉一部分债务，同时又保护生产力(5%)。第三是优势的企业融资，股权融资从新增融资的10%增加到30%、40%、50%，这应该是一个要通过五年、十年实现的中长期目标。第四是柔性地、柔和地通货膨胀，稀释债务，一年2个点，五年就是10个点，也很可观。第五是在基本面上保持M2增长率和GDP增长率与物价指数增长率之和大体相当。我相信通过这五方面措施，假以时日，务实推进，那么有个三五年、近十年，我们去杠杆四五十个百分点的宏观目标就会实现。</p>
<p>怎么把股市融资和私募投资从10%上升到40%、50%，这就是我们中国投融资体制，金融体制要发生一个坐标转换。这里最重要的，实际上是两件事。第一件事，要把证券市场、资本市场搞好。十几年前上证指数两三千点，现在还是两三千点。美国股市指数翻了两番，香港股市指数也翻了两番。我国国民经济总量翻了两番，为什么股市指数不增长？这里面要害是什么呢？可以说有散户的结构问题，有长期资本缺乏的问题，有违规运作处罚不力的问题，有注册制不到位的问题，各种问题都可以说。归根到底最重要的一个问题是什么呢？就是退市制度没搞好。</p>
<h2 id="供应侧结构化改革三去一降一补"><a href="#供应侧结构化改革三去一降一补" class="headerlink" title="供应侧结构化改革三去一降一补"></a>供应侧结构化改革三去一降一补</h2><p>供应侧结构化改革三去一降一补：去产能、去库存、去杠杆、降成本、补短板</p>
<p>中国所有的货物运输量占GDP的比重是15%，美国、欧洲都在7%，日本只有百分之五点几。我们占15%就比其他国家额外多了几万亿元的运输成本。中国交通运输的物流成本高，除了基础设施很大一部分是新建投资、折旧成本较高以外，相当大的部分是管理体制造成的。由于我们的管理、软件、系统协调性、无缝对接等方面存在很多问题，造成了各种物流成本抬高。在这个问题上，各个地方，各个系统，各个行业都把这方面问题重视一下、协调一下，人家7%，我们哪怕降不到7%的GDP占比，能够降3%—4%的占比，就省了3万亿—4万亿元</p>
<p>按国际惯例，个人所得税率一般低于企业所得税率，我国的个人所得税采取超额累进税率与比例税率相结合的方式征收，工资薪金类为超额累进税率5%—45%。最高边际税率45%，是在1950年定的，当时我国企业所得税率是55%，个人所得税率定在45%有它的理由。现在企业所得税率已经降到25%，个人所得税率还保持在45%，明显高于前者，也高于大多数国家25%左右的水平。</p>
<p>到2018年底，中国个人住房贷款余额25.75万亿元，而公积金个人住房贷款余额为4.98万亿元，在整个贷款余额中不到20%，其为人们购房提供低息贷款的功能完全可以交由商业银行按揭贷款来解决。可以考虑公积金合并为年金</p>
<p>互联网金融平台、物联网金融平台、物联网+金融形成的平台会在这里起颠覆性的、全息性的、五个全方位信息（全产业链的信息、全流程的信息、全空间布局的信息、全场景的信息、全价值链的信息）的配置作用</p>
<p>“三元悖论”，即安全、廉价、便捷三者不可能同时存在</p>
<p>鉴于互联网商业平台公司的商业模式已经远远超出传统商业规模所能达成的社会影响力，所以，互联网商业平台公司与其说是在从事商业经营，不如说是在从事网络社会的经营和管理。正因如此，国家有必要通过立法，构建一种由网络安全、金融安全、社会安全、财政安全等相关部门参加的“互联网技术研发信息日常跟踪制度”。</p>
<h2 id="货币"><a href="#货币" class="headerlink" title="货币"></a>货币</h2><p>“二战”后建立的“布雷顿森林体系”，即“美元与黄金挂钩，其他国家货币与美元挂钩”的“双挂钩”制度，其实质也是一种“金本位”制度，而1971年美国总统尼克松宣布美元与黄金脱钩也正式标志着美元放弃了以黄金为本位的货币制度，随之采取的是“主权信用货币制”</p>
<p>近十余年来，美国为了摆脱金融危机，政府债务总量从2007年的9万亿美元上升到2019年的22万亿美元，已经超过美国GDP</p>
<p>从1970年开始，欧美、日本等大部分世界发达国家逐步采用了“<strong>主权信用货币制</strong>”，在实践中总体表现较好，货币的发行量与经济增量相匹配，保持了经济的健康增长和物价的稳定。<strong>这种货币制度通常以M3、经济增长率、通胀率、失业率等作为“间接锚”，并不是完全的无锚制货币。</strong></p>
<p>从1970年到2008年，美国政府在应用主权信用货币制度的过程中基本遵守货币发行纪律，货币的增长与GDP增长、政府债务始终保持适度的比例。1970年，美国基础货币约为700亿美元，2007年底约为8200亿美元，大约增长了12倍。与此同时，美国GDP从1970年的1.1万亿美元增长到2007年的14.5万亿美元，大概增长了13倍。美元在全世界外汇储备中的占比稳定在65%以上</p>
<p>从2008年年底至2014年10月，美联储先后出台三轮量化宽松政策，总共购买资产约3.9万亿美元。美联储持有的资产规模占国内生产总值的比例从2007年年底的6.1%大幅升至2014年年底的25.3%，资产负债表扩张到前所未有的水平。</p>
<p>从2008年到2019年，美国基础货币供应量从8200亿美元飙升到4万亿美元，整体约增长了5倍，与此同时，美国GDP仅增长了1.5倍，基础货币的发行增速几乎是同期GDP增速的3倍以上。在这种货币政策的驱动下，美国股市开启了十年长牛之路，股市从6000点涨到28000点。各类资产价格开始重新走上上涨之路，美国经济沉浸在一片欣欣向荣之中。</p>
<p>1913年美国《联邦储备法案》规定，美元的发行权归美联储所有。美国政府没有发行货币的权力，只有发行国债的权力。但实际上，美国政府可以通过发行国债间接发行货币。美国国会批准国债发行规模，财政部将设计好的不同种类的国债债券拿到市场上进行拍卖，最后拍卖交易中没有卖出去的由美联储照单全收，并将相应的美元现金交给财政部。这个过程中，财政部把国债卖给美联储取得现金，美联储通过买进国债获得利息，两全其美，皆大欢喜。</p>
<p>2008年前美国以国家信用为担保发行美债，美债余额始终控制在GDP的70%比例之内，国家信用良好。美债作为全世界交易规模最大的政府债券，长久以来保持稳定的收益，成为黄金以外另一种可靠的无风险资产。美国的货币供给总体上与世界经济的需求也保持着适当的比例，进一步加强了美元的信用。</p>
<p>美国GDP占全球GDP的比重已经从50%下降到24%，但美元仍然是主要的国际交易结算货币。尽管近年来美元在国际储备货币中的占比逐渐从70%左右滑落到62%，但美元的地位短时间内仍然看不到动摇的迹象</p>
<p>布雷顿森林体系解体后，各国以美元为货币“名义锚”的强制性随之弱化，但在自由选择条件下，绝大多数发展中国家仍然选择美元为“名义锚”，实行了锚定美元，允许一定浮动的货币调控制度。另有一些发展中国家选择锚定原来的宗主国，以德国马克、法国法郎和英镑等货币为“名义锚”。而主要的发达国家在货币寻锚的过程中，经历了一些波折之后，大多选择以“利率、货币发行量、通货膨胀率”等指标作为货币发行中间目标，实际上锚定的是国内资产。总之，从当前来看，世界的货币大致形成了两类发行制度：以“其他货币”为名义锚的货币发行体制和以“本国资产”为名义锚的货币发行体系，也称为主权信用货币制度。</p>
<p>香港采用的货币制度很独特，被称为“联系汇率”制度，又被称为“钞票局”制度，据说是19世纪一位英国爵士的发明。其基本内容是香港以某一种国际货币为锚（20世纪70年代以前以英镑为本位，80年代后改以美元为本位），即以该货币为储备发行港币，通过中央银行吞吐储备货币来达到稳定本币汇率的目标。在这种货币制度下，不仅需要储备相当规模的锚货币，其还有一个重大缺陷是必须放弃独立的货币政策，即本位货币加降息时，其也必须跟随。因此，这种货币制度只适用于小国或者小型经济体，对大国或大型经济体则不适用。</p>
<p>从新中国成立到如今，人民币发行制度经历了从“物资本位制”到“汇兑本位制”两个阶段，这两种不同时期实施的货币制度在当时都有效地促进了国民经济的发展。1995年以后，面对新的形势，中国人民银行探索通过改革实行了新的货币制度——“汇兑本位制”，即通过发行人民币对流入的外汇强制结汇，人民币汇率采取盯住美元的策略，从而保持人民币的汇率基本稳定。</p>
<p>在“汇兑本位制”下，我国主要有两种渠道完成人民币的发行。第一种，当外资到我国投资时，需要将其持有的外币兑换成人民币，这就是被结汇。结汇以后，在中国人民银行资产负债表上，一边增加了外汇资产，另一边增加了存款准备金的资产，这实际上就是基础货币发出的过程。中央银行的资产负债表中关于金融资产分为两部分，一部分是外币资产，另一部分是基础货币。基础货币包括M0和存款准备金，而这部分的准备金就是因为外汇占款而出现的。</p>
<p>第二种则是贸易顺差。中国企业由于进出口业务产生贸易顺差，实际上是外汇结余。企业将多余的外汇卖给商业银行，再由央行通过发行基础货币来购买商业银行收到的外汇。商业银行收到央行用于购买外汇的基础货币，就会通过M0流入社会。长此以往，就会增加通货膨胀的风险。央行为规避通货膨胀的风险，就会通过提高准备金率将多出的基础货币回收。</p>
<p>在“汇兑本位制”下，外汇储备可以视作人民币发行的基础或储备，且由于实行强制结汇，外汇占款逐渐成为我国基础货币发行的主要途径，到2013年末达到83%的峰值，此后略有下降，截至2019年7月末，外汇占款占中国人民银行资产总规模达到59.35%，说明有近六成人民币仍然通过外汇占款的方式发行。</p>
<blockquote>
<p><strong>现代货币理论</strong>（缩写<strong>MMT</strong>）是一种<a href="https://zh.wikipedia.org/wiki/非主流經濟學" target="_blank" rel="external">非主流</a><a href="https://zh.wikipedia.org/wiki/现代货币理论#cite_note-Heterodox_Views_of_Money_and_Modern_Monetary_Theory_(MMT" target="_blank" rel="external">[1]</a>:_Phil_Armstrong-1)<a href="https://zh.wikipedia.org/wiki/宏观经济学" target="_blank" rel="external">宏观经济学</a>理论，认为现代货币体系实际上是一种政府信用货币体系。<a href="https://zh.wikipedia.org/wiki/现代货币理论#cite_note-2" target="_blank" rel="external">[2]</a> 现代货币理论即<a href="https://zh.wikipedia.org/wiki/主權國家" target="_blank" rel="external">主权国家</a>的货币并不与任何商品和其他外币挂钩，只与未来<a href="https://zh.wikipedia.org/wiki/税收" target="_blank" rel="external">税收</a>与<a href="https://zh.wikipedia.org/wiki/公债" target="_blank" rel="external">公债</a>相对应。<a href="https://zh.wikipedia.org/wiki/现代货币理论#cite_note-3" target="_blank" rel="external">[3]</a>因为主权货币具有<a href="https://zh.wikipedia.org/wiki/无限法偿" target="_blank" rel="external">无限法偿</a>性质，没有名义预算约束，只存在<a href="https://zh.wikipedia.org/wiki/通貨膨脹" target="_blank" rel="external">通货膨胀</a>的实际约束。–基本上就是：主权信用货币制度</p>
</blockquote>
<p>“汇兑本位制”的实质：锚定美元</p>
<p>我国的人民币汇率制度基于两个环节。第一，人民币的汇率是人民币和外币之间的交换比率，是人民币与一篮子货币形成的一个比价。我国在同其他国家进行投资、贸易时，人民币按照汇率进行兑换。由于美元是目前世界上最主要的货币，所以虽然人民币与一篮子货币形成相对均衡的比价，但由于美元在一篮子货币中占有较大的比重，人民币最重要的比价货币是美元。第二，我国实行结汇制，即我国的商业银行、企业，基本上不能保存外汇，必须将收到的外汇卖给央行。因此，由于我国的货币发行的基础是外汇，而美元在我国的外汇占款、一篮子货币中占比较高，因此可以说人民币是间接锚定美元发行的。</p>
<p>截至2018年12月底，中国人民银行资产总规模为37.25万亿元，其中外汇占款达21.25万亿元。外汇占款在货币发行中的份额已经从2013年的83%降低至2008年初的57%左右。与此同时，央行对其他存款性公司债权迅速扩张，从2014年到2016年底扩张了2.4倍，占总资产份额从7.4%升至24.7%。这说明了随着外汇占款成为基础货币回笼而非投放的主渠道，央行主要通过公开市场操作和各类再贷款便利操作购买国内资产来投放货币，不失为在外汇占款不足的情况下供给货币的明智选择。同时，央行连续降低法定存款准备金率，提高了货币乘数，一定程度上也缓解了国内流动性不足的问题。</p>
<p>外汇占款在货币发行存量中的比重仍然接近60%，只是通过一些货币政策工具缓解了原来“汇兑本位制”的问题。一旦日后出现大量贸易顺差导致外汇储备增加，货币发行制度就会又回到老路上。</p>
<p>实施“主权信用货币制度”是大国崛起的必然选择</p>
<p>从根本上来说，税收是货币的信用，财政可以是货币发行的手段，而且是最高效公平的手段，央行买国债是能够自主收放的货币政策手段。一旦货币超发后，央行只需要提高利率、提高存款准备金率回收基础货币，而财政部门也可以通过增加税收、注销政府债券的方式来消除多余的货币、避免通货膨胀。</p>
<p>实际上，信用货币制度最大的问题在于锚的不清晰、不稳定，缺乏刚性。</p>
<blockquote>
<p><strong>特别提款权</strong>（Special Drawing Right，<strong>SDR</strong>），亦称“纸黄金”（Paper Gold），最早发行于1969年，是国际货币基金组织根据会员国认缴的份额分配的，可用于偿还国际货币基金组织债务、弥补会员国政府之间国际收支逆差的一种账面资产。 其价值由美元、欧元、人民币、日元和英镑组成的一篮子储备货币决定。</p>
</blockquote>
<p>主权信用货币背景下，人民币是由国债做锚的。中央银行为了发行基础货币，需要购买财政部发行的国债，但中央银行不能购买财政部为了弥补财政亏空发行的国债。《中华人民共和国中国人民银行法》规定，中国人民银行不得直接认购、包销国债和其他政府债券。这意味着中国人民银行不能以政府的债权作为抵押发行货币，只能参与国债二级市场的交易而不能参与国债一级市场的发行，央行直接购买国债来发行基础货币的方式就被法律禁止了。因此，建立以国债为基础的人民币发行制度，必须对相关法律法规进行修改。</p>
<h2 id="2017年5月-房地产"><a href="#2017年5月-房地产" class="headerlink" title="2017年5月 房地产"></a>2017年5月 房地产</h2><p>中国房地产和实体经济存在“十大失衡”——土地供求失衡、土地价格失衡、房地产投资失衡、房地产融资比例失衡、房地产税费占地方财力比重失衡、房屋销售租赁比失衡、房价收入比失衡、房地产内部结构失衡、房地产市场秩序失衡、政府房地产调控失衡</p>
<p>土地调控得当、法律制度到位、土地金融规范、税制结构改革和公租房制度保障，并特别强调了“地票制度”对盘活土地存量，提高耕地增量的重要意义</p>
<p>为国家粮食战略安全计，我国土地供应应逐步收紧，2015年供地770万亩，2016年700万亩，今年计划供应600万亩</p>
<p>国家每年批准供地中，约有三分之一用于农村建设性用地，比如水利基础设施、高速公路等，真正用于城市的只占三分之二，这部分又一分为三：55%左右用于各种基础设施和公共设施，30%左右给了工业，实际给房地产开发的建设用地只有15%。这是三分之二城市建设用地中的15%，摊到全部建设用地中只占到10%左右，这个比例是不平衡的</p>
<p>对于供过于求的商品，哪怕货币泛滥，也可能价格跌掉一半。货币膨胀只是房价上涨的必要条件而非充分条件，只是外部因素而非内部因素。内因只能是供需关系</p>
<p>住房作为附着在土地上的不动产，地价高房价必然会高，地价低房价自然会低，地价是决定房价的根本性因素。如果只有货币这个外因存在，地价这个内因不配合，房价想涨也是涨不起来的。控制房价的关键就是要控制地价。</p>
<p>拍卖机制，加上新供土地短缺，旧城改造循环，这三个因素相互叠加，地价就会不断上升—核心加大供地可解地价过高</p>
<p>按经济学的经验逻辑，一个城市的固定资产投资中房地产投资每年不应超过25%</p>
<p>正常情况下，一个家庭用于租房的支出最好不要超过月收入的六分之一，超过了就会影响正常生活。买房也如此，不能超过职工全部工作年限收入的六分之一，按每个人一生工作40年左右时间算，“6—7年的家庭年收入买一套房”是合理的。—-中国每个人体制外算20年工作时间，体制内可算35年</p>
<p>从均价看，一线城市北京、上海、广州、深圳、杭州等，房价收入比往往已到40年左右。这个比例在世界已经处于很高的水平了。考虑房价与居民收入比，必须高收入对高房价，低收入对低房价，均价对均价。有人说，纽约房子比上海还贵，伦敦海德公园的房价也比上海高。但伦敦城市居民的人均收入要高出上海几倍。就均价而言，伦敦房价收入比还是在10年以内。</p>
<p>每年固定资产投资不应超过GDP的60%。如果GDP有1万亿元，固定资产投资达到1.3万亿元甚至1.5万亿元，一年两年可以，长远就会不可持续。固定资产投资不超过GDP的60%，再按“房地产投资不超过固定资产投资的25%”，也符合“房地产投资不超过GDP六分之一”这一基本逻辑。</p>
<p>大陆31个省会城市和直辖市中，房地产投资连续多年占GDP 60%以上的有5个，占40%以上的有16个，显然偏高</p>
<blockquote>
<p>房地产17-18w亿,大概8.5w亿是直接留给卖地的地方政府<br>之后3-4w亿是各种建筑商供应商的辛苦钱<br>还有1w亿+流向的银行贷款的利息<br>1w亿+是各种非银金融机构,如信托和平安保险等<br>之后又是一轮税收,然后才是房地产商和房地产人<br>搞死房地产也许容易,但是你得指条明路,让这些人找到新地方做业务活着啊..</p>
<p><a href="http://m.fangchan.com/data/13/2020-03-09/6642690492314489641.html#:~:text=该报告显示%3A,、19.3%和18.8%。" target="_blank" rel="external">上海易居房地产研究院3月9日发布《2019年区域房地产依赖度》。该报告显示</a>:房地产开发投资占GDP比重可以用来衡量当地经济对房地产的依赖程度，占比越高，说明经济对房地产的依赖度越高。2019年，房地产开发投资占GDP比重排名前三位的省市分别是海南、天津和重庆，占比分别为25.2%、19.3%和18.8%。</p>
<p>2020年杭州市GDP总量达到16106亿元，比2019年增长3.9%</p>
<p>有15个城市去年房地产开发投资额超过1000亿元，其中超过2000亿元的共8个，分别是杭州、郑州、广州、武汉、成都、南京、西安和昆明，杭州、郑州、广州三城更是超过了3000亿元。杭州以3397.27亿元在26城中位居榜首</p>
<p>2019年，除了杭州土地出让金继续领跑外，数据显示南京的卖地收入达到了1696.8亿元，同比增长77.32%，福州增幅为63.37%，昆明为59.85%，武汉为27.89%。去年土地出让金额没有超过1000亿元的城市中，合肥土地出让收入增长了33.32%，长沙增长了33.75%，贵阳增长了52%。</p>
<p>2020年，GDP前10强的城市依次为：上海、北京、深圳、广州、重庆、苏州、成都、杭州、武汉、南京。</p>
<p><img src="/images/951413iMgBlog/1627013968961-98b92fc1-b93c-47db-bbb2-03c84f5071c3.png" alt="image.png"></p>
</blockquote>
<p>2011年，全国人民币贷款余额54.8万亿元，其中房地产贷款余额10.7万亿元，占比不到20%。这一比例逐年走高，2016年全国106万亿元的贷款余额中，房地产贷款余额26.9万亿元，占比超过25%。也就是说，房地产占用了全部金融资金量的25%，而房地产贡献的GDP只有7%左右。2016年全国贷款增量的45%来自房地产，一些国有大型银行甚至70%—80%的增量是房地产。从这个意义上讲，房地产绑架了太多的金融资源，导致众多金融“活水”没有进入到实体经济，就是“脱实就虚”的具体表现。</p>
<p>这些年，中央加地方的全部财政收入中，房地产税费差不多占了35%，乍一看来，这一比例感觉还不高。但考虑到房地产税费属地方税、地方费，和中央财力无关，把房地产税费与地方财力相比较，则显得比重太高。全国10万亿元地方税中，有40%也就是4万亿是与房地产关联的，再加上土地出让金3.7万亿元，全部13万亿元左右的地方财政预算收入中就有近8万亿元与房地产有关（60%）。政府的活动太依赖房地产，地方政府财力离了房地产是会断粮的，这也是失衡的。</p>
<p>一般中等城市每2万元GDP造1平方米就够了，再多必过剩。对大城市而言，每平方米写字楼成本高一些，其资源利用率也会高一些，大体按每平方米4万元GDP来规划。</p>
<p>一个城市的土地供应总量一般可按每人100平方米来控制，这应该成为一个法制化原则。100万城市人口就供应100平方千米。爬行钉住，后发制人。</p>
<p>人均100平方米的城市建设用地，该怎么分配呢？不能都拿来搞基础设施、公共设施，也不能都拿来搞商业住宅。大体上，应该有55平方米用于交通、市政、绿地等基础设施和学校、医院、文化等公共设施，这是城市环境塑造的基本需要。对工业用地，应该控制在20平方米以内，每平方千米要做到100亿元产值。剩下的25平用于房地产开发</p>
<p>房产税应包括五个要点：（1）对各种房子存量、增量一网打尽，增量、存量一起收；（2）根据房屋升值额度计税，如果1%的税率，价值100万元的房屋就征收1万元，升值到500万元税额就涨到5万元；（3）越高档的房屋持有成本越高，税率也要相对提高；（4）低端的、中端的房屋要有抵扣项，使得全社会70%—80%的中低端房屋的交税压力不大；（5）房产税实施后，已批租土地70年到期后可不再二次缴纳土地出让金，实现制度的有序接替。这五条是房产税应考虑的基本原则。</p>
<p>房地产调控的长效机制：一是金融；二是土地；三是财税；四是投资；五是立法。</p>
<p>在1990年之前，中国是没有商品房交易的，那时候一年就是1000多万平方米。在1998年和1999年的时候，中国房地产一年新建房的交易销售量实际上刚刚达到1亿平方米。从1998年到2008年，这十年里平均涨了6倍，有的城市实际上涨到8倍以上，十年翻三番。2007年，销售量本来已经到了差不多7亿平方米，2008年全球金融危机发生了，在这个冲击下，中国的房产交易量也下降了，萎缩到6亿平方米。后来又过了5年，到了2012年前后，房地产的交易量翻了一番，从6亿平方米增长到12亿平方米。从2012年到2018年，又增加了5亿平方米。总之在过去的20年，中国房地产每年的新房销售交易量差不多从1亿平方米增长到17亿平方米，翻了四番多。</p>
<p>今后十几年，中国每年的房地产新房的交易量不仅不会继续增长翻番，还会每年小比例地有所萎缩，或者零增长，或者负增长。十几年以后，每年房地产的新房销售交易量可能下降到10亿平方米以内，大体上减少40%的总量。</p>
<p>今后十几年的房地产业发展趋势，不会是17亿平方米、18亿平方米、20亿平方米、30亿平方米，而是逐渐萎缩，当然这个萎缩不会在一年里面大规模萎缩20%、30%，大体上有十几年的过程，每年往下降。十几年后产生的销售量下降到10亿平方米以下</p>
<blockquote>
<p><a href="http://www.ce.cn/cysc/fdc/fc/202101/18/t20210118_36234860.shtml#:~:text=当日发布的数据显示,的15.97万亿元。" target="_blank" rel="external">http://www.ce.cn/cysc/fdc/fc/202101/18/t20210118_36234860.shtml#:~:text=%E5%BD%93%E6%97%A5%E5%8F%91%E5%B8%83%E7%9A%84%E6%95%B0%E6%8D%AE%E6%98%BE%E7%A4%BA,%E7%9A%8415.97%E4%B8%87%E4%BA%BF%E5%85%83%E3%80%82</a>:  <strong>2020</strong>年,<strong>中国</strong>商品房<strong>销售</strong>面积176086万平方米,比上年增长2.6%,2019年为下降0.1%。 商品房<strong>销售</strong>额173613亿元,增长8.7%,增速比上年提高2.2个百分点。 此前,<strong>中国</strong>商品房<strong>销售</strong>面积和<strong>销售</strong>额的最高纪录分为2018年的近17.17亿平方米和2019年的15.97万亿元。</p>
<p><img src="/images/951413iMgBlog/1627372037702-30d2c2b6-6c58-48a5-8891-071e918b47c1.png" alt="image.png"></p>
</blockquote>
<p>1990年，中国人均住房面积只有6平方米；到2000年，城市人均住房面积也仅十几平方米，现在城市人均住房面积已近50平方米。人均住房面积偏小，也会产生改善性的购房需求。</p>
<blockquote>
<p>根据国家统计局公布的数据，1982年至2019年，我国常住人口城镇<strong>化率</strong>从21.1%上升至60.6%，上升超过39个百分点；同期，户籍人口城镇<strong>化率</strong>仅从17.6%上升至44.4%，上升不到27个百分点。</p>
<p>经济日报-<strong>中国</strong>经济网北京2月28日讯国家统计局网站2月28日发布我国<strong>2020</strong>年国民经济和社会发展统计公报。 公报显示，<strong>2020</strong>年末，我国常住人口城镇<strong>化率</strong>超过60%。Feb 28, 2021</p>
<p><img src="/images/951413iMgBlog/1627371470209-2e0d97e4-bccc-4910-acc1-024da9f647af.png" alt="image.png"></p>
<p>官方数据显示，2020年，我国的城镇化率高达63.89%，比发达国家80%的平均水平低了16.11%，与美国82.7%的城镇化水平还有18.81%的距离。</p>
<p><img src="/images/951413iMgBlog/1627371544599-b6933e76-7231-4b80-882f-104e3767a7b3.png" alt="image.png"></p>
<p><img src="/images/951413iMgBlog/1627371610846-ed83be5e-08f1-4685-ab5e-b4f04c706c20.png" alt="image.png"></p>
</blockquote>
<p>当前我国人均住房面积已经达到近50平方米</p>
<p>2012年，住建部下发了一个关于住宅和写字楼等各种商品性房屋的建筑质量标准，把原来中国住宅商品房30年左右的安全标准提升到了至少70年，甚至100年。这意味着从2010年以后，新建造的各种城市商品房，理论上符合质量要求的话，可以使用70年到100年，这也就是说老城市的折旧改造量会大量减少。</p>
<blockquote>
<p>实际据说12年后因为利润率的原因房子质量在下降？！待证</p>
</blockquote>
<p>中国各个省的省会城市大体上发展规律都会遵循“一二三四”的逻辑。所谓“一二三四”，就是这个省会城市往往占有这个省土地面积的10%不到，一般是5%—10%；但是它的人口一般会等于这个省总人口的20%；它的GDP有可能达到这个省总GDP的30%；它的服务业，不论是学校、医院、文化等政府主导的公共服务，还是金融、商业、旅游等市场化的服务业，一般会占到这个省总量的40%。</p>
<p>河南省有1亿人口，郑州目前只有1000万人口。作为省会城市，应承担全省20%的人口，所以十几年、20年以后郑州发展成2000万人口一点不用惊讶。同样的道理，郑州的GDP现在到了1万亿元，整个河南5万亿元，它贡献了20%，如果要30%的话应该是1.5万亿元，还相差甚远。对于服务业，一个地方每100万人应该有一个三甲医院，如果河南省1亿人口要有100个的话，郑州就应该有40个，它现在才9个三甲医院，每造一个三甲医院投资20多亿元，产生的营业额也是20多亿元，作为服务业，营业额对增加值贡献比率在80%以上。</p>
<p>大家可以关注现在近十个人口超过1000万的国家级超级大城市，根据这些省总的经济人口规模去算一下，它们都有十几年以后人口增长500万以上的可能。只要人口增长了，城市住宅房地产就会跟上去。所以我刚才说的大都市、超级大城市，人口在1000万—2000万之间的有一批城市还会扩张，过了2000万的，可能上面要封顶，但是在1000万—2000万之间的不会封顶，会形成它的趋势。</p>
<p>在今后的十几年，房地产开发不再是四处开花，而会相对集聚在省会城市及同等级区域性中心城市、都市圈中的中小城市和城市群中的大中型城市三个热点地区。</p>
<blockquote>
<p>根据住房和城乡建设部于2020年底最新公布的《<a href="https://baike.baidu.com/item/2019年城市建设统计年鉴/56070783" target="_blank" rel="external">2019年城市建设统计年鉴</a>》，符合中国“超大城市”标准的共有<a href="https://baike.baidu.com/item/上海/114606" target="_blank" rel="external">上海</a>、<a href="https://baike.baidu.com/item/北京/128981" target="_blank" rel="external">北京</a>、<a href="https://baike.baidu.com/item/重庆/23586" target="_blank" rel="external">重庆</a>、<a href="https://baike.baidu.com/item/广州/72101" target="_blank" rel="external">广州</a>、<a href="https://baike.baidu.com/item/深圳/140588" target="_blank" rel="external">深圳</a>、<a href="https://baike.baidu.com/item/天津/132308" target="_blank" rel="external">天津</a>。</p>
<p>东莞、武汉、成都、杭州、南京、郑州、西安、济南、沈阳和青岛这10个<strong>城市</strong>的城区<strong>人口</strong>处于<strong>500万</strong>到1000<strong>万</strong>之间，属于特大<strong>城市</strong>。Jan 12, 2021</p>
<p>根据住建部最新数据，2019年底，<strong>长沙城区人口</strong>384.75万人，建成区面积377.95平方公里。 与2018年的374.43万人相比，2019年底<strong>长沙城区人口</strong>增加约10万人。 注意，这个<strong>城区</strong>的统计范围包括雨花、岳麓、芙蓉、天心、开福、望城六区，应该不包括<strong>长沙</strong>县，因为<strong>长沙</strong>县目前不是设区<strong>市</strong>，也不是县级<strong>市</strong>。Jan 26, 2021</p>
<p>长沙市总人口810万</p>
<p><img src="/images/951413iMgBlog/1627372812022-e9a7792a-234e-43f9-ae31-d25851761097.png" alt="image.png"></p>
<p><img src="/images/951413iMgBlog/1627372683037-b3f0c7df-c4c4-4b31-8ec1-7de1e4fe5f53.png" alt="image.png"></p>
</blockquote>
<p>从通货膨胀看，我国M2已经到了190万亿元，会不会今后十年M2再翻两番？不可能，这两年国家去杠杆、稳金融已经做到了让M2的增长率大体上等于GDP的增长率加物价指数，这几年的GDP增长率百分之六点几，物价指数加两个点，所以M2在2017年、2018年都是八点几，2019年1—6月份是8.5%，基本上是这样。M2如果是八点几的话，今后十几年，基本上是GDP增长率加物价指数，保持均衡的增长。如果中国的GDP今后十几年平均增长率大体在5%—6%，房地产价格的增长大体上不会超过M2的增长率，也不会超过GDP的增长率，一般会小于老百姓家庭收入的增长率。</p>
<p>9万多个房产企业中，排名在前的15%大开发商在去年的开发，实际的施工、竣工、销售的面积，在17亿平方米里面它们可能占了85%。意思是什么呢？15%的企业解决了17亿平方米的85%，就是14亿多平方米，剩下的企业只干了那么2亿多平方米，有大量的空壳公司。</p>
<p>中国的房地产企业，我刚才说9万多个，9万多个房产商的总负债率2018年是84%。中国前十位的销售规模在1万亿元左右的房产商，它们的负债率也是在81%。</p>
<blockquote>
<p>REITs: Real Estate Investment Trusts，译为房地产投资信托基金</p>
</blockquote>
<p>地王的产生都是因为房产商背后有银行，所以政府的土地部门，只要资格审查的时候查定金从哪儿来，拍卖的时候资金从哪儿来，只要审查管住这个，就一定能管住地王现象的出现</p>
<blockquote>
<p>2000年，中国房地产增加值仅为4141亿元，在当年GDP中占比4.1%。二十年后，2020年中国房地产增加值跃升至74553亿元，GDP占比7.3%。在20年时间里，房地产增加值大涨70412亿元，增长率达78%。</p>
<p>房地产增加值从1万亿元增加到2万亿元用了5年，从2万亿元到3万亿元用了3年，从5万亿元到7万亿年只用了4年。房地产新创造价值的增长速度越来越快。</p>
<p><img src="/images/951413iMgBlog/1627376113361-25fa29fb-1293-4a31-ad62-dabf28616be6.png" alt="image.png"></p>
<p>香港公租房面积：現時<strong>公</strong>屋單位的<strong>人均</strong>室內<strong>面積</strong>不得小於七平方米，惟房委會近年興建單位時，<strong>面積</strong>均僅停留在合格線，供一至二人入住的甲類單位，<strong>面積</strong>只及14平方米，二至三人的乙類單位也只有21平方米。 尤有甚者，有報章整理房委會資料，2020至2024年度的甲、乙類單位佔52%，總數3.44萬個，較跟2015至2019年度落成單位高11個百分點。Jan 19, 2021</p>
</blockquote>
<h2 id="对外开放"><a href="#对外开放" class="headerlink" title="对外开放"></a>对外开放</h2><p>近40年以来世界贸易的格局，国际贸易的产品结构、企业组织和管理的方式，国家和国家之间贸易有关的政策均发生了重要的变化。货物贸易中的中间品的比重上升到70%以上，在总贸易量中服务贸易的比重从百分之几变成了30%。</p>
<p>产品交易和贸易格局的变化，导致跨国公司的组织管理方式发生变化，谁控制着产业链的集群、供应链的纽带、价值链的枢纽，谁就是龙头老大。由于世界贸易格局特征的变化，由于跨国公司管理世界级的产品的管理模式的变化，也就是“三链”这种特征性的发展，引出了世界贸易新格局中的一个新的国际贸易规则制度的变化，就是零关税、零壁垒和零补助“三零”原则的提出，并将是大势所趋。中国自贸试验区的核心，也就是“三零”原则在自己这个区域里先行先试，等到国家签订FTA的时候，自贸试验区就为国家签订FTA提供托底的经验。</p>
<p>现在一个产品，涉及几千个零部件，由上千个企业在几百个城市、几十个国家，形成一个游走的逻辑链，那么谁牵头、谁在管理、谁把众多的几百个上千个中小企业产业链中的企业组织在一起，谁就是这个世界制造业的大头、领袖、集群的灵魂。</p>
<p>能提出行业标准、产品标准的企业往往是产品技术最大的发明者。谁控制供应链，谁其实就是供应链的纽带。你在组织整个供应链体系，几百个、上千个企业，都跟着你的指挥棒，什么时间、什么地点、到哪儿，一天的间隙都不差，在几乎没有零部件库存的背景下，几百个工厂，非常有组织、非常高效地在世界各地形成一个组合。在这个意义上讲，供应链的纽带也十分重要。</p>
<p>50年前关税平均是50%—60%。到了20世纪八九十年代，关税一般都降到了WTO要求的关税水平，降到了10%以下。WTO要求中国的关税也要下降。以前我们汽车进口关税最高达到170%。后来降到50%。现在我们汽车进口关税还在20%的水平。但我们整个中国的加权平均的关税率，20世纪八九十年代是在40%—50%，到了90年代末加入WTO的时候到了百分之十几。WTO给我们一个过渡期，要求我们15年内降到10%以内。我们到2015年的确降到9.5%，到去年已经降到7.5%。现在整个世界的贸易平均关税已经降到了5%以内，美国现在是2.5%。</p>
<blockquote>
<p>目前对于<strong>进口汽车</strong>收取的<strong>关税</strong>税率是25%，还有对<strong>进口</strong>车收取17%的增值税，而根据<strong>汽车</strong>的排量收取不同的消费税税率。 排量在在1.5升(含)以下3%，1.5升至2.0升(含) 5%，2.0升至2.5升(含) 9%，2.5升至3.0升(含)12%，3.0升至4.0升(含) 15%，4.0升以上20%。Jun 26, 2020</p>
<p><img src="/images/951413iMgBlog/xUmW-hcaquev3441323.png" alt="进口货物的增值税则在2018年5月1日进行了下调，由17%降为16%。"></p>
<p>假定这辆到岸价24万的进口车为中规进口车（原厂授权，4S店销售），排气量为4.0以上，且到岸时间为5月1日前，套入相关计算公式，则其所要缴纳税费为：</p>
<p>　　关税：24万×25% =6万</p>
<p>　　消费税：（24万+6万）÷（1-40%）×40% =20万</p>
<p>　　增值税：（24万+6万+20万）×17% =8.5万</p>
<p>　　税费合计34.5万，加上24万的到岸价，总共58.5万，即这辆进口汽车的抵岸价。</p>
<p>　　常规而言，这个价格与90万指导价间的31.5万价差，即为运输等成本费用和国内经销商的利润。</p>
<p><img src="/images/951413iMgBlog/nBJ2-hcaquev3441409.png" alt="24万的进口车为何国内要卖90万？"></p>
</blockquote>
<p>在这七八年，FTA，双边贸易体的讨论，或者是一个地区，五六个国家、七八个国家形成一个贸易体的讨论就不断增加，成为趋势。给人感觉好像发达国家都在进行双边谈判，把WTO边缘化了</p>
<blockquote>
<p>所谓自由贸易协定（Free Trade Agrement:<strong>FTA</strong>）是指两个或两个以上的国家（包括独立关税地区）根据WTO相关规则，为实现相互之间的贸易自由化所进行的地区性贸易安排。 由自由贸易协定的缔约方所形成的区域称为自由贸易区。 <strong>FTA</strong>的传统<strong>含义</strong>是缔约国之间相互取消货物贸易关税和非关税贸易壁垒。</p>
<p>2019年10月8日，日本驻美国大使杉山晋辅与美国贸易谈判代表莱特希泽在白宫正式签署新日美贸易协定。美国总统特朗普不仅亲自出席见证签字，还邀请多位西北部农业州农民代表参加，声称自己为美国农民赢得了巨大市场，巧妙地将国际贸易协定变成了国内拉票筹码。</p>
</blockquote>
<p>中国已经形成了世界产业链里面最大的产业链集群，但是这个集群里面，我们掌控纽带的，掌控标准的，掌控结算枢纽的，掌控价值链枢纽的企业并不多。比如华为，华为的零部件，由3600多家大大小小供应链上的企业生产。这全球的3000多家企业每年都来开供应链大会。华为就是掌控标准。它的供应链企业比苹果多两倍。为什么？苹果主要做手机，华为既做手机又做服务器、通信设备。通信设备里面的零部件原材料更多。所以，它掌控产业链上中下游的集群，掌控标准，也掌控价值链中的牵制中枢。</p>
<p>零关税第一个好处：对进口中间品实行零关税，将降低企业成本，提高产品的国际竞争力。</p>
<p>当中国制造业实施零关税的时候，事实上对于整个制造业产业链的完整化、集群化和纽带、控制能力有好处，对于中国制造业的产业链、供应链、价值链在中国形成枢纽、形成纽带、形成集团的龙头等各方面会有提升作用，这是第二个好处。</p>
<p>关税下降，会促进中国的生产力结构的提升，促进我们企业的竞争能力的加强，使得我们工商企业的成本下降。</p>
<p>我们现在差不多有6.6亿吨农作物粮食是在中国的土地上生产出来的，但是我们现在每年要进口农产品1亿吨。加在一起，也就是中国14亿人，一年要吃7.6亿吨农作物。这1亿吨里面，有个基本的分类。我们现在进口的1亿吨里面，有8000多万吨进口的是大豆、300多万吨小麦、300多万吨玉米、300多万吨糖，另外就是进口的猪肉、牛肉和其他的肉类，也有几百万吨。</p>
<p>从2010年起步，当年人民币只有近千亿元的结算量，从这些年发展来看，2018年已经到7万亿元了。也就是说，中国进出口贸易里面有7万亿元人民币是人家收了人民币而不去收美元</p>
<h3 id="自贸区"><a href="#自贸区" class="headerlink" title="自贸区"></a>自贸区</h3><p>在过去的40年，我们国家的开放有五个基本特点：</p>
<p>第一个就是以出口导向为基础，利用国内的资源优势和劳动力的比较优势，推动出口发展，带动中国经济更好地发展；</p>
<p>第二个就是以引进外资为主，弥补我们中国当时还十分贫困的经济和财力；</p>
<p>第三个就是以沿海开放为主，各种开发区或者各种特区，包括新区、保税区，都以沿海地区先行，中西部内陆逐步跟进；</p>
<p>第四个就是开放的领域主要是工业、制造业、房地产业、建筑业等先行开放，至于服务业、服务贸易、金融保险业务开放的程度比较低，即以制造业、建筑业等第二产业开放为主；</p>
<p>第五个就是我们国家最初几十年的开放以适应国际经济规则为主，用国际经济规则、国际惯例倒逼国内营商环境改革、改善，倒逼国内的各种机制体制变化，是用开放倒逼改革的这样一个过程。</p>
<p>2012年以后我们每年退休的人员平均在1500万人左右，但每年能够上岗的劳动力，不管农村的、城市的，新生的劳动力是1200万左右。实际最近五年，我们每年少了300万劳动力补充。</p>
<p>本来GDP应该每掉1个点退出200万就业人口。为什么几年下来没有感觉到有500万、1000万下岗工人出现呢？就是因为人口出现了对冲性均衡，正好这边下降，要退出人员，跟那边补充的人员不足，形成了平衡，所以实际上我们基础性劳动力条件发生了变化，人口红利逐步退出。</p>
<p>如果一个国家在五到十年里，连续都是世界第一、第二、第三的进口大国，那一定成为世界经济的强国。要知道进口大国是和经济强国连在一起的，美国是世界第一大进口国，它也理所当然是世界最大的经济强国。</p>
<h2 id="中美贸易战"><a href="#中美贸易战" class="headerlink" title="中美贸易战"></a>中美贸易战</h2><p>关于中国加入WTO，莱特希泽有五个观点：一是中国入世美国吃亏论；二是中国没有兑现入世承诺；三是中国强制美国企业转让技术；四是中国的巨额外汇顺差造成了美国2008年的金融危机；五是中国买了大量美国国债，操纵了汇率。</p>
<p>2008年美国金融危机原因是2001年科技互联网危机后，当时股市一年里跌了50%以上，再加上“9·11”事件，美国政府一是降息，从6%降到1%，二是采取零按揭刺激房地产，三是将房地产次贷在资本市场1∶20加杠杆搞CDS，最终导致泡沫经济崩盘。2007年，美国房地产总市值24.3万亿美元、占GDP比重达到173%；股市总市值达到了20万亿美元、占GDP比重达到135%。2008年金融危机后，美国股市缩水50%，剩下10万亿美元左右；房地产总市值缩水40%，从2008年的25万亿美元下降到2009年的15万亿美元。</p>
<p>一个成熟的经济体，政府每年总有占GDP　20%—30%的财政收入要支出使用，通常会生成15%左右的GDP，这部分国有经济产生的GDP是通过政府的投资和消费产生的，美国和欧洲各国都是如此。比如2017年，美国的GDP中有13.5%是美国政府财力支出形成的GDP。中国政府除了财政税收以外，还有土地批租等预算外收入，所以，中国政府财力支出占GDP的比重相对高一点，占17%左右</p>
<p>自1971年布雷顿森林体系解体，美元脱离了金本位，形成“无锚货币”，美元的货币发行体制转化为政府发债，美联储购买发行基础货币之后，全球的基础货币总量如脱缰野马，快速增长。从1970年不到1000亿美元，到1980年的3500亿美元，到1990年的7000亿美元，到2000年的1.5万亿美元，到2008年的4万亿美元，到2017年的21万亿美元。其中，美元的基础货币也从20世纪70年代的几百亿美元发展到今天的6万亿美元。</p>
<blockquote>
<p>报告还预计，截至2021财年底，<strong>美国</strong>联邦公共<strong>债务</strong>将达23万亿美元，约占<strong>美国GDP</strong>的103%；到2031财年，<strong>美国</strong>联邦公共<strong>债务</strong>占<strong>GDP</strong>的比重将进一步升至106%。Jul 2, 2021</p>
<p><img src="/images/951413iMgBlog/image-20210729172410379.png" alt="image-20210729172410379"></p>
<p>以下资料来源：<a href="https://pdf.dfcfw.com/pdf/H3_AP202106171498408427_1.pdf?1623944100000.pdf" target="_blank" rel="external">https://pdf.dfcfw.com/pdf/H3_AP202106171498408427_1.pdf?1623944100000.pdf</a></p>
<p><img src="/images/951413iMgBlog/image-20210729172820145.png" alt="image-20210729172820145"></p>
<p><img src="/images/951413iMgBlog/image-20210729173011549.png" alt="image-20210729173011549"></p>
<p><img src="/images/951413iMgBlog/image-20210729173116617.png" alt="image-20210729173116617"></p>
</blockquote>
<p>2020-01</p>
<p>上市公司几千家，几十家金融企业每年利润几乎占了几千家实体经济企业利润的50%，这个比重太高，造成我们脱实向虚。三是金融企业占GDP的比重是百分之八点几，是全世界最高的。世界平均金融业GDP占全球GDP的5%左右，欧洲也好、美国也好、日本也好，只要一到7%、8%，就会冒出一场金融危机，自己消除坏账后萎缩到5%、6%，过了几年，又扩张达到7%、8%，又崩盘。</p>
<blockquote>
<p>SWIFT 是 Society for Worldwide Interbank Financial Telecommunications 的缩写，翻译成中文叫做「环球银行金融电讯协会」。看名字就知道，他是个搞通讯的，还冠冕堂皇的是一个非盈利性组织。</p>
<p>另外，SWIFT 官网上是这么用中文介绍自己的：SWIFT 为社群提供报文传送平台和通信标准，并在连接、集成、身份识别、数据分析和合规等领域的产品和服务（我一字未改，这多语言做的……语句都不通顺，好在不影响理解）；用英文则是这么介绍的：SWIFT is a global member-owned cooperative and the world’s leading provider of secure financial messaging services。</p>
</blockquote>
<p>2022 <a href="https://weibo.com/1687445053/MgRSVCTy5" target="_blank" rel="external">黄奇帆谈贫富差距和共同富裕</a>：</p>
<p>差距的三个原因：</p>
<ol>
<li>地域差异，胡焕庸线导致东西部差距，西部地理天气条件不如东部；2000年收入是3.5比1，2010年是3比1,2020年是2 比1</li>
<li>城乡差距，农民没有财产性收入(比如房屋、宅基地升值；比如贷款通货膨胀后变相升值)；</li>
<li>行业差距，金融业息差大躺着赚钱，互联网、房地产法律法规不完善赚钱快</li>
</ol>
<p>解决办法：</p>
<ol>
<li>地域：通过大资本大投入在西部搞能源、资源开发，从西气东送到西电东送，比如青海700平方公里的太阳能基地年产生2万亿度电，2毛钱一度就是4000亿人民币，有可能能成为中国的阿拉伯能源基地；发展生产力</li>
<li>2020年土地法，可以直接拍卖宅基地？获得土地收入以及增值。落户城市享受城市工作机会尤其是社保体系；要素生产</li>
<li>更多地开放金融牌照，降低息差；完善相关法律法规，让竞争尽可能在公平公正的环境下良性发生；行业边际效应、结构性均衡、资源优化配置</li>
</ol>
<p>目前间接税过高，过高的间接税虽然有一部分用来转移支付但是效果不是最优，以后希望降低来改善营商环境藏富于民。以后可能会收取房产税、遗产税(都有免征门槛)等</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/26/TCP相关参数解释/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/26/TCP相关参数解释/" itemprop="url">TCP相关参数解释</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-26T17:30:03+08:00">
                2020-01-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TCP相关参数解释"><a href="#TCP相关参数解释" class="headerlink" title="TCP相关参数解释"></a>TCP相关参数解释</h1><p>读懂TCP参数前得先搞清楚内核中出现的HZ、Tick、Jiffies三个值是什么意思</p>
<h2 id="HZ"><a href="#HZ" class="headerlink" title="HZ"></a>HZ</h2><p>它可以理解为1s，所以120*HZ就是120秒，HZ/5就是200ms。</p>
<p>HZ表示CPU一秒种发出多少次时间中断–IRQ-0，Linux中通常用HZ来做时间片的计算（<a href="http://blog.csdn.net/bdc995/article/details/4144031" target="_blank" rel="external">参考</a>）。</p>
<p>这个值在内核编译的时候可设定100、250、300或1000，一般设置的是1000</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#cat /boot/config-`uname -r` |grep &apos;CONFIG_HZ=&apos;</div><div class="line">CONFIG_HZ=1000 //一般默认1000, Linux核心每隔固定周期会发出timer interrupt (IRQ 0)，HZ是用来定义</div><div class="line">每一秒有几次timer interrupts。举例来说，HZ为1000，代表每秒有1000次timer interrupts</div></pre></td></tr></table></figure>
<p>HZ的设定：<br>#make menuconfig<br>processor type and features—&gt;Timer frequency (250 HZ)—&gt;</p>
<p>HZ的不同值会影响timer （节拍）中断的频率</p>
<h2 id="Tick"><a href="#Tick" class="headerlink" title="Tick"></a>Tick</h2><p>Tick是HZ的倒数，意即timer interrupt每发生一次中断的间隔时间。如HZ为250时，tick为4毫秒(millisecond)。</p>
<h2 id="Jiffies"><a href="#Jiffies" class="headerlink" title="Jiffies"></a>Jiffies</h2><p>Jiffies为Linux核心变数(32位元变数，unsigned long)，它被用来记录系统自开机以来，已经过多少的tick。每发生一次timer interrupt，Jiffies变数会被加一。值得注意的是，Jiffies于系统开机时，并非初始化成零，而是被设为-300*HZ (arch/i386/kernel/time.c)，即代表系统于开机五分钟后，jiffies便会溢位。那溢出怎么办?事实上，Linux核心定义几个macro(timer_after、time_after_eq、time_before与time_before_eq)，即便是溢位，也能藉由这几个macro正确地取得jiffies的内容。</p>
<p>另外，80x86架构定义一个与jiffies相关的变数jiffies_64 ，此变数64位元，要等到此变数溢位可能要好几百万年。因此要等到溢位这刻发生应该很难吧。那如何经由jiffies_64取得jiffies呢?事实上，jiffies被对应至jiffies_64最低的32位元。因此，经由jiffies_64可以完全不理会溢位的问题便能取得jiffies。</p>
<h2 id="数据取自于4-19内核代码中的-include-net-tcp-h"><a href="#数据取自于4-19内核代码中的-include-net-tcp-h" class="headerlink" title="数据取自于4.19内核代码中的 include/net/tcp.h"></a>数据取自于4.19内核代码中的 include/net/tcp.h</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">//rto的定义，不让修改，到每个ip的rt都不一样，必须通过rtt计算所得, HZ 一般是1000</div><div class="line">#define TCP_RTO_MAX     ((unsigned)(120*HZ))</div><div class="line">#define TCP_RTO_MIN     ((unsigned)(HZ/5)) //在rt很小的环境中计算下来RTO基本等于TCP_RTO_MIN</div><div class="line"></div><div class="line">/* Maximal number of ACKs sent quickly to accelerate slow-start. */</div><div class="line">#define TCP_MAX_QUICKACKS       16U //默认前16个ack必须quick ack来加速慢启动</div><div class="line"></div><div class="line">//默认delay ack不能超过200ms</div><div class="line">#define TCP_DELACK_MAX  ((unsigned)(HZ/5))  /* maximal time to delay before sending an ACK */</div><div class="line">#if HZ &gt;= 100</div><div class="line">//默认 delay ack 40ms，不能修改和关闭</div><div class="line">#define TCP_DELACK_MIN  ((unsigned)(HZ/25))     /* minimal time to delay before sending an ACK */</div><div class="line">#define TCP_ATO_MIN     ((unsigned)(HZ/25))</div><div class="line">#else</div><div class="line">#define TCP_DELACK_MIN  4U</div><div class="line">#define TCP_ATO_MIN     4U</div><div class="line">#endif</div><div class="line"></div><div class="line">#define TCP_SYNQ_INTERVAL       (HZ/5)  /* Period of SYNACK timer */</div><div class="line">#define TCP_KEEPALIVE_TIME      (120*60*HZ)     /* two hours */</div><div class="line">#define TCP_KEEPALIVE_PROBES    9               /* Max of 9 keepalive probes    */</div><div class="line">#define TCP_KEEPALIVE_INTVL     (75*HZ)</div><div class="line"></div><div class="line">/* cwnd init 默认大小是10个拥塞窗口，也可以通过sysctl_tcp_init_cwnd来设置，要求内核编译的时候支持*/</div><div class="line">#if IS_ENABLED(CONFIG_TCP_INIT_CWND_PROC)</div><div class="line">extern u32 sysctl_tcp_init_cwnd;</div><div class="line">/* TCP_INIT_CWND is rvalue */</div><div class="line">#define TCP_INIT_CWND           (sysctl_tcp_init_cwnd + 0)</div><div class="line">#else</div><div class="line">/* TCP initial congestion window as per rfc6928 */</div><div class="line">#define TCP_INIT_CWND           10</div><div class="line">#endif</div><div class="line"></div><div class="line">/* Flags in tp-&gt;nonagle 默认nagle算法关闭的*/</div><div class="line">#define TCP_NAGLE_OFF           1       /* Nagle&apos;s algo is disabled */</div><div class="line">#define TCP_NAGLE_CORK          2       /* Socket is corked         */</div><div class="line">#define TCP_NAGLE_PUSH          4       /* Cork is overridden for already queued data */</div><div class="line"></div><div class="line">#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT</div><div class="line">                                  * state, about 60 seconds     */</div><div class="line">                                  </div><div class="line">#define TCP_SYN_RETRIES  6      /* This is how many retries are done</div><div class="line">                                 * when active opening a connection.</div><div class="line">                                 * RFC1122 says the minimum retry MUST</div><div class="line">                                 * be at least 180secs.  Nevertheless</div><div class="line">                                 * this value is corresponding to</div><div class="line">                                 * 63secs of retransmission with the</div><div class="line">                                 * current initial RTO.</div><div class="line">                                 */</div><div class="line"></div><div class="line">#define TCP_SYNACK_RETRIES 5    /* This is how may retries are done</div><div class="line">                                 * when passive opening a connection.</div><div class="line">                                 * This is corresponding to 31secs of</div><div class="line">                                 * retransmission with the current</div><div class="line">                                 * initial RTO.</div><div class="line">                                 */</div></pre></td></tr></table></figure>
<p>rto不能设置，而是根据到不同server的rtt计算得到，即使RTT很小（比如0.8ms），但是因为RTO有下限，最小必须是200ms，所以这是RTT再小也白搭；RTO最小值是内核编译是决定的，socket程序中无法修改，Linux TCP也没有任何参数可以改变这个值。</p>
<h3 id="delay-ack"><a href="#delay-ack" class="headerlink" title="delay ack"></a>delay ack</h3><p>正常情况下ack可以quick ack也可以delay ack，redhat在sysctl中可以设置这两个值</p>
<blockquote>
<p>/proc/sys/net/ipv4/tcp_ato_min</p>
</blockquote>
<p>默认都是推荐delay ack的，一定要修改成quick ack的话（3.10.0-327之后的内核版本）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$sudo ip route show</div><div class="line">default via 10.0.207.253 dev eth0 proto dhcp src 10.0.200.23 metric 1024</div><div class="line">10.0.192.0/20 dev eth0 proto kernel scope link src 10.0.200.23</div><div class="line">10.0.207.253 dev eth0 proto dhcp scope link src 10.0.200.23 metric 1024</div><div class="line"></div><div class="line">$sudo ip route change default via 10.0.207.253  dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</div><div class="line"></div><div class="line">$sudo ip route show</div><div class="line">default via 10.0.207.253 dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</div><div class="line">10.0.192.0/20 dev eth0 proto kernel scope link src 10.0.200.23</div><div class="line">10.0.207.253 dev eth0 proto dhcp scope link src 10.0.200.23 metric 1024</div></pre></td></tr></table></figure>
<p>默认开启delay ack的抓包情况如下，可以清晰地看到有几个40ms的ack</p>
<p><img src="/images/oss/7f4590cccf73fd672268dbf0e6a1b309.png" alt="image.png"></p>
<p>第一个40ms 的ack对应的包， 3306收到 update请求后没有ack，而是等了40ms update也没结束，就ack了</p>
<p><img src="/images/oss/b06d3148450fc24fa26b2a9cdfe07831.png" alt="image.png"></p>
<p>同样的机器，执行quick ack后的抓包</p>
<blockquote>
<p>sudo ip route change default via 10.0.207.253  dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</p>
</blockquote>
<p><img src="/images/oss/9fba9819e769494bc09a2a11245e4769.png" alt="image.png"></p>
<p><strong>同样场景下，改成quick ack后基本所有的ack都在0.02ms内发出去了。</strong></p>
<p>比较奇怪的是在delay ack情况下不是每个空ack都等了40ms，这么多包只看到4个delay了40ms，其它的基本都在1ms内就以空包就行ack了。</p>
<p>将 quick ack去掉后再次抓包仍然抓到了很多的40ms的ack。</p>
<p>Java中setNoDelay是指关掉nagle算法，但是delay ack还是存在的。</p>
<p>C代码中关闭的话：At the application level with the <code>TCP_QUICKACK</code> socket option. See <code>man 7 tcp</code> for further details. This option needs to be set with <code>setsockopt()</code> after each operation of TCP on a given socket</p>
<p>连接刚建立前16个包一定是quick ack的，目的是加快慢启动</p>
<p>一旦后面进入延迟ACK模式后，<a href="https://www.cnblogs.com/lshs/p/6038635.html" target="_blank" rel="external">如果接收的还没有回复ACK确认的报文总大小超过88bytes的时候就会立即回复ACK报文</a>。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://access.redhat.com/solutions/407743" target="_blank" rel="external">https://access.redhat.com/solutions/407743</a></p>
<p><a href="https://www.cnblogs.com/lshs/p/6038635.html" target="_blank" rel="external">https://www.cnblogs.com/lshs/p/6038635.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/25/SSD存储原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/25/SSD存储原理/" itemprop="url">存储原理</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-25T17:30:03+08:00">
                2020-01-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="存储原理"><a href="#存储原理" class="headerlink" title="存储原理"></a>存储原理</h1><p>本文记录各种存储、接口等原理性东西</p>
<p><img src="/images/951413iMgBlog/d81b49e46e4c5de80a554a453d92e08f.png" alt="img"></p>
<p><img src="/images/oss/52eca12b2b73a72861fc777919457f5b.png" alt="img"></p>
<h2 id="磁盘信息"><a href="#磁盘信息" class="headerlink" title="磁盘信息"></a>磁盘信息</h2><p>Here are some common ones:</p>
<ul>
<li><code>hdX</code> — ATA hard disk, pre-libata. You’ll only see this with old distros (probably based on Linux 2.4.x or older)</li>
<li><code>sdX</code> — “SCSI” hard disk. Also includes SATA and SAS. And IDE disks using libata (on any recent distro).</li>
<li><code>hdXY</code>, <code>sdXY</code> — Partition on the hard disk <code>hdX</code> or <code>sdX</code>.</li>
<li><code>loopX</code> — Loopback device, used for mounting disk images, etc.</li>
<li><code>loopXpY</code> — Partitions on the loopback device <code>loopX</code>; used when mounting an image of a complete hard drive, etc.</li>
<li><code>scdX</code>, <code>srX</code> — “SCSI” CD, using same weird definition of “SCSI”. Also includes DVD, Blu-ray, etc.</li>
<li><code>mdX</code> — Linux MDraid</li>
<li><code>dm-X</code> — Device Mapper. Use <code>-N</code> to see what these are, or <code>ls -l /dev/mapper</code>. Device Mapper underlies LVM2 and dm-crypt. If you’re using either LVM or encrypted volumes, you’ll see <code>dm-X</code> devices.</li>
</ul>
<p>回顾串行磁盘技术的发展历史，从光纤通道，到SATA，再到SAS，几种技术各有所长。光纤通道最早出现的串行化存储技术，可以满足高性能、高可靠和高扩展性的存储需要，但是价格居高不下；SATA硬盘成本倒是降下来了，但主要是用于近线存储和非关键性应用，毕竟在性能等方面差强人意；SAS应该算是个全才，可以支持SAS和SATA磁盘，很方便地满足不同性价比的存储需求，是具有高性能、高可靠和高扩展性的解决方案。</p>
<h2 id="SSD中，SATA、m2、PCIE和NVME各有什么意义"><a href="#SSD中，SATA、m2、PCIE和NVME各有什么意义" class="headerlink" title="SSD中，SATA、m2、PCIE和NVME各有什么意义"></a>SSD中，SATA、m2、PCIE和NVME各有什么意义</h2><h3 id="高速信号协议"><a href="#高速信号协议" class="headerlink" title="高速信号协议"></a>高速信号协议</h3><p> SAS，SATA，PCIe 这三个是同一个层面上的，模拟串行高速接口。</p>
<ul>
<li>SAS 对扩容比较友好，也支持双控双活。接上SAS RAID 卡，一般在阵列上用的比较多。</li>
<li>SATA 对热插拔很友好，早先台式机装机市场的 SSD基本上都是SATA的，现在的 机械硬盘也是SATA接口居多。但速率上最高只能到 6Gb/s，上限 768MB/s左右，现在已经慢慢被pcie取代。</li>
<li>PCIe 支持速率更高，也离CPU最近。很多设备 如 网卡，显卡也都走pcie接口，当然也有SSD。现在比较主流的是PCIe 3.0,8Gb/s 看起来好像也没比 SATA 高多少，但是 PCIe 支持多个LANE，每个LANE都是 8Gb/s，这样性能就倍数增加了。目前，SSD主流的是 PCIe 3.0x4 lane，性能可以做到 3500MB/s 左右。</li>
</ul>
<h3 id="传输层协议"><a href="#传输层协议" class="headerlink" title="传输层协议"></a>传输层协议</h3><p>SCSI，ATA，NVMe 都属于这一层。主要是定义命令集，数字逻辑层。</p>
<ul>
<li>SCSI 命令集 历史悠久，应用也很广泛。U盘，SAS 盘，还有手机上 UFS 之类很多设备都走的这个命令集。</li>
<li>ATA 则只是跑在SATA 协议上</li>
<li>NVMe 协议是有特意为 NAND 进行优化。相比于上面两者，效率更高。主要是跑在 PCIe 上的。当然，也有NVMe-MI，NVMe-of之类的。是个很好的传输层协议。</li>
</ul>
<h4 id="NVMe（Non-Volatile-Memory-Express）"><a href="#NVMe（Non-Volatile-Memory-Express）" class="headerlink" title="NVMe（Non-Volatile Memory Express）"></a>NVMe（Non-Volatile Memory Express）</h4><p>NVMe其实与AHCI一样都是逻辑设备接口标准（是接口标准，并不是接口），NVMe全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，NVMe的设计之初就有充分利用到PCI-E SSD的低延时以及并行性，还有当代处理器、平台与应用的并行性。SSD的并行性可以充分被主机的硬件与软件充分利用，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升。</p>
<p>NVMe标准是面向PCI-E SSD的，使用原生PCI-E通道与CPU直连可以免去SATA与SAS接口的外置控制器（PCH）与CPU通信所带来的延时。而在软件层方面，NVMe标准的延时只有AHCI的一半不到，NVMe精简了调用方式，执行命令时不需要读取寄存器；而AHCI每条命令则需要读取4次寄存器，一共会消耗8000次CPU循环，从而造成大概2.5微秒的延迟。</p>
<p>NVMe标准和传统的SATA/SAS相比，一个重大的差别是引入了<strong>多队列机制</strong>。</p>
<p>主机与SSD进行数据交互采用“生产者-消费者”模型进行数据交互。在原有AHCI规范中，只定义了一个交互队列，主机与HDD之间的数据交互只能通过一个队列通信，多核处理器也只能通过一个队列与HDD进行数据交互。在传统磁盘存储时代，单队列在一个IO调度器，可以很好的保证提交请求的IO顺序最优化。</p>
<p>而NAND存储介质具有很高的性能，AHCI原有的规范不再适用，NVMe规范替代了原有的AHCI规范，在软件层面的处理命令也进行了重新定义，不再采用SCSI／ATA命令规范集。相比以前AHCI、SAS等协议规范，NVMe规范是一种非常简化，面向新型存储介质的协议规范。该规范将存储外设拉到了处理器局部总线上，性能大为提升。并且<strong>主机和SSD处理器之间采用多队列的设计，适应了多核的发展趋势，每个处理器核与SSD之间可以采用独立的硬件Queue Pair进行数据交互。</strong></p>
<p><img src="/images/951413iMgBlog/7ac6b1847a9bc6e029f3cb51716ef413.png" alt="img"></p>
<p>如上图从软件的角度来看，每个CPU Core都可以创建一对Queue Pair和SSD进行数据交互。Queue Pair由Submission Queue与Completion Queue构成，通过Submission queue发送数据；通过Completion queue接受完成事件。SSD硬件和主机驱动软件控制queue的Head与Tail指针完成双方的数据交互。</p>
<p>nvme多队列</p>
<p><img src="/images/951413iMgBlog/ef86af3155d7dd2e6f78ec4f896179db.png" alt="img"></p>
<p><img src="/images/oss/8cb9e6f80895141d875df66c3d0069f6.png" alt="img"></p>
<h3 id="物理接口"><a href="#物理接口" class="headerlink" title="物理接口"></a>物理接口</h3><p>M.2 , U.2 , AIC, NGFF 这些属于物理接口</p>
<p>像 M.2 可以是 SATA SSD 也可以是 NVMe（PCIe） SSD。金手指上有一个 SATA/PCIe 的选择信号，来区分两者。很多笔记本的M.2 接口也是同时支持两种类型的盘的。</p>
<ul>
<li>M.2 , 主要用在 笔记本上，优点是体积小，缺点是散热不好。</li>
<li>U.2,主要用在 数据中心或者一些企业级用户，对热插拔需求高的地方。优点热插拔，散热也不错。一般主要是pcie ssd(也有sas ssd)，受限于接口，最多只能是 pcie 4lane</li>
<li>AIC，企业，行业用户用的比较多。通常会支持pcie 4lane/8lane，带宽上限更高</li>
</ul>
<h2 id="SSD-的性能特性和机制"><a href="#SSD-的性能特性和机制" class="headerlink" title="SSD 的性能特性和机制"></a>SSD 的性能特性和机制</h2><p>SSD 的内部工作方式和 HDD 大相径庭，我们先了解几个概念。</p>
<p><strong>单元（Cell）、页面（Page）、块（Block）</strong>。</p>
<p>当今的主流 SSD 是基于 NAND 的，它将数字位存储在单元中。每个 SSD 单元可以存储一位或多位。对单元的每次擦除都会降低单元的寿命，所以单元只能承受一定数量的擦除。单元存储的位数越多，制造成本就越低，SSD 的容量也就越大，但是耐久性（擦除次数）也会降低。</p>
<p>一个页面包括很多单元，典型的页面大小是 4KB，页面也是要读写的最小存储单元。SSD 上没有“重写”操作，不像 HDD 可以直接对任何字节重写覆盖。一个页面一旦写入内容后就不能进行部分重写，必须和其它相邻页面一起被整体擦除重置。</p>
<p>多个页面组合成块。一个块(Block)的典型大小为 512KB 或 1MB，也就是大约 128 或 256 (Page–16KB)页。<strong>块是擦除的基本单位，每次擦除都是整个块内的所有页面都被重置。</strong></p>
<p><img src="/images/951413iMgBlog/image-20210915090731401.png" alt="image-20210915090731401"></p>
<p><strong>擦除速度相对很慢，通常为几毫秒</strong>。所以对同步的 IO，发出 IO 的应用程序可能会因为块的擦除，而经历很大的写入延迟。为了尽量地减少这样的场景，保持空闲块的阈值对于快速的写响应是很有必要的。SSD 的垃圾回收（GC）的目的就在于此。GC 可以回收用过的块，这样可以确保以后的页写入可以快速分配到一个全新的页。</p>
<p>SSD的基本结构：</p>
<p><img src="/images/951413iMgBlog/image-20210915090459823.png" alt="image-20210915090459823"></p>
<p>比如Intel P4510 SSD控制器内部集成了两个Cotex A15 ARM core，这两个CPU core各自处理50%物理地址空间的读写命令（不同CPU负责不同的Die，以提高并发度）。在处理IO命令的过程中，为了充分发挥两个cpu的并行处理效率，每个cpu core单次处理的最大数据块是128kB。所以P4510对于128k对齐（4k，8k，16k，32k，64k，128k）或者128k整数倍（256k，512k，1024k）的数据块的处理效率最高。因为这些数据块都能够在SSD内部被组装或者拆分为完整的128k数据块。但是，对于非128k对齐的数据块（68k，132k，260k，516k，1028k），由于每个提交给SSD的写命令都有一个非128k对齐的“尾巴”需要跨CPU来处理，这样便会导致SSD处理单个命令的效率下降，写带宽随之也下降。</p>
<p><img src="/images/951413iMgBlog/8882214c8d1dd6c52cd4b54fbb7a109a.jpg" alt="img" style="zoom:50%;"><img src="/images/951413iMgBlog/bb8e3c40ede0432021d7d56d1387aa08.jpg" alt="img"></p>
<p>SSD内部使用写缓存。写缓存主要用来降低写延迟。当写请求发送给SSD时，写数据会被先保存在写缓存，此时SSD会直接发送确认消息通知主机端写请求已完成，实现最低的写延迟。SSD固件在后台会异步的定期把写缓存中的数据通过写操作命令刷回给NAND颗粒。为了满足写操作的持久化语义，SSD内有大容量电容保证写缓存中数据的安全。当紧急断电情况发生时，固件会及时把写缓存中的数据写回NAND颗粒. 也就是紧急断电后还能通过大电容供电来维持最后的落盘。</p>
<p>SSD内嵌内存容量的问题也限制了大容量NVMe SSD的发展，为了解决内存问题，目前一种可行的方法是增大sector size。标准NVMe SSD的sector size为4KB，为了进一步增大NVMe SSD的容量，有些厂商已经开始采用16KB的sector size。16KB Sector size的普及应用，会加速大容量NVMe SSD的推广。</p>
<p><a href="https://www.bilibili.com/read/cv4139832" target="_blank" rel="external">以海康威视E200P为例</a>，PCB上的硬件PLP掉电保护电路从D200Pro的10个钽电容+6个电感，简化为6个钽电容+6个电感。钽电容来自Panasonic，单颗47uF，6个钽电容并联可以为SSD提供几十毫秒的放电时间，让SSD把处理中的数据写入NAND中并更新映射表。这样的硬件PLP电路对比普通的家用产品要强悍很多。 </p>
<p><img src="/images/951413iMgBlog/1af29b5592f05ee826d96c4efd25d3333e781527.jpg@942w_1226h_progressive.webp" alt="img"></p>
<p>或者如下结构：</p>
<p><img src="/images/951413iMgBlog/image-20220923172817591.png" alt="image-20220923172817591"></p>
<h3 id="SSD存储持久化原理"><a href="#SSD存储持久化原理" class="headerlink" title="SSD存储持久化原理"></a><a href="https://zhuanlan.zhihu.com/p/347599423" target="_blank" rel="external">SSD存储持久化原理</a></h3><p>记录一个比特很容易理解。给电容里面充上电有电压的时候就是 1，给电容放电里面没有电就是 0。采用这样方式存储数据的 SSD 硬盘，我们一般称之为使用了 SLC 的颗粒，全称是 Single-Level Cell，也就是一个存储单元中只有一位数据。</p>
<p>但是，这样的方式会遇到和 CPU Cache 类似的问题，那就是，同样的面积下，能够存放下的元器件是有限的。如果只用 SLC，我们就会遇到，存储容量上不去，并且价格下不来的问题。于是呢，硬件工程师们就陆续发明了 MLC（Multi-Level Cell）、TLC（Triple-Level Cell）以及 QLC（Quad-Level Cell），也就是能在一个电容里面存下 2 个、3 个乃至 4 个比特。</p>
<p>只有一个电容，我们怎么能够表示更多的比特呢？别忘了，这里我们还有一个电压计。4 个比特一共可以从 0000-1111 表示 16 个不同的数。那么，如果我们能往电容里面充电的时候，充上 15 个不同的电压，并且我们电压计能够区分出这 15 个不同的电压。加上电容被放空代表的 0，就能够代表从 0000-1111 这样 4 个比特了。</p>
<p>不过，要想表示 15 个不同的电压，充电和读取的时候，对于精度的要求就会更高。这会导致充电和读取的时候都更慢，所以 QLC 的 SSD 的读写速度，要比 SLC 的慢上好几倍。</p>
<p>SSD对碎片很敏感，类似JVM的内存碎片需要整理，碎片整理就带来了写入放大。也就是写入空间不够的时候需要先进行碎片整理、搬运，这样写入的数据更大了。</p>
<p>SSD寿命：以Intel 335为例再来算一下，BT用户可以用600TB × 1024 / 843 = <strong>728天</strong>，普通用户可以用600TB/2 = <strong>300年</strong>！情况十分乐观</p>
<h4 id="两种逻辑门"><a href="#两种逻辑门" class="headerlink" title="两种逻辑门"></a>两种逻辑门</h4><p><strong>NAND</strong>(NOT-AND)  gate </p>
<p><strong>NOR</strong>(NOT-OR)  gate </p>
<p>如上两种门实现的介质<strong>都是非易失存储介质</strong>，<strong>在写入前都需要擦除</strong>。实际上NOR Flash的一个bit可以从1变成0，而要从0变1就要擦除整块。NAND flash都需要擦除。</p>
<table>
<thead>
<tr>
<th></th>
<th>NAND Flash</th>
<th>NOR Flash</th>
</tr>
</thead>
<tbody>
<tr>
<td>芯片容量</td>
<td>&lt;32GBit</td>
<td>&lt;1GBit</td>
</tr>
<tr>
<td>访问方式</td>
<td>块读写（顺序读写）</td>
<td>随机读写</td>
</tr>
<tr>
<td>接口方式</td>
<td>任意I/O口</td>
<td>特定完整存储器接口</td>
</tr>
<tr>
<td>读写性能</td>
<td>读取快（顺序读） 写入快 擦除快（可按块擦除）</td>
<td>读取快（RAM方式） 写入慢 檫除很慢</td>
</tr>
<tr>
<td>使用寿命</td>
<td>百万次</td>
<td>十万次</td>
</tr>
<tr>
<td>价格</td>
<td>低廉</td>
<td>高昂</td>
</tr>
</tbody>
</table>
<p>NAND Flash更适合在各类需要大数据的设备中使用，如U盘、SSD、各种存储卡、MP3播放器等，而NOR Flash更适合用在高性能的工业产品中。</p>
<p><a href="https://www.amc-systeme.de/files/pdf/wp_adv_flash_type_comparison_2016.pdf" target="_blank" rel="external">高端SSD会选取MLC</a>（Multi-Level Cell）甚至SLC（Single-Level Cell），低端SSD则选取 TLC（Triple-Level Cell）。SD卡一般选取 TLC（Triple-Level Cell）</p>
<p><img src="/images/951413iMgBlog/image-20210603161822079.png" alt="image-20210603161822079"></p>
<p><img src="/images/951413iMgBlog/image-20220105201724752.png" alt="image-20220105201724752"></p>
<p><img src="/images/951413iMgBlog/slc-mlc-tlc-buckets.jpg" alt="slc-mlc-tlc-buckets"></p>
<p>umlc</p>
<p><img src="/images/951413iMgBlog/image-20220105201749003.png" alt="image-20220105201749003"></p>
<p>NOR FLash主要用于：Bios、机顶盒，大小一般是1-32MB</p>
<p>对于TLC NAND （每个NAND cell存储3 bits的信息），下面列出了每种操作的典型耗时的范围：</p>
<p>​      读操作（Tread）    ：      50-100us，</p>
<p>​      写操作（Tprog）    ：     500us-5ms，</p>
<p>​      擦除操作（Terase） ：      2-10ms。</p>
<h4 id="为什么断电后SSD不丢数据"><a href="#为什么断电后SSD不丢数据" class="headerlink" title="为什么断电后SSD不丢数据"></a>为什么断电后SSD不丢数据</h4><p>SSD的存储硬件都是NAND Flash。实现原理和通过改变电压，让电子进入绝缘层的浮栅(Floating Gate)内。断电之后，电子仍然在FG里面。但是如果长时间不通电，比如几年，仍然可能会丢数据。所以换句话说，SSD的确也不适合作为冷数据备份。比如标准要求SSD：温度在30度的情况下，数据要能保持52周。</p>
<h3 id="写入放大（Write-Amplification-or-WA"><a href="#写入放大（Write-Amplification-or-WA" class="headerlink" title="写入放大（Write Amplification, or WA)"></a>写入放大（Write Amplification, or WA)</h3><p>这是 SSD 相对于 HDD 的一个缺点，即实际写入 SSD 的物理数据量，有可能是应用层写入数据量的多倍。一方面，页级别的写入需要移动已有的数据来腾空页面。另一方面，GC 的操作也会移动用户数据来进行块级别的擦除。所以对 SSD 真正的写操作的数据可能比实际写的数据量大，这就是写入放大。一块 SSD 只能进行有限的擦除次数，也称为编程 / 擦除（P/E）周期，所以写入放大效用会缩短 SSD 的寿命。</p>
<p>SSD 的读取和写入的基本单位，不是一个比特（bit）或者一个字节（byte），而是一个页（Page）。SSD 的擦除单位就更夸张了，我们不仅不能按照比特或者字节来擦除，连按照页来擦除都不行，我们必须按照块来擦除。</p>
<p>SLC 的芯片，可以擦除的次数大概在 10 万次，MLC 就在 1 万次左右，而 TLC 和 QLC 就只在几千次了。这也是为什么，你去购买 SSD 硬盘，会看到同样的容量的价格差别很大，因为它们的芯片颗粒和寿命完全不一样。</p>
<p>从本质上讲，NAND Flash是一种不可靠介质，非常容易出现Bit翻转问题。SSD通过控制器和固件程序将这种不可靠的NAND Flash变成了可靠的数据存储介质。</p>
<p>为了在这种不可靠介质上构建可靠存储，SSD内部做了大量工作。在硬件层面，需要通过ECC单元解决经常出现的比特翻转问题。每次数据存储的时候，硬件单元需要为存储的数据计算ECC校验码；在数据读取的时候，硬件单元会根据校验码恢复被破坏的bit数据。ECC硬件单元集成在SSD控制器内部，代表了SSD控制器的能力。在MLC存储时代，BCH编解码技术可以解决问题，4KB数据中存在100bit翻转时可以纠正错误；在TLC存储时代，bit错误率大为提升，需要采用更高纠错能力的LDPC编解码技术，在4KB出现550bit翻转时，LDPC硬解码仍然可以恢复数据。对比LDPC硬解码、BCH以及LDPC软解码之间的能力，可以看出LDPC软解码具有更强的纠错能力，通常使用在硬解码失效的情况下。LDPC软解码的不足之处在于增加了IO的延迟。</p>
<p>在软件层面，SSD内部设计了FTL（Flash Translation Layer），该软件层的设计思想和Log-Structured File System设计思想类似。采用log追加写的方式记录数据，采用LBA至PBA的地址映射表记录数据组织方式。Log-structured系统最大的一个问题就是垃圾回收(GC)。因此，虽然NAND Flash本身具有很高的IO性能，但受限于GC的影响，SSD层面的性能会大受影响，并且存在十分严重的IO QoS问题，这也是目前标准NVMe SSD一个很重要的问题。</p>
<h3 id="耗损平衡-Wear-Leveling"><a href="#耗损平衡-Wear-Leveling" class="headerlink" title="耗损平衡 (Wear Leveling)"></a>耗损平衡 (Wear Leveling)</h3><p>对每一个块而言，一旦达到最大数量，该块就会死亡。对于 SLC 块，P/E 周期的典型数目是十万次；对于 MLC 块，P/E 周期的数目是一万；而对于 TLC 块，则可能是几千。为了确保 SSD 的容量和性能，我们需要在擦除次数上保持平衡，SSD 控制器具有这种“耗损平衡”机制可以实现这一目标。在损耗平衡期间，数据在各个块之间移动，以实现均衡的损耗，这种机制也会对前面讲的写入放大推波助澜。</p>
<h2 id="non-volatile-memory-NVM"><a href="#non-volatile-memory-NVM" class="headerlink" title="non-volatile memory (NVM)"></a><a href="https://nvmexpress.org/open-source-nvme-management-utility-nvme-command-line-interface-nvme-cli/?continueFlag=6093989111564a68c297d3f4bb8831b0" target="_blank" rel="external">non-volatile memory (NVM)</a></h2><p>NVM是一种新型的硬件存储介质，同时具备磁盘和DRAM的一些特性。突出的NVM技术产品有：PC-RAM、STT-RAM和R-RAM。因为NVM具有设备层次上的持久性，所以不需要向DRAM一样的刷新周期以维持数据状态。因此NVM和DRAM相比，每bit耗费的能量更少。另外，NVM比硬盘有更小的延迟，读延迟甚至和DRAM相当；字节寻址；比DRAM密度更大。</p>
<p><strong>1、NVM特性</strong></p>
<p><strong>数据访问延迟</strong>：NVM的读延迟比磁盘小很多。由于NVM仍处于开发阶段，来源不同延迟不同。STT-RAM的延迟1-20ns。尽管如此，他的延迟也已经非常接近DRAM了。</p>
<p>PC_RAM 和R-RAM的写延迟比DRAM高。但是写延迟不是很重要，因为可以通过buffer来缓解。</p>
<p><strong>密度</strong>：NVM的密度比DRAM高，可以作为主存的替代品，尤其是在嵌入式系统中。例如，相对于DRAM，PC-RAM提供2到4倍的容量，便于扩展。</p>
<p><strong>耐久性</strong>：即每个内存单元写的最大次数。最具竞争性的是PC-RAM和STT-RAM，提供接近DRAM的耐久性。更精确的说，NVM的耐久性是1015而DRAM是1016。另外，NVM比闪存技术的耐久性更大。</p>
<p><strong>能量消耗</strong>：NVM不需要像DRAM一样周期性刷写以维护内存中数据，所以消耗的能量更少。PC-RAM比DRAM消耗能量显著的少，其他比较接近。</p>
<p>此外，还有字节寻址、持久性。Interl和Micron已经发起了3D XPoint技术，同时Interl开发了新的指令以支持持久内存的使用。</p>
<h2 id="傲腾-PMem"><a href="#傲腾-PMem" class="headerlink" title="傲腾 PMem"></a>傲腾 PMem</h2><p>Optane 的工作原理与 NAND Flash 有很大区别，NAND Flash 是基于 <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Floating-gate_MOSFET" target="_blank" rel="external">Floating-gate MOSFET</a> 而 Optane 虽然因特尔没有官方资料说明但是根据国外机构用<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Mass_spectrometry" target="_blank" rel="external">质谱法</a> 测试的结果表明 Optane 是一种基于 <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Phase-change_memory" target="_blank" rel="external">PCM</a> (相变化存储器) 原理的存储器，PCM 简单来说就是某种物质目前主要是一种或多种硫族化物的玻璃，经由加热可以改变它的状态，成为晶体或者非晶体，这些不同的状态具有相应的电阻值。</p>
<p><img src="/images/951413iMgBlog/v2-a008e63dd545c8afe7d0f083e1d4b7f5_1440w.webp" alt="img"></p>
<p>从以上数据来看 PCM 读接近 DRAM，比 NAND Flash 快了500倍，PCM 写比 DRAM 慢了20倍，但是仍然比 NAND Flash 快了500倍。DRAM 读写速度一致，PCM 和 NAND Flash 写都比读要慢20倍。</p>
<h2 id="磁盘类型查看"><a href="#磁盘类型查看" class="headerlink" title="磁盘类型查看"></a>磁盘类型查看</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">$cat /sys/block/vda/queue/rotational</div><div class="line">1  //1表示旋转，非ssd，0表示ssd</div><div class="line">or</div><div class="line">lsblk -d -o name,rota,size,label,uuid</div><div class="line"></div><div class="line">SATA SSD测试数据</div><div class="line"># cat /sys/block/sda/queue/rotational </div><div class="line">0</div><div class="line"># lsblk -d -o name,rota</div><div class="line">NAME     ROTA</div><div class="line">sda         0</div><div class="line">sfdv0n1     0</div><div class="line"></div><div class="line">ESSD磁盘测试用一块虚拟的阿里云网络盘，不能算完整意义的SSD（承诺IOPS 4200），数据仅供参考，磁盘概况：</div><div class="line">$df -lh</div><div class="line">Filesystem      Size  Used Avail Use% Mounted on</div><div class="line">/dev/vda1        99G   30G   65G  32% /</div><div class="line"></div><div class="line">$cat /sys/block/vda/queue/rotational</div><div class="line">1</div></pre></td></tr></table></figure>
<h2 id="fio-结果解读"><a href="#fio-结果解读" class="headerlink" title="fio 结果解读"></a>fio 结果解读</h2><p>slat，异步场景下才有</p>
<blockquote>
<p>其中slat指的是发起IO的时间，在异步IO模式下，发起IO以后，IO会异步完成。例如调用一个异步的write，虽然write返回成功了，但是IO还未完成，slat约等于发起write的耗时；</p>
<p>slat (usec): min=4, max=6154, avg=48.82, stdev=56.38： The first latency metric you’ll see is the ‘slat’ or submission latency. It is pretty much what it sounds like, meaning “how long did it take to submit this IO to the kernel for processing?”</p>
</blockquote>
<p>clat</p>
<blockquote>
<p>clat指的是完成时间，从发起IO后到完成IO的时间，在同步IO模式下，clat是指整个写动作完成时间</p>
</blockquote>
<p>lat</p>
<blockquote>
<p>lat是总延迟时间，指的是IO单元创建到完成的总时间，通常这项数据关注较多。同步场景几乎等于clat，异步场景等于clat+slat<br>这项数据需要关注的是max，看看有没有极端的高延迟IO；另外还需要关注stdev，这项数据越大说明，IO响应时间波动越大，反之越小，波动越小</p>
</blockquote>
<p>clat percentiles (usec)：处于某个百分位的io操作时延</p>
<p>cpu          : usr=9.11%, sys=57.07%, ctx=762410, majf=0, minf=1769  //用户和系统的CPU占用时间百分比，线程切换次数，major以及minor页面错误的数量。</p>
<p>SSD的direct和buffered似乎很奇怪，应该是direct=0性能更好，实际不是这样，这里还需要找资料求证下</p>
<blockquote>
<ul>
<li><p><code>direct``=bool</code></p>
<p>If value is true, use non-buffered I/O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don’t support direct I/O. On Windows the synchronous ioengines don’t support direct I/O. Default: false.</p>
</li>
<li><p><code>buffered``=bool</code></p>
<p>If value is true, use buffered I/O. This is the opposite of the <a href="https://fio.readthedocs.io/en/latest/fio_man.html#cmdoption-arg-direct" target="_blank" rel="external"><code>direct</code></a> option. Defaults to true.</p>
</li>
</ul>
</blockquote>
<h2 id="iostat-结果解读"><a href="#iostat-结果解读" class="headerlink" title="iostat 结果解读"></a><a href="linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html">iostat 结果解读</a></h2><p>Dm-0就是lvm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           0.32    0.00    3.34    0.13    0.00   96.21</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda               0.00    11.40   66.00    7.20  1227.20    74.40    35.56     0.03    0.43    0.47    0.08   0.12   0.88</div><div class="line">nvme0n1           0.00  8612.00    0.00 51749.60     0.00 241463.20     9.33     4.51    0.09    0.00    0.09   0.02  78.56</div><div class="line">dm-0              0.00     0.00    0.00 60361.80     0.00 241463.20     8.00   152.52    2.53    0.00    2.53   0.01  78.26</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           0.36    0.00    3.46    0.17    0.00   96.00</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda               0.00     8.80    9.20    5.20  1047.20    67.20   154.78     0.01    0.36    0.46    0.19   0.33   0.48</div><div class="line">nvme0n1           0.00 11354.20    0.00 50876.80     0.00 248944.00     9.79     5.25    0.10    0.00    0.10   0.02  80.06</div><div class="line">dm-0              0.00     0.00    0.00 62231.00     0.00 248944.80     8.00   199.49    3.21    0.00    3.21   0.01  78.86</div></pre></td></tr></table></figure>
<p>avgqu_sz，是iostat的一项比较重要的数据。如果队列过长，则表示有大量IO在处理或等待，但是这还不足以说明后端的存储系统达到了处理极限。例如后端存储的并发能力是4096，客户端并发发送了256个IO下去，那么队列长度就是256。即使长时间队列长度是256，也不能说明什么，仅仅表明队列长度是256，有256个IO在处理或者排队。</p>
<p>那么怎么判断IO是在调度队列排队等待，还是在设备上处理呢？iostat有两项数据可以给出一个大致的判断。svctime，这项数据的指的是IO在设备处理中耗费的时间。另外一项数据await，指的是IO从排队到完成的时间，包括了svctime和排队等待的时间。那么通过对比这两项数据，如果两项数据差不多，则说明IO基本没有排队等待，耗费的时间都是设备处理。如果await远大于svctime，则说明有大量的IO在排队，并没有发送给设备处理。</p>
<h2 id="rq-affinity"><a href="#rq-affinity" class="headerlink" title="rq_affinity"></a>rq_affinity</h2><p>参考<a href="https://help.aliyun.com/knowledge_detail/65077.html#title-x10-2c0-yll" target="_blank" rel="external">aliyun测试文档</a> , rq_affinity增加2的commit： git show 5757a6d76c</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">function RunFio</div><div class="line">&#123;</div><div class="line"> numjobs=$1   # 实例中的测试线程数，例如示例中的10</div><div class="line"> iodepth=$2   # 同时发出I/O数的上限，例如示例中的64</div><div class="line"> bs=$3        # 单次I/O的块文件大小，例如示例中的4k</div><div class="line"> rw=$4        # 测试时的读写策略，例如示例中的randwrite</div><div class="line"> filename=$5  # 指定测试文件的名称，例如示例中的/dev/your_device</div><div class="line"> nr_cpus=`cat /proc/cpuinfo |grep &quot;processor&quot; |wc -l`</div><div class="line"> if [ $nr_cpus -lt $numjobs ];then</div><div class="line">     echo “Numjobs is more than cpu cores, exit!”</div><div class="line">     exit -1</div><div class="line"> fi</div><div class="line"> let nu=$numjobs+1</div><div class="line"> cpulist=&quot;&quot;</div><div class="line"> for ((i=1;i&lt;10;i++))</div><div class="line"> do</div><div class="line">     list=`cat /sys/block/your_device/mq/*/cpu_list | awk &apos;&#123;if(i&lt;=NF) print $i;&#125;&apos; i=&quot;$i&quot; | tr -d &apos;,&apos; | tr &apos;\n&apos; &apos;,&apos;`</div><div class="line">     if [ -z $list ];then</div><div class="line">         break</div><div class="line">     fi</div><div class="line">     cpulist=$&#123;cpulist&#125;$&#123;list&#125;</div><div class="line"> done</div><div class="line"> spincpu=`echo $cpulist | cut -d &apos;,&apos; -f 2-$&#123;nu&#125;`</div><div class="line"> echo $spincpu</div><div class="line"> fio --ioengine=libaio --runtime=30s --numjobs=$&#123;numjobs&#125; --iodepth=$&#123;iodepth&#125; --bs=$&#123;bs&#125; --rw=$&#123;rw&#125; --filename=$&#123;filename&#125; --time_based=1 --direct=1 --name=test --group_reporting --cpus_allowed=$spincpu --cpus_allowed_policy=split</div><div class="line">&#125;</div><div class="line">echo 2 &gt; /sys/block/your_device/queue/rq_affinity</div><div class="line">sleep 5</div><div class="line">RunFio 10 64 4k randwrite filename</div></pre></td></tr></table></figure>
<p>对NVME SSD进行测试，左边rq_affinity是2，右边rq_affinity为1，在这个测试参数下rq_affinity为1的性能要好(后许多次测试两者性能差不多)</p>
<p><img src="/images/951413iMgBlog/image-20210607113709945.png" alt="image-20210607113709945"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://cizixs.com/2017/01/03/how-slow-is-disk-and-network" target="_blank" rel="external">http://cizixs.com/2017/01/03/how-slow-is-disk-and-network</a></p>
<p><a href="https://tobert.github.io/post/2014-04-17-fio-output-explained.html" target="_blank" rel="external">https://tobert.github.io/post/2014-04-17-fio-output-explained.html</a> </p>
<p><a href="https://zhuanlan.zhihu.com/p/40497397" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/40497397</a></p>
<p><a href="https://www.atatech.org/articles/167736?spm=ata.home.0.0.11fd75362qwsg7&amp;flag_data_from=home_algorithm_article" target="_blank" rel="external">块存储NVMe云盘原型实践</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247483999&amp;idx=1&amp;sn=238d3d1a8cf24443db0da4aa00c9fb7e&amp;chksm=a6e3036491948a72704e0b114790483f227b7ce82f5eece5dd870ef88a8391a03eca27e8ff61&amp;scene=178&amp;cur_album_id=1371808335259090944#rd" target="_blank" rel="external">机械硬盘随机IO慢的超乎你的想象</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247484023&amp;idx=1&amp;sn=1946b4c286ed72da023b402cc30908b6&amp;chksm=a6e3034c91948a5aa3b0e6beb31c1d3804de9a11c668400d598c2a6b12462e179cf9f1dc33e2&amp;scene=178&amp;cur_album_id=1371808335259090944#rd" target="_blank" rel="external">搭载固态硬盘的服务器究竟比搭机械硬盘快多少？</a></p>
<p><a href="http://www.360doc.com/content/15/0318/15/16824943_456186965.shtml" target="_blank" rel="external">SSD基本工作原理</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/347599423" target="_blank" rel="external">SSD原理解读</a></p>
<p><a href="https://blog.gslin.org/archives/2022/02/02/10524/backblaze-%E7%9A%84-2021-%E5%B9%B4%E7%A1%AC%E7%A2%9F%E6%AD%BB%E4%BA%A1%E5%A0%B1%E5%91%8A/" target="_blank" rel="external">Backblaze 的 2021 年硬盘死亡報告</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/25/ssd san和sas 磁盘性能比较/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/25/ssd san和sas 磁盘性能比较/" itemprop="url">ssd/san/sas  磁盘 光纤性能比较</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-25T17:30:03+08:00">
                2020-01-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ssd-san-sas-磁盘-光纤性能比较"><a href="#ssd-san-sas-磁盘-光纤性能比较" class="headerlink" title="ssd/san/sas 磁盘 光纤性能比较"></a>ssd/san/sas 磁盘 光纤性能比较</h1><p>正好有机会用到一个san存储设备，跑了一把性能数据，记录一下</p>
<p><img src="/images/oss/d57a004c846e193126ca01398e394319.png" alt="image.png"></p>
<p>所使用的测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -size=1000G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div></pre></td></tr></table></figure>
<p>ssd（Solid State Drive）和san的比较是在同一台物理机上，所以排除了其他因素的干扰。</p>
<p>简要的结论： </p>
<ul>
<li><p>本地ssd性能最好、sas机械盘(RAID10)性能最差</p>
</li>
<li><p>san存储走特定的光纤网络，不是走tcp的san（至少从网卡看不到san的流量），性能居中</p>
</li>
<li><p>从rt来看 ssd:san:sas 大概是 1:3:15</p>
</li>
<li><p>san比本地sas机械盘性能要好，这也许取决于san的网络传输性能和san存储中的设备（比如用的ssd而不是机械盘）</p>
</li>
</ul>
<h2 id="NVMe-SSD-和-HDD的性能比较"><a href="#NVMe-SSD-和-HDD的性能比较" class="headerlink" title="NVMe SSD 和 HDD的性能比较"></a>NVMe SSD 和 HDD的性能比较</h2><p><img src="/images/oss/d64a0f78ebf471ac69d447ecb46d90f1.png" alt="image.png"></p>
<p>表中性能差异比上面测试还要大，SSD 的随机 IO 延迟比传统硬盘快百倍以上，一般在微妙级别；IO 带宽也高很多倍，可以达到每秒几个 GB；随机 IOPS 更是快了上千倍，可以达到几十万。</p>
<p><strong>HDD只有一个磁头，并发没有意义，但是SSD支持高并发写入读取。SSD没有磁头、不需要旋转，所以随机读取和顺序读取基本没有差别。</strong></p>
<h2 id="SSD-的性能特性和机制"><a href="#SSD-的性能特性和机制" class="headerlink" title="SSD 的性能特性和机制"></a>SSD 的性能特性和机制</h2><p>SSD 的内部工作方式和 HDD 大相径庭，我们先了解几个概念。</p>
<p><strong>单元（Cell）、页面（Page）、块（Block）</strong>。当今的主流 SSD 是基于 NAND 的，它将数字位存储在单元中。每个 SSD 单元可以存储一位或多位。对单元的每次擦除都会降低单元的寿命，所以单元只能承受一定数量的擦除。单元存储的位数越多，制造成本就越少，SSD 的容量也就越大，但是耐久性（擦除次数）也会降低。</p>
<p>一个页面包括很多单元，典型的页面大小是 4KB，页面也是要读写的最小存储单元。SSD 上没有“重写”操作，不像 HDD 可以直接对任何字节重写覆盖。一个页面一旦写入内容后就不能进行部分重写，必须和其它相邻页面一起被整体擦除重置。</p>
<p>多个页面组合成块。一个块的典型大小为 512KB 或 1MB，也就是大约 128 或 256 页。<strong>块是擦除的基本单位，每次擦除都是整个块内的所有页面都被重置。</strong></p>
<p><strong>擦除速度相对很慢，通常为几毫秒</strong>。所以对同步的 IO，发出 IO 的应用程序可能会因为块的擦除，而经历很大的写入延迟。为了尽量地减少这样的场景，保持空闲块的阈值对于快速的写响应是很有必要的。SSD 的垃圾回收（GC）的目的就在于此。GC 可以回收用过的块，这样可以确保以后的页写入可以快速分配到一个全新的页。</p>
<h3 id="SSD原理"><a href="#SSD原理" class="headerlink" title="SSD原理"></a>SSD原理</h3><p>对于 SSD 硬盘，类似SRAM（CPU cache）它是由一个电容加上一个电压计组合在一起，记录了一个或者多个比特。能够记录一个比特很容易理解。给电容里面充上电有电压的时候就是 1，给电容放电里面没有电就是 0。采用这样方式存储数据的 SSD 硬盘，我们一般称之为使用了 SLC 的颗粒，全称是 Single-Level Cell，也就是一个存储单元中只有一位数据。</p>
<p>但是，这样的方式会遇到和 CPU Cache 类似的问题，那就是，同样的面积下，能够存放下的元器件是有限的。如果只用 SLC，我们就会遇到，存储容量上不去，并且价格下不来的问题。于是呢，硬件工程师们就陆续发明了 MLC（Multi-Level Cell）、TLC（Triple-Level Cell）以及 QLC（Quad-Level Cell），也就是能在一个电容里面存下 2 个、3 个乃至 4 个比特。</p>
<p>只有一个电容，我们怎么能够表示更多的比特呢？别忘了，这里我们还有一个电压计。4 个比特一共可以从 0000-1111 表示 16 个不同的数。那么，如果我们能往电容里面充电的时候，充上 15 个不同的电压，并且我们电压计能够区分出这 15 个不同的电压。加上电容被放空代表的 0，就能够代表从 0000-1111 这样 4 个比特了。</p>
<p>不过，要想表示 15 个不同的电压，充电和读取的时候，对于精度的要求就会更高。这会导致充电和读取的时候都更慢，所以 QLC 的 SSD 的读写速度，要比 SLC 的慢上好几倍。</p>
<p>SSD对碎片很敏感，类似JVM的内存碎片需要整理，碎片整理就带来了写入放大。也就是写入空间不够的时候需要先进行碎片整理、搬运，这样写入的数据更大了。</p>
<p>SSD寿命：以Intel 335为例再来算一下，BT用户可以用600TB × 1024 / 843 = <strong>728天</strong>，普通用户可以用600TB/2 = <strong>300年</strong>！情况十分乐观</p>
<h4 id="两种逻辑门"><a href="#两种逻辑门" class="headerlink" title="两种逻辑门"></a>两种逻辑门</h4><p><strong>NAND</strong>(NOT-AND)  gate </p>
<p><strong>NOR</strong>(NOT-OR)  gate </p>
<p>如上两种门实现的介质<strong>都是非易失存储介质</strong>，<strong>在写入前都需要擦除</strong>。实际上NOR Flash的一个bit可以从1变成0，而要从0变1就要擦除整块。NAND flash都需要擦除。</p>
<table>
<thead>
<tr>
<th></th>
<th>NAND Flash</th>
<th>NOR Flash</th>
</tr>
</thead>
<tbody>
<tr>
<td>芯片容量</td>
<td>&lt;32GBit</td>
<td>&lt;1GBit</td>
</tr>
<tr>
<td>访问方式</td>
<td>块读写（顺序读写）</td>
<td>随机读写</td>
</tr>
<tr>
<td>接口方式</td>
<td>任意I/O口</td>
<td>特定完整存储器接口</td>
</tr>
<tr>
<td>读写性能</td>
<td>读取快（顺序读） 写入快 擦除快（可按块擦除）</td>
<td>读取快（RAM方式） 写入慢 檫除很慢</td>
</tr>
<tr>
<td>使用寿命</td>
<td>百万次</td>
<td>十万次</td>
</tr>
<tr>
<td>价格</td>
<td>低廉</td>
<td>高昂</td>
</tr>
</tbody>
</table>
<p>NAND Flash更适合在各类需要大数据的设备中使用，如U盘、SSD、各种存储卡、MP3播放器等，而NOR Flash更适合用在高性能的工业产品中。</p>
<p><a href="https://www.amc-systeme.de/files/pdf/wp_adv_flash_type_comparison_2016.pdf" target="_blank" rel="external">高端SSD会选取MLC</a>（Multi-Level Cell）甚至SLC（Single-Level Cell），低端SSD则选取 TLC（Triple-Level Cell）。SD卡一般选取 TLC（Triple-Level Cell）</p>
<p><img src="/images/951413iMgBlog/image-20210603161822079.png" alt="image-20210603161822079"></p>
<p><img src="/images/951413iMgBlog/slc-mlc-tlc-buckets.jpg" alt="slc-mlc-tlc-buckets"></p>
<p>NOR FLash主要用于：Bios、机顶盒，大小一般是1-32MB</p>
<h4 id="为什么断电后SSD不丢数据"><a href="#为什么断电后SSD不丢数据" class="headerlink" title="为什么断电后SSD不丢数据"></a>为什么断电后SSD不丢数据</h4><p>SSD的存储硬件都是NAND Flash。实现原理和通过改变电压，让电子进入绝缘层的浮栅(Floating Gate)内。断电之后，电子仍然在FG里面。但是如果长时间不通电，比如几年，仍然可能会丢数据。所以换句话说，SSD的确也不适合作为冷数据备份。</p>
<p>比如标准要求SSD：温度在30度的情况下，数据要能保持52周。</p>
<h3 id="写入放大（Write-Amplification-or-WA"><a href="#写入放大（Write-Amplification-or-WA" class="headerlink" title="写入放大（Write Amplification, or WA)"></a>写入放大（Write Amplification, or WA)</h3><p>这是 SSD 相对于 HDD 的一个缺点，即实际写入 SSD 的物理数据量，有可能是应用层写入数据量的多倍。一方面，页级别的写入需要移动已有的数据来腾空页面。另一方面，GC 的操作也会移动用户数据来进行块级别的擦除。所以对 SSD 真正的写操作的数据可能比实际写的数据量大，这就是写入放大。一块 SSD 只能进行有限的擦除次数，也称为编程 / 擦除（P/E）周期，所以写入放大效用会缩短 SSD 的寿命。</p>
<p>SSD 的读取和写入的基本单位，不是一个比特（bit）或者一个字节（byte），而是一个页（Page）。SSD 的擦除单位就更夸张了，我们不仅不能按照比特或者字节来擦除，连按照页来擦除都不行，我们必须按照块来擦除。</p>
<p>SLC 的芯片，可以擦除的次数大概在 10 万次，MLC 就在 1 万次左右，而 TLC 和 QLC 就只在几千次了。这也是为什么，你去购买 SSD 硬盘，会看到同样的容量的价格差别很大，因为它们的芯片颗粒和寿命完全不一样。</p>
<h3 id="耗损平衡-Wear-Leveling"><a href="#耗损平衡-Wear-Leveling" class="headerlink" title="耗损平衡 (Wear Leveling)"></a>耗损平衡 (Wear Leveling)</h3><p>对每一个块而言，一旦达到最大数量，该块就会死亡。对于 SLC 块，P/E 周期的典型数目是十万次；对于 MLC 块，P/E 周期的数目是一万；而对于 TLC 块，则可能是几千。为了确保 SSD 的容量和性能，我们需要在擦除次数上保持平衡，SSD 控制器具有这种“耗损平衡”机制可以实现这一目标。在损耗平衡期间，数据在各个块之间移动，以实现均衡的损耗，这种机制也会对前面讲的写入放大推波助澜。</p>
<h2 id="磁盘类型查看"><a href="#磁盘类型查看" class="headerlink" title="磁盘类型查看"></a>磁盘类型查看</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$cat /sys/block/vda/queue/rotational</div><div class="line">1  //1表示旋转，非ssd，0表示ssd</div><div class="line"></div><div class="line">或者</div><div class="line">lsblk -d -o name,rota</div></pre></td></tr></table></figure>
<h2 id="fio测试"><a href="#fio测试" class="headerlink" title="fio测试"></a>fio测试</h2><p>以下是两块测试的SSD磁盘测试前的基本情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">/dev/sda	240.06G  SSD_SATA  //sata</div><div class="line">/dev/sfd0n1	3200G	 SSD_PCIE  //PCIE</div><div class="line"></div><div class="line">Filesystem      Size  Used Avail Use% Mounted on</div><div class="line">/dev/sda3        49G   29G   18G  63% / </div><div class="line">/dev/sfdv0n1p1  2.0T  803G  1.3T  40% /data</div><div class="line"></div><div class="line"># cat /sys/block/sda/queue/rotational </div><div class="line">0</div><div class="line"># cat /sys/block/sfdv0n1/queue/rotational </div><div class="line">0</div><div class="line"></div><div class="line">#测试前的iostat状态</div><div class="line"># iostat -d sfdv0n1 sda3 1 -x</div><div class="line">Linux 3.10.0-957.el7.x86_64 (nu4d01142.sqa.nu8) 	2021年02月23日 	_x86_64_	(104 CPU)</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda3              0.00    10.67    1.24   18.78     7.82   220.69    22.83     0.03    1.64    1.39    1.66   0.08   0.17</div><div class="line">sfdv0n1           0.00     0.21    9.91  841.42   128.15  8237.10    19.65     0.93    0.04    0.25    0.04   1.05  89.52</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda3              0.00    15.00    0.00   17.00     0.00   136.00    16.00     0.03    2.00    0.00    2.00   1.29   2.20</div><div class="line">sfdv0n1           0.00     0.00    0.00 11158.00     0.00 54448.00     9.76     1.03    0.02    0.00    0.02   0.09 100.00</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda3              0.00     5.00    0.00   18.00     0.00   104.00    11.56     0.01    0.61    0.00    0.61   0.61   1.10</div><div class="line">sfdv0n1           0.00     0.00    0.00 10970.00     0.00 53216.00     9.70     1.02    0.03    0.00    0.03   0.09 100.10</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda3              0.00     0.00    0.00   24.00     0.00   100.00     8.33     0.01    0.58    0.00    0.58   0.08   0.20</div><div class="line">sfdv0n1           0.00     0.00    0.00 11206.00     0.00 54476.00     9.72     1.03    0.03    0.00    0.03   0.09  99.90</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sda3              0.00    14.00    0.00   21.00     0.00   148.00    14.10     0.01    0.48    0.00    0.48   0.33   0.70</div><div class="line">sfdv0n1           0.00     0.00    0.00 10071.00     0.00 49028.00     9.74     1.02    0.03    0.00    0.03   0.10  99.80</div></pre></td></tr></table></figure>
<h3 id="NVMe-SSD测试数据"><a href="#NVMe-SSD测试数据" class="headerlink" title="NVMe SSD测试数据"></a>NVMe SSD测试数据</h3><p>对一块ssd进行如下测试(挂载在/data 目录)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div><div class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.7</div><div class="line">Starting 1 thread</div><div class="line">EBS 4K randwrite test: Laying out IO file (1 file / 16384MiB)</div><div class="line">Jobs: 1 (f=1): [w(1)][100.0%][r=0KiB/s,w=63.8MiB/s][r=0,w=16.3k IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=258871: Tue Feb 23 14:12:23 2021</div><div class="line">  write: IOPS=18.9k, BW=74.0MiB/s (77.6MB/s)(4441MiB/60001msec)</div><div class="line">    slat (usec): min=4, max=6154, avg=48.82, stdev=56.38</div><div class="line">    clat (nsec): min=1049, max=12360k, avg=3326362.62, stdev=920683.43</div><div class="line">     lat (usec): min=68, max=12414, avg=3375.52, stdev=928.97</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[ 1483],  5.00th=[ 1811], 10.00th=[ 2114], 20.00th=[ 2376],</div><div class="line">     | 30.00th=[ 2704], 40.00th=[ 3130], 50.00th=[ 3523], 60.00th=[ 3785],</div><div class="line">     | 70.00th=[ 3949], 80.00th=[ 4080], 90.00th=[ 4293], 95.00th=[ 4490],</div><div class="line">     | 99.00th=[ 5604], 99.50th=[ 5997], 99.90th=[ 7111], 99.95th=[ 7832],</div><div class="line">     | 99.99th=[ 9634]</div><div class="line">   bw (  KiB/s): min=61024, max=118256, per=99.98%, avg=75779.58, stdev=12747.95, samples=120</div><div class="line">   iops        : min=15256, max=29564, avg=18944.88, stdev=3186.97, samples=120</div><div class="line">  lat (usec)   : 2=0.01%, 100=0.01%, 250=0.01%, 500=0.01%, 750=0.02%</div><div class="line">  lat (usec)   : 1000=0.06%</div><div class="line">  lat (msec)   : 2=7.40%, 4=66.19%, 10=26.32%, 20=0.01%</div><div class="line">  cpu          : usr=5.23%, sys=46.71%, ctx=846953, majf=0, minf=6</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwts: total=0,1136905,0,0 short=0,0,0,0 dropped=0,0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">  WRITE: bw=74.0MiB/s (77.6MB/s), 74.0MiB/s-74.0MiB/s (77.6MB/s-77.6MB/s), io=4441MiB (4657MB), run=60001-60001msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  sfdv0n1: ios=0/1821771, merge=0/7335, ticks=0/39708, in_queue=78295, util=100.00%</div></pre></td></tr></table></figure>
<p>slat (usec): min=4, max=6154, avg=48.82, stdev=56.38： The first latency metric you’ll see is the ‘slat’ or submission latency. It is pretty much what it sounds like, meaning “how long did it take to submit this IO to the kernel for processing?”</p>
<p>如上测试iops为：18944，测试期间的iostat，测试中一直有mysql在导入数据，所以测试开始前util就已经100%了，并且w/s到了13K左右</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"># iostat -d sfdv0n1 3 -x</div><div class="line">Linux 3.10.0-957.el7.x86_64 (nu4d01142.sqa.nu8) 	2021年02月23日 	_x86_64_	(104 CPU)</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     0.18    3.45  769.17   102.83  7885.16    20.68     0.93    0.04    0.26    0.04   1.16  89.46</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     0.00    0.00 13168.67     0.00 66244.00    10.06     1.05    0.03    0.00    0.03   0.08 100.10</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     0.00    0.00 12822.67     0.00 65542.67    10.22     1.04    0.02    0.00    0.02   0.08 100.07</div><div class="line"></div><div class="line">//增加压力</div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     0.00    0.00 27348.33     0.00 214928.00    15.72     1.27    0.02    0.00    0.02   0.04 100.17</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     1.00    0.00 32661.67     0.00 271660.00    16.63     1.32    0.02    0.00    0.02   0.03 100.37</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     0.00    0.00 31645.00     0.00 265988.00    16.81     1.33    0.02    0.00    0.02   0.03 100.37</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00   574.00    0.00 31961.67     0.00 271094.67    16.96     1.36    0.02    0.00    0.02   0.03 100.13</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">sfdv0n1           0.00     0.00    0.00 27656.33     0.00 224586.67    16.24     1.28    0.02    0.00    0.02   0.04 100.37</div></pre></td></tr></table></figure>
<p>从iostat看出，测试开始前util已经100%（因为ssd，util失去参考意义），w/s 13K左右，压力跑起来后w/s能到30K，svctm、await均保持稳定</p>
<p>SSD的direct和buffered似乎很奇怪，应该是direct=0性能更好，实际不是这样，这里还需要找资料求证下</p>
<blockquote>
<ul>
<li><p><code>direct``=bool</code></p>
<p>If value is true, use non-buffered I/O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don’t support direct I/O. On Windows the synchronous ioengines don’t support direct I/O. Default: false.</p>
</li>
<li><p><code>buffered``=bool</code></p>
<p>If value is true, use buffered I/O. This is the opposite of the <a href="https://fio.readthedocs.io/en/latest/fio_man.html#cmdoption-arg-direct" target="_blank" rel="external"><code>direct</code></a> option. Defaults to true.</p>
</li>
</ul>
</blockquote>
<p>如下测试中direct=1和direct=0的write avg iops分别为42K、16K</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line"># fio -ioengine=libaio -bs=4k -direct=1 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=16G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.7</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=507MiB/s,w=216MiB/s][r=130k,w=55.2k IOPS][eta 00m:00s] </div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=415921: Tue Feb 23 14:34:33 2021</div><div class="line">   read: IOPS=99.8k, BW=390MiB/s (409MB/s)(11.2GiB/29432msec)</div><div class="line">    slat (nsec): min=1043, max=917837, avg=4273.86, stdev=3792.17</div><div class="line">    clat (usec): min=2, max=4313, avg=459.80, stdev=239.61</div><div class="line">     lat (usec): min=4, max=4328, avg=464.16, stdev=241.81</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  251],  5.00th=[  277], 10.00th=[  289], 20.00th=[  310],</div><div class="line">     | 30.00th=[  326], 40.00th=[  343], 50.00th=[  363], 60.00th=[  400],</div><div class="line">     | 70.00th=[  502], 80.00th=[  603], 90.00th=[  750], 95.00th=[  881],</div><div class="line">     | 99.00th=[ 1172], 99.50th=[ 1401], 99.90th=[ 3032], 99.95th=[ 3359],</div><div class="line">     | 99.99th=[ 3785]</div><div class="line">   bw (  KiB/s): min=182520, max=574856, per=99.24%, avg=395975.64, stdev=119541.78, samples=58</div><div class="line">   iops        : min=45630, max=143714, avg=98993.90, stdev=29885.42, samples=58</div><div class="line">  write: IOPS=42.8k, BW=167MiB/s (175MB/s)(4915MiB/29432msec)</div><div class="line">    slat (usec): min=3, max=263, avg= 9.34, stdev= 4.35</div><div class="line">    clat (usec): min=14, max=2057, avg=402.26, stdev=140.67</div><div class="line">     lat (usec): min=19, max=2070, avg=411.72, stdev=142.67</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  237],  5.00th=[  281], 10.00th=[  293], 20.00th=[  314],</div><div class="line">     | 30.00th=[  330], 40.00th=[  343], 50.00th=[  359], 60.00th=[  379],</div><div class="line">     | 70.00th=[  404], 80.00th=[  457], 90.00th=[  586], 95.00th=[  717],</div><div class="line">     | 99.00th=[  930], 99.50th=[ 1004], 99.90th=[ 1254], 99.95th=[ 1385],</div><div class="line">     | 99.99th=[ 1532]</div><div class="line">   bw (  KiB/s): min=78104, max=244408, per=99.22%, avg=169671.52, stdev=51142.10, samples=58</div><div class="line">   iops        : min=19526, max=61102, avg=42417.86, stdev=12785.51, samples=58</div><div class="line">  lat (usec)   : 4=0.01%, 10=0.01%, 20=0.01%, 50=0.02%, 100=0.04%</div><div class="line">  lat (usec)   : 250=1.02%, 500=73.32%, 750=17.28%, 1000=6.30%</div><div class="line">  lat (msec)   : 2=1.83%, 4=0.19%, 10=0.01%</div><div class="line">  cpu          : usr=15.84%, sys=83.31%, ctx=13765, majf=0, minf=7</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwts: total=2936000,1258304,0,0 short=0,0,0,0 dropped=0,0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=390MiB/s (409MB/s), 390MiB/s-390MiB/s (409MB/s-409MB/s), io=11.2GiB (12.0GB), run=29432-29432msec</div><div class="line">  WRITE: bw=167MiB/s (175MB/s), 167MiB/s-167MiB/s (175MB/s-175MB/s), io=4915MiB (5154MB), run=29432-29432msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  sfdv0n1: ios=795793/1618341, merge=0/11, ticks=218710/27721, in_queue=264935, util=100.00%</div><div class="line">[root@nu4d01142 data]# </div><div class="line">[root@nu4d01142 data]# fio -ioengine=libaio -bs=4k -direct=0 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=6G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.7</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=124MiB/s,w=53.5MiB/s][r=31.7k,w=13.7k IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=437523: Tue Feb 23 14:37:54 2021</div><div class="line">   read: IOPS=38.6k, BW=151MiB/s (158MB/s)(4300MiB/28550msec)</div><div class="line">    slat (nsec): min=1205, max=1826.7k, avg=13253.36, stdev=17173.87</div><div class="line">    clat (nsec): min=236, max=5816.8k, avg=1135969.25, stdev=337142.34</div><div class="line">     lat (nsec): min=1977, max=5831.2k, avg=1149404.84, stdev=341232.87</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  461],  5.00th=[  627], 10.00th=[  717], 20.00th=[  840],</div><div class="line">     | 30.00th=[  938], 40.00th=[ 1029], 50.00th=[ 1123], 60.00th=[ 1221],</div><div class="line">     | 70.00th=[ 1319], 80.00th=[ 1434], 90.00th=[ 1565], 95.00th=[ 1680],</div><div class="line">     | 99.00th=[ 1893], 99.50th=[ 1975], 99.90th=[ 2671], 99.95th=[ 3261],</div><div class="line">     | 99.99th=[ 3851]</div><div class="line">   bw (  KiB/s): min=119304, max=216648, per=100.00%, avg=154273.07, stdev=29925.10, samples=57</div><div class="line">   iops        : min=29826, max=54162, avg=38568.25, stdev=7481.30, samples=57</div><div class="line">  write: IOPS=16.5k, BW=64.6MiB/s (67.7MB/s)(1844MiB/28550msec)</div><div class="line">    slat (usec): min=3, max=3565, avg=21.07, stdev=22.23</div><div class="line">    clat (usec): min=14, max=9983, avg=1164.21, stdev=459.66</div><div class="line">     lat (usec): min=21, max=10011, avg=1185.57, stdev=463.28</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  498],  5.00th=[  619], 10.00th=[  709], 20.00th=[  832],</div><div class="line">     | 30.00th=[  930], 40.00th=[ 1020], 50.00th=[ 1123], 60.00th=[ 1237],</div><div class="line">     | 70.00th=[ 1336], 80.00th=[ 1450], 90.00th=[ 1598], 95.00th=[ 1713],</div><div class="line">     | 99.00th=[ 2311], 99.50th=[ 3851], 99.90th=[ 5932], 99.95th=[ 6456],</div><div class="line">     | 99.99th=[ 7701]</div><div class="line">   bw (  KiB/s): min=50800, max=92328, per=100.00%, avg=66128.47, stdev=12890.64, samples=57</div><div class="line">   iops        : min=12700, max=23082, avg=16532.07, stdev=3222.66, samples=57</div><div class="line">  lat (nsec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%</div><div class="line">  lat (usec)   : 2=0.01%, 4=0.01%, 10=0.01%, 20=0.02%, 50=0.03%</div><div class="line">  lat (usec)   : 100=0.04%, 250=0.18%, 500=1.01%, 750=11.05%, 1000=25.02%</div><div class="line">  lat (msec)   : 2=61.87%, 4=0.62%, 10=0.14%</div><div class="line">  cpu          : usr=10.87%, sys=61.98%, ctx=218415, majf=0, minf=7</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwts: total=1100924,471940,0,0 short=0,0,0,0 dropped=0,0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=151MiB/s (158MB/s), 151MiB/s-151MiB/s (158MB/s-158MB/s), io=4300MiB (4509MB), run=28550-28550msec</div><div class="line">  WRITE: bw=64.6MiB/s (67.7MB/s), 64.6MiB/s-64.6MiB/s (67.7MB/s-67.7MB/s), io=1844MiB (1933MB), run=28550-28550msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  sfdv0n1: ios=536103/822037, merge=0/1442, ticks=66507/17141, in_queue=99429, util=100.00%</div></pre></td></tr></table></figure>
<h3 id="SATA-SSD测试数据"><a href="#SATA-SSD测试数据" class="headerlink" title="SATA SSD测试数据"></a>SATA SSD测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># cat /sys/block/sda/queue/rotational </div><div class="line">0</div><div class="line"># lsblk -d -o name,rota</div><div class="line">NAME     ROTA</div><div class="line">sda         0</div><div class="line">sfdv0n1     0</div></pre></td></tr></table></figure>
<p>-direct=0 -buffered=0读写iops分别为15.8K、6.8K 比ssd差了不少（都是direct=0），如果direct、buffered都是1的话，ESSD性能很差，读写iops分别为4312、1852</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line"># fio -ioengine=libaio -bs=4k -direct=0 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=2G -filename=/var/lib/docker/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.7</div><div class="line">Starting 1 thread</div><div class="line">EBS 4K randwrite test: Laying out IO file (1 file / 2048MiB)</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=68.7MiB/s,w=29.7MiB/s][r=17.6k,w=7594 IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=13261: Tue Feb 23 14:42:41 2021</div><div class="line">   read: IOPS=15.8k, BW=61.8MiB/s (64.8MB/s)(1432MiB/23172msec)</div><div class="line">    slat (nsec): min=1266, max=7261.0k, avg=7101.88, stdev=20655.54</div><div class="line">    clat (usec): min=167, max=27670, avg=2832.68, stdev=1786.18</div><div class="line">     lat (usec): min=175, max=27674, avg=2839.93, stdev=1784.42</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  437],  5.00th=[  668], 10.00th=[  873], 20.00th=[  988],</div><div class="line">     | 30.00th=[ 1401], 40.00th=[ 2442], 50.00th=[ 2835], 60.00th=[ 3195],</div><div class="line">     | 70.00th=[ 3523], 80.00th=[ 4047], 90.00th=[ 5014], 95.00th=[ 5866],</div><div class="line">     | 99.00th=[ 8160], 99.50th=[ 9372], 99.90th=[13829], 99.95th=[15008],</div><div class="line">     | 99.99th=[23725]</div><div class="line">   bw (  KiB/s): min=44183, max=149440, per=99.28%, avg=62836.17, stdev=26590.84, samples=46</div><div class="line">   iops        : min=11045, max=37360, avg=15709.02, stdev=6647.72, samples=46</div><div class="line">  write: IOPS=6803, BW=26.6MiB/s (27.9MB/s)(616MiB/23172msec)</div><div class="line">    slat (nsec): min=1566, max=11474k, avg=8460.17, stdev=38221.51</div><div class="line">    clat (usec): min=77, max=24047, avg=2789.68, stdev=2042.55</div><div class="line">     lat (usec): min=80, max=24054, avg=2798.29, stdev=2040.85</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  265],  5.00th=[  433], 10.00th=[  635], 20.00th=[  840],</div><div class="line">     | 30.00th=[  979], 40.00th=[ 2212], 50.00th=[ 2671], 60.00th=[ 3130],</div><div class="line">     | 70.00th=[ 3523], 80.00th=[ 4228], 90.00th=[ 5342], 95.00th=[ 6456],</div><div class="line">     | 99.00th=[ 9241], 99.50th=[10421], 99.90th=[13960], 99.95th=[15533],</div><div class="line">     | 99.99th=[23725]</div><div class="line">   bw (  KiB/s): min=18435, max=63112, per=99.26%, avg=27012.57, stdev=11299.42, samples=46</div><div class="line">   iops        : min= 4608, max=15778, avg=6753.11, stdev=2824.87, samples=46</div><div class="line">  lat (usec)   : 100=0.01%, 250=0.23%, 500=3.14%, 750=5.46%, 1000=15.27%</div><div class="line">  lat (msec)   : 2=11.47%, 4=43.09%, 10=20.88%, 20=0.44%, 50=0.01%</div><div class="line">  cpu          : usr=3.53%, sys=18.08%, ctx=47448, majf=0, minf=6</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwts: total=366638,157650,0,0 short=0,0,0,0 dropped=0,0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=61.8MiB/s (64.8MB/s), 61.8MiB/s-61.8MiB/s (64.8MB/s-64.8MB/s), io=1432MiB (1502MB), run=23172-23172msec</div><div class="line">  WRITE: bw=26.6MiB/s (27.9MB/s), 26.6MiB/s-26.6MiB/s (27.9MB/s-27.9MB/s), io=616MiB (646MB), run=23172-23172msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  sda: ios=359202/155123, merge=299/377, ticks=946305/407820, in_queue=1354596, util=99.61%</div><div class="line">  </div><div class="line"># fio -ioengine=libaio -bs=4k -direct=1 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=2G -filename=/var/lib/docker/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.7</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][95.5%][r=57.8MiB/s,w=25.7MiB/s][r=14.8k,w=6568 IOPS][eta 00m:01s] </div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=26167: Tue Feb 23 14:44:40 2021</div><div class="line">   read: IOPS=16.9k, BW=65.9MiB/s (69.1MB/s)(1432MiB/21730msec)</div><div class="line">    slat (nsec): min=1312, max=4454.2k, avg=8489.99, stdev=15763.97</div><div class="line">    clat (usec): min=201, max=18856, avg=2679.38, stdev=1720.02</div><div class="line">     lat (usec): min=206, max=18860, avg=2688.03, stdev=1717.19</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  635],  5.00th=[  832], 10.00th=[  914], 20.00th=[  971],</div><div class="line">     | 30.00th=[ 1090], 40.00th=[ 2114], 50.00th=[ 2704], 60.00th=[ 3064],</div><div class="line">     | 70.00th=[ 3392], 80.00th=[ 3851], 90.00th=[ 4817], 95.00th=[ 5735],</div><div class="line">     | 99.00th=[ 7767], 99.50th=[ 8979], 99.90th=[13698], 99.95th=[15139],</div><div class="line">     | 99.99th=[16581]</div><div class="line">   bw (  KiB/s): min=45168, max=127528, per=100.00%, avg=67625.19, stdev=26620.82, samples=43</div><div class="line">   iops        : min=11292, max=31882, avg=16906.28, stdev=6655.20, samples=43</div><div class="line">  write: IOPS=7254, BW=28.3MiB/s (29.7MB/s)(616MiB/21730msec)</div><div class="line">    slat (nsec): min=1749, max=3412.2k, avg=9816.22, stdev=14501.05</div><div class="line">    clat (usec): min=97, max=23473, avg=2556.02, stdev=1980.53</div><div class="line">     lat (usec): min=107, max=23477, avg=2566.01, stdev=1977.65</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[  277],  5.00th=[  486], 10.00th=[  693], 20.00th=[  824],</div><div class="line">     | 30.00th=[  881], 40.00th=[ 1205], 50.00th=[ 2442], 60.00th=[ 2868],</div><div class="line">     | 70.00th=[ 3326], 80.00th=[ 3949], 90.00th=[ 5080], 95.00th=[ 6128],</div><div class="line">     | 99.00th=[ 8717], 99.50th=[10159], 99.90th=[14484], 99.95th=[15926],</div><div class="line">     | 99.99th=[18744]</div><div class="line">   bw (  KiB/s): min=19360, max=55040, per=100.00%, avg=29064.05, stdev=11373.59, samples=43</div><div class="line">   iops        : min= 4840, max=13760, avg=7266.00, stdev=2843.41, samples=43</div><div class="line">  lat (usec)   : 100=0.01%, 250=0.17%, 500=1.66%, 750=3.74%, 1000=22.57%</div><div class="line">  lat (msec)   : 2=12.66%, 4=40.62%, 10=18.20%, 20=0.38%, 50=0.01%</div><div class="line">  cpu          : usr=4.17%, sys=22.27%, ctx=14314, majf=0, minf=7</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwts: total=366638,157650,0,0 short=0,0,0,0 dropped=0,0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=65.9MiB/s (69.1MB/s), 65.9MiB/s-65.9MiB/s (69.1MB/s-69.1MB/s), io=1432MiB (1502MB), run=21730-21730msec</div><div class="line">  WRITE: bw=28.3MiB/s (29.7MB/s), 28.3MiB/s-28.3MiB/s (29.7MB/s-29.7MB/s), io=616MiB (646MB), run=21730-21730msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  sda: ios=364744/157621, merge=779/473, ticks=851759/352008, in_queue=1204024, util=99.61%</div><div class="line"></div><div class="line"># fio -ioengine=libaio -bs=4k -direct=1 -buffered=1 -thread -rw=randrw -rwmixread=70 -size=2G -filename=/var/lib/docker/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.7</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=15.9MiB/s,w=7308KiB/s][r=4081,w=1827 IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=31560: Tue Feb 23 14:46:10 2021</div><div class="line">   read: IOPS=4312, BW=16.8MiB/s (17.7MB/s)(1011MiB/60001msec)</div><div class="line">    slat (usec): min=63, max=14320, avg=216.76, stdev=430.61</div><div class="line">    clat (usec): min=5, max=778861, avg=10254.92, stdev=22345.40</div><div class="line">     lat (usec): min=1900, max=782277, avg=10472.16, stdev=22657.06</div><div class="line">    clat percentiles (msec):</div><div class="line">     |  1.00th=[    6],  5.00th=[    6], 10.00th=[    6], 20.00th=[    7],</div><div class="line">     | 30.00th=[    7], 40.00th=[    7], 50.00th=[    7], 60.00th=[    7],</div><div class="line">     | 70.00th=[    8], 80.00th=[    8], 90.00th=[    8], 95.00th=[   11],</div><div class="line">     | 99.00th=[  107], 99.50th=[  113], 99.90th=[  132], 99.95th=[  197],</div><div class="line">     | 99.99th=[  760]</div><div class="line">   bw (  KiB/s): min=  168, max=29784, per=100.00%, avg=17390.92, stdev=10932.90, samples=119</div><div class="line">   iops        : min=   42, max= 7446, avg=4347.71, stdev=2733.21, samples=119</div><div class="line">  write: IOPS=1852, BW=7410KiB/s (7588kB/s)(434MiB/60001msec)</div><div class="line">    slat (usec): min=3, max=666432, avg=23.59, stdev=2745.39</div><div class="line">    clat (msec): min=3, max=781, avg=10.14, stdev=20.50</div><div class="line">     lat (msec): min=3, max=781, avg=10.16, stdev=20.72</div><div class="line">    clat percentiles (msec):</div><div class="line">     |  1.00th=[    6],  5.00th=[    6], 10.00th=[    6], 20.00th=[    7],</div><div class="line">     | 30.00th=[    7], 40.00th=[    7], 50.00th=[    7], 60.00th=[    7],</div><div class="line">     | 70.00th=[    7], 80.00th=[    8], 90.00th=[    8], 95.00th=[   11],</div><div class="line">     | 99.00th=[  107], 99.50th=[  113], 99.90th=[  131], 99.95th=[  157],</div><div class="line">     | 99.99th=[  760]</div><div class="line">   bw (  KiB/s): min=   80, max=12328, per=100.00%, avg=7469.53, stdev=4696.69, samples=119</div><div class="line">   iops        : min=   20, max= 3082, avg=1867.34, stdev=1174.19, samples=119</div><div class="line">  lat (usec)   : 10=0.01%</div><div class="line">  lat (msec)   : 2=0.01%, 4=0.01%, 10=94.64%, 20=1.78%, 50=0.11%</div><div class="line">  lat (msec)   : 100=1.80%, 250=1.63%, 500=0.01%, 750=0.02%, 1000=0.01%</div><div class="line">  cpu          : usr=2.51%, sys=10.98%, ctx=260210, majf=0, minf=7</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwts: total=258768,111147,0,0 short=0,0,0,0 dropped=0,0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=16.8MiB/s (17.7MB/s), 16.8MiB/s-16.8MiB/s (17.7MB/s-17.7MB/s), io=1011MiB (1060MB), run=60001-60001msec</div><div class="line">  WRITE: bw=7410KiB/s (7588kB/s), 7410KiB/s-7410KiB/s (7588kB/s-7588kB/s), io=434MiB (455MB), run=60001-60001msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  sda: ios=258717/89376, merge=0/735, ticks=52540/564186, in_queue=616999, util=90.07%</div></pre></td></tr></table></figure>
<h3 id="ESSD磁盘测试数据"><a href="#ESSD磁盘测试数据" class="headerlink" title="ESSD磁盘测试数据"></a>ESSD磁盘测试数据</h3><p>这是一块虚拟的阿里云网络盘，不能算完整意义的SSD（承诺IOPS 4200），数据仅供参考，磁盘概况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$df -lh</div><div class="line">Filesystem      Size  Used Avail Use% Mounted on</div><div class="line">/dev/vda1        99G   30G   65G  32% /</div><div class="line"></div><div class="line">$cat /sys/block/vda/queue/rotational</div><div class="line">1</div></pre></td></tr></table></figure>
<p>测试数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div></pre></td><td class="code"><pre><div class="line">$fio -ioengine=libaio -bs=4k -direct=1 -buffered=1  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.1</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=10.8MiB/s,w=11.2MiB/s][r=2757,w=2876 IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=25641: Tue Feb 23 16:35:19 2021</div><div class="line">   read: IOPS=2136, BW=8545KiB/s (8750kB/s)(501MiB/60001msec)</div><div class="line">    slat (usec): min=190, max=830992, avg=457.20, stdev=3088.80</div><div class="line">    clat (nsec): min=1792, max=1721.3M, avg=14657528.60, stdev=63188988.75</div><div class="line">     lat (usec): min=344, max=1751.1k, avg=15115.20, stdev=65165.80</div><div class="line">    clat percentiles (msec):</div><div class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</div><div class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</div><div class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</div><div class="line">     | 99.00th=[   17], 99.50th=[   53], 99.90th=[ 1028], 99.95th=[ 1167],</div><div class="line">     | 99.99th=[ 1653]</div><div class="line">   bw (  KiB/s): min=   56, max=12648, per=100.00%, avg=8598.92, stdev=5289.40, samples=118</div><div class="line">   iops        : min=   14, max= 3162, avg=2149.73, stdev=1322.35, samples=118</div><div class="line">  write: IOPS=2137, BW=8548KiB/s (8753kB/s)(501MiB/60001msec)</div><div class="line">    slat (usec): min=2, max=181, avg= 6.67, stdev= 7.22</div><div class="line">    clat (usec): min=628, max=1721.1k, avg=14825.32, stdev=65017.66</div><div class="line">     lat (usec): min=636, max=1721.1k, avg=14832.10, stdev=65018.10</div><div class="line">    clat percentiles (msec):</div><div class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</div><div class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</div><div class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</div><div class="line">     | 99.00th=[   17], 99.50th=[   53], 99.90th=[ 1045], 99.95th=[ 1200],</div><div class="line">     | 99.99th=[ 1687]</div><div class="line">   bw (  KiB/s): min=   72, max=13304, per=100.00%, avg=8602.99, stdev=5296.31, samples=118</div><div class="line">   iops        : min=   18, max= 3326, avg=2150.75, stdev=1324.08, samples=118</div><div class="line">  lat (usec)   : 2=0.01%, 500=0.01%, 750=0.01%</div><div class="line">  lat (msec)   : 2=0.01%, 4=0.01%, 10=37.85%, 20=61.53%, 50=0.10%</div><div class="line">  lat (msec)   : 100=0.06%, 250=0.03%, 500=0.01%, 750=0.03%, 1000=0.25%</div><div class="line">  lat (msec)   : 2000=0.14%</div><div class="line">  cpu          : usr=0.70%, sys=4.01%, ctx=135029, majf=0, minf=4</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwt: total=128180,128223,0, short=0,0,0, dropped=0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=8545KiB/s (8750kB/s), 8545KiB/s-8545KiB/s (8750kB/s-8750kB/s), io=501MiB (525MB), run=60001-60001msec</div><div class="line">  WRITE: bw=8548KiB/s (8753kB/s), 8548KiB/s-8548KiB/s (8753kB/s-8753kB/s), io=501MiB (525MB), run=60001-60001msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  vda: ios=127922/87337, merge=0/237, ticks=55122/4269885, in_queue=2209125, util=94.29%</div><div class="line"></div><div class="line">$fio -ioengine=libaio -bs=4k -direct=1 -buffered=0  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.1</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=9680KiB/s,w=9712KiB/s][r=2420,w=2428 IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=25375: Tue Feb 23 16:33:03 2021</div><div class="line">   read: IOPS=2462, BW=9849KiB/s (10.1MB/s)(577MiB/60011msec)</div><div class="line">    slat (nsec): min=1558, max=10663k, avg=5900.28, stdev=46286.64</div><div class="line">    clat (usec): min=290, max=93493, avg=13054.57, stdev=4301.89</div><div class="line">     lat (usec): min=332, max=93497, avg=13060.60, stdev=4301.68</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[ 1844],  5.00th=[10159], 10.00th=[10290], 20.00th=[10421],</div><div class="line">     | 30.00th=[10552], 40.00th=[10552], 50.00th=[10683], 60.00th=[10814],</div><div class="line">     | 70.00th=[18482], 80.00th=[19006], 90.00th=[19006], 95.00th=[19268],</div><div class="line">     | 99.00th=[19530], 99.50th=[19792], 99.90th=[29492], 99.95th=[30278],</div><div class="line">     | 99.99th=[43779]</div><div class="line">   bw (  KiB/s): min= 9128, max=30392, per=100.00%, avg=9850.12, stdev=1902.00, samples=120</div><div class="line">   iops        : min= 2282, max= 7598, avg=2462.52, stdev=475.50, samples=120</div><div class="line">  write: IOPS=2465, BW=9864KiB/s (10.1MB/s)(578MiB/60011msec)</div><div class="line">    slat (usec): min=2, max=10586, avg= 6.92, stdev=67.34</div><div class="line">    clat (usec): min=240, max=69922, avg=12902.33, stdev=4307.92</div><div class="line">     lat (usec): min=244, max=69927, avg=12909.37, stdev=4307.03</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[ 1729],  5.00th=[10159], 10.00th=[10290], 20.00th=[10290],</div><div class="line">     | 30.00th=[10421], 40.00th=[10421], 50.00th=[10552], 60.00th=[10683],</div><div class="line">     | 70.00th=[18220], 80.00th=[18744], 90.00th=[19006], 95.00th=[19006],</div><div class="line">     | 99.00th=[19268], 99.50th=[19530], 99.90th=[21103], 99.95th=[35390],</div><div class="line">     | 99.99th=[50594]</div><div class="line">   bw (  KiB/s): min= 8496, max=31352, per=100.00%, avg=9862.92, stdev=1991.48, samples=120</div><div class="line">   iops        : min= 2124, max= 7838, avg=2465.72, stdev=497.87, samples=120</div><div class="line">  lat (usec)   : 250=0.01%, 500=0.03%, 750=0.02%, 1000=0.02%</div><div class="line">  lat (msec)   : 2=1.70%, 4=0.41%, 10=1.25%, 20=96.22%, 50=0.34%</div><div class="line">  lat (msec)   : 100=0.01%</div><div class="line">  cpu          : usr=0.89%, sys=4.09%, ctx=206337, majf=0, minf=4</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwt: total=147768,147981,0, short=0,0,0, dropped=0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=9849KiB/s (10.1MB/s), 9849KiB/s-9849KiB/s (10.1MB/s-10.1MB/s), io=577MiB (605MB), run=60011-60011msec</div><div class="line">  WRITE: bw=9864KiB/s (10.1MB/s), 9864KiB/s-9864KiB/s (10.1MB/s-10.1MB/s), io=578MiB (606MB), run=60011-60011msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  vda: ios=147515/148154, merge=0/231, ticks=1922378/1915751, in_queue=3780605, util=98.46%</div><div class="line">  </div><div class="line">$fio -ioengine=libaio -bs=4k -direct=0 -buffered=1  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.1</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=132KiB/s,w=148KiB/s][r=33,w=37 IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=25892: Tue Feb 23 16:37:41 2021</div><div class="line">   read: IOPS=1987, BW=7949KiB/s (8140kB/s)(467MiB/60150msec)</div><div class="line">    slat (usec): min=192, max=599873, avg=479.26, stdev=2917.52</div><div class="line">    clat (usec): min=15, max=1975.6k, avg=16004.22, stdev=76024.60</div><div class="line">     lat (msec): min=5, max=2005, avg=16.48, stdev=78.00</div><div class="line">    clat percentiles (msec):</div><div class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</div><div class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</div><div class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</div><div class="line">     | 99.00th=[   19], 99.50th=[  317], 99.90th=[ 1133], 99.95th=[ 1435],</div><div class="line">     | 99.99th=[ 1871]</div><div class="line">   bw (  KiB/s): min=   32, max=12672, per=100.00%, avg=8034.08, stdev=5399.63, samples=119</div><div class="line">   iops        : min=    8, max= 3168, avg=2008.52, stdev=1349.91, samples=119</div><div class="line">  write: IOPS=1984, BW=7937KiB/s (8127kB/s)(466MiB/60150msec)</div><div class="line">    slat (usec): min=2, max=839634, avg=18.39, stdev=2747.10</div><div class="line">    clat (msec): min=5, max=1975, avg=15.64, stdev=73.06</div><div class="line">     lat (msec): min=5, max=1975, avg=15.66, stdev=73.28</div><div class="line">    clat percentiles (msec):</div><div class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</div><div class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</div><div class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</div><div class="line">     | 99.00th=[   18], 99.50th=[  153], 99.90th=[ 1116], 99.95th=[ 1435],</div><div class="line">     | 99.99th=[ 1921]</div><div class="line">   bw (  KiB/s): min=   24, max=13160, per=100.00%, avg=8021.18, stdev=5405.12, samples=119</div><div class="line">   iops        : min=    6, max= 3290, avg=2005.29, stdev=1351.28, samples=119</div><div class="line">  lat (usec)   : 20=0.01%</div><div class="line">  lat (msec)   : 10=36.51%, 20=62.63%, 50=0.21%, 100=0.12%, 250=0.05%</div><div class="line">  lat (msec)   : 500=0.02%, 750=0.02%, 1000=0.19%, 2000=0.26%</div><div class="line">  cpu          : usr=0.62%, sys=4.04%, ctx=125974, majf=0, minf=3</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwt: total=119533,119347,0, short=0,0,0, dropped=0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=7949KiB/s (8140kB/s), 7949KiB/s-7949KiB/s (8140kB/s-8140kB/s), io=467MiB (490MB), run=60150-60150msec</div><div class="line">  WRITE: bw=7937KiB/s (8127kB/s), 7937KiB/s-7937KiB/s (8127kB/s-8127kB/s), io=466MiB (489MB), run=60150-60150msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  vda: ios=119533/108186, merge=0/214, ticks=54093/4937255, in_queue=2525052, util=93.99%</div><div class="line">  </div><div class="line">$fio -ioengine=libaio -bs=4k -direct=0 -buffered=0  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</div><div class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</div><div class="line">fio-3.1</div><div class="line">Starting 1 thread</div><div class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=9644KiB/s,w=9792KiB/s][r=2411,w=2448 IOPS][eta 00m:00s]</div><div class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=26139: Tue Feb 23 16:39:43 2021</div><div class="line">   read: IOPS=2455, BW=9823KiB/s (10.1MB/s)(576MiB/60015msec)</div><div class="line">    slat (nsec): min=1619, max=18282k, avg=5882.81, stdev=71214.52</div><div class="line">    clat (usec): min=281, max=64630, avg=13055.68, stdev=4233.17</div><div class="line">     lat (usec): min=323, max=64636, avg=13061.69, stdev=4232.79</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[ 2040],  5.00th=[10290], 10.00th=[10421], 20.00th=[10421],</div><div class="line">     | 30.00th=[10552], 40.00th=[10552], 50.00th=[10683], 60.00th=[10814],</div><div class="line">     | 70.00th=[18220], 80.00th=[19006], 90.00th=[19006], 95.00th=[19268],</div><div class="line">     | 99.00th=[19530], 99.50th=[20055], 99.90th=[28967], 99.95th=[29754],</div><div class="line">     | 99.99th=[30540]</div><div class="line">   bw (  KiB/s): min= 8776, max=27648, per=100.00%, avg=9824.29, stdev=1655.78, samples=120</div><div class="line">   iops        : min= 2194, max= 6912, avg=2456.05, stdev=413.95, samples=120</div><div class="line">  write: IOPS=2458, BW=9835KiB/s (10.1MB/s)(576MiB/60015msec)</div><div class="line">    slat (usec): min=2, max=10681, avg= 6.79, stdev=71.30</div><div class="line">    clat (usec): min=221, max=70411, avg=12909.50, stdev=4312.40</div><div class="line">     lat (usec): min=225, max=70414, avg=12916.40, stdev=4312.05</div><div class="line">    clat percentiles (usec):</div><div class="line">     |  1.00th=[ 1909],  5.00th=[10159], 10.00th=[10290], 20.00th=[10290],</div><div class="line">     | 30.00th=[10421], 40.00th=[10421], 50.00th=[10552], 60.00th=[10683],</div><div class="line">     | 70.00th=[18220], 80.00th=[18744], 90.00th=[19006], 95.00th=[19006],</div><div class="line">     | 99.00th=[19268], 99.50th=[19530], 99.90th=[28705], 99.95th=[40109],</div><div class="line">     | 99.99th=[60031]</div><div class="line">   bw (  KiB/s): min= 8568, max=28544, per=100.00%, avg=9836.03, stdev=1737.29, samples=120</div><div class="line">   iops        : min= 2142, max= 7136, avg=2458.98, stdev=434.32, samples=120</div><div class="line">  lat (usec)   : 250=0.01%, 500=0.03%, 750=0.02%, 1000=0.02%</div><div class="line">  lat (msec)   : 2=1.03%, 4=1.10%, 10=0.98%, 20=96.43%, 50=0.38%</div><div class="line">  lat (msec)   : 100=0.01%</div><div class="line">  cpu          : usr=0.82%, sys=4.32%, ctx=212008, majf=0, minf=4</div><div class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</div><div class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</div><div class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</div><div class="line">     issued rwt: total=147386,147564,0, short=0,0,0, dropped=0,0,0</div><div class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</div><div class="line"></div><div class="line">Run status group 0 (all jobs):</div><div class="line">   READ: bw=9823KiB/s (10.1MB/s), 9823KiB/s-9823KiB/s (10.1MB/s-10.1MB/s), io=576MiB (604MB), run=60015-60015msec</div><div class="line">  WRITE: bw=9835KiB/s (10.1MB/s), 9835KiB/s-9835KiB/s (10.1MB/s-10.1MB/s), io=576MiB (604MB), run=60015-60015msec</div><div class="line"></div><div class="line">Disk stats (read/write):</div><div class="line">  vda: ios=147097/147865, merge=0/241, ticks=1916703/1915836, in_queue=3791443, util=98.68%</div></pre></td></tr></table></figure>
<h3 id="测试数据总结"><a href="#测试数据总结" class="headerlink" title="测试数据总结"></a>测试数据总结</h3><table>
<thead>
<tr>
<th></th>
<th>-direct=1 -buffered=1</th>
<th>-direct=1 -buffered=0</th>
<th>-direct=0 -buffered=0</th>
<th>-direct=0 -buffered=1</th>
</tr>
</thead>
<tbody>
<tr>
<td>NVMe SSD</td>
<td>R=10.6k W=4544</td>
<td>R=99.8K W=42.8K</td>
<td>R=38.6k W=16.5k</td>
<td>R=10.8K W=4642</td>
</tr>
<tr>
<td>SATA SSD</td>
<td>R=4312 W=1852</td>
<td>R=16.9k W=7254</td>
<td>R=15.8k W=6803</td>
<td>R=5389 W=2314</td>
</tr>
<tr>
<td>ESSD</td>
<td>R=2149 W=2150</td>
<td>R=2462 W=2465</td>
<td>R=2455 W=2458</td>
<td>R=1987 W=1984</td>
</tr>
</tbody>
</table>
<p>看起来，<strong>对于SSD如果buffered为1的话direct没啥用，如果buffered为0那么direct为1性能要好很多</strong></p>
<p><strong>SATA SSD的IOPS比NVMe性能差很多</strong>。</p>
<p>SATA SSD当-buffered=1参数下SATA SSD的latency在7-10us之间。 </p>
<p>NVMe SSD以及SATA SSD当buffered=0的条件下latency均为2-3us,  NVMe SSD latency参考文章第一个表格， 和本次NVMe测试结果一致.  </p>
<p>ESSD的latency基本是13-16us。</p>
<p>以上NVMe SSD测试数据是在测试过程中还有mysql在全力导入数据的情况下，用fio测试所得。所以空闲情况下测试结果会更好。</p>
<h3 id="HDD性能测试数据"><a href="#HDD性能测试数据" class="headerlink" title="HDD性能测试数据"></a>HDD性能测试数据</h3><p><img src="/images/oss/0868d560-067f-4302-bc60-bffc3d4460ed.png" alt="img"></p>
<p>从上图可以看到这个磁盘的IOPS 读 935 写 400，读rt 10731nsec 大约10us, 写 17us。如果IOPS是1000的话，rt应该是1ms，实际比1ms小两个数量级，<del>应该是cache、磁盘阵列在起作用。</del></p>
<p>SATA硬盘，10K转</p>
<p>万转机械硬盘组成RAID5阵列，在顺序条件最好的情况下，带宽可以达到1GB/s以上，平均延时也非常低，最低只有20多us。但是在随机IO的情况下，机械硬盘的短板就充分暴露了，零点几兆的带宽，将近5ms的延迟，IOPS只有200左右。其原因是因为</p>
<ul>
<li>随机访问直接让RAID卡缓存成了个摆设</li>
<li>磁盘不能并行工作，因为我的机器RAID宽度Strip Size为128 KB</li>
<li>机械轴也得在各个磁道之间跳来跳去。</li>
</ul>
<p>理解了磁盘顺序IO时候的几十M甚至一个GB的带宽，随机IO这个真的是太可怜了。</p>
<p>从上面的测试数据中我们看到了机械硬盘在顺序IO和随机IO下的巨大性能差异。在顺序IO情况下，磁盘是最擅长的顺序IO,再加上Raid卡缓存命中率也高。这时带宽表现有几十、几百M，最好条件下甚至能达到1GB。IOPS这时候能有2-3W左右。到了随机IO的情形下，机械轴也被逼的跳来跳去寻道，RAID卡缓存也失效了。带宽跌到了1MB以下，最低只有100K，IOPS也只有可怜巴巴的200左右。</p>
<h3 id="网上测试数据参考"><a href="#网上测试数据参考" class="headerlink" title="网上测试数据参考"></a><a href="https://zhuanlan.zhihu.com/p/40497397" target="_blank" rel="external">网上测试数据参考</a></h3><p>我们来一起看一下具体的数据。首先来看NVＭe如何减小了协议栈本身的时间消耗，我们用<em>blktrace</em>工具来分析一组传输在应用程序层、操作系统层、驱动层和硬件层消耗的时间和占比，来了解AHCI和NVMe协议的性能区别：</p>
<p><img src="https://pic4.zhimg.com/80/v2-8b37f236d5c754efabe17aa9706f99a3_720w.jpg" alt="img"></p>
<p>硬盘HDD作为一个参考基准，它的时延是非常大的，达到14ms，而AHCI SATA为125us，NVMe为111us。我们从图中可以看出，NVMe相对AHCI，协议栈及之下所占用的时间比重明显减小，应用程序层面等待的时间占比很高，这是因为SSD物理硬盘速度不够快，导致应用空转。NVMe也为将来Optane硬盘这种低延迟介质的速度提高留下了广阔的空间。</p>
<h2 id="rq-affinity"><a href="#rq-affinity" class="headerlink" title="rq_affinity"></a>rq_affinity</h2><p>参考<a href="https://help.aliyun.com/knowledge_detail/65077.html#title-x10-2c0-yll" target="_blank" rel="external">aliyun测试文档</a> , rq_affinity增加2的commit： git show 5757a6d76c</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">function RunFio</div><div class="line">&#123;</div><div class="line"> numjobs=$1   # 实例中的测试线程数，例如示例中的10</div><div class="line"> iodepth=$2   # 同时发出I/O数的上限，例如示例中的64</div><div class="line"> bs=$3        # 单次I/O的块文件大小，例如示例中的4k</div><div class="line"> rw=$4        # 测试时的读写策略，例如示例中的randwrite</div><div class="line"> filename=$5  # 指定测试文件的名称，例如示例中的/dev/your_device</div><div class="line"> nr_cpus=`cat /proc/cpuinfo |grep &quot;processor&quot; |wc -l`</div><div class="line"> if [ $nr_cpus -lt $numjobs ];then</div><div class="line">     echo “Numjobs is more than cpu cores, exit!”</div><div class="line">     exit -1</div><div class="line"> fi</div><div class="line"> let nu=$numjobs+1</div><div class="line"> cpulist=&quot;&quot;</div><div class="line"> for ((i=1;i&lt;10;i++))</div><div class="line"> do</div><div class="line">     list=`cat /sys/block/your_device/mq/*/cpu_list | awk &apos;&#123;if(i&lt;=NF) print $i;&#125;&apos; i=&quot;$i&quot; | tr -d &apos;,&apos; | tr &apos;\n&apos; &apos;,&apos;`</div><div class="line">     if [ -z $list ];then</div><div class="line">         break</div><div class="line">     fi</div><div class="line">     cpulist=$&#123;cpulist&#125;$&#123;list&#125;</div><div class="line"> done</div><div class="line"> spincpu=`echo $cpulist | cut -d &apos;,&apos; -f 2-$&#123;nu&#125;`</div><div class="line"> echo $spincpu</div><div class="line"> fio --ioengine=libaio --runtime=30s --numjobs=$&#123;numjobs&#125; --iodepth=$&#123;iodepth&#125; --bs=$&#123;bs&#125; --rw=$&#123;rw&#125; --filename=$&#123;filename&#125; --time_based=1 --direct=1 --name=test --group_reporting --cpus_allowed=$spincpu --cpus_allowed_policy=split</div><div class="line">&#125;</div><div class="line">echo 2 &gt; /sys/block/your_device/queue/rq_affinity</div><div class="line">sleep 5</div><div class="line">RunFio 10 64 4k randwrite filename</div></pre></td></tr></table></figure>
<p>对NVME SSD进行测试，左边rq_affinity是2，右边rq_affinity为1，在这个测试参数下rq_affinity为1的性能要好(后许多次测试两者性能差不多)</p>
<p><img src="/images/951413iMgBlog/image-20210607113709945.png" alt="image-20210607113709945"></p>
<h2 id="LVM性能对比"><a href="#LVM性能对比" class="headerlink" title="LVM性能对比"></a>LVM性能对比</h2><p>磁盘信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#lsblk</div><div class="line">NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</div><div class="line">sda            8:0    0 223.6G  0 disk</div><div class="line">├─sda1         8:1    0     3M  0 part</div><div class="line">├─sda2         8:2    0     1G  0 part /boot</div><div class="line">├─sda3         8:3    0    96G  0 part /</div><div class="line">├─sda4         8:4    0    10G  0 part /tmp</div><div class="line">└─sda5         8:5    0 116.6G  0 part /home</div><div class="line">nvme0n1      259:4    0   2.7T  0 disk</div><div class="line">└─nvme0n1p1  259:5    0   2.7T  0 part</div><div class="line">  └─vg1-drds 252:0    0   5.4T  0 lvm  /drds</div><div class="line">nvme1n1      259:0    0   2.7T  0 disk</div><div class="line">└─nvme1n1p1  259:2    0   2.7T  0 part /u02</div><div class="line">nvme2n1      259:1    0   2.7T  0 disk</div><div class="line">└─nvme2n1p1  259:3    0   2.7T  0 part</div><div class="line">  └─vg1-drds 252:0    0   5.4T  0 lvm  /drds</div></pre></td></tr></table></figure>
<p>单块nvme SSD盘跑mysql server，运行sysbench导入测试数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">#iostat -x nvme1n1 1</div><div class="line">Linux 3.10.0-327.ali2017.alios7.x86_64 (k28a11352.eu95sqa) 	05/13/2021 	_x86_64_	(64 CPU)</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           0.32    0.00    0.17    0.07    0.00   99.44</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">nvme1n1           0.00    47.19    0.19  445.15     2.03 43110.89   193.62     0.31    0.70    0.03    0.70   0.06   2.85</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.16    0.00    0.36    0.17    0.00   98.31</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">nvme1n1           0.00   122.00    0.00 3290.00     0.00 271052.00   164.77     1.65    0.50    0.00    0.50   0.05  17.00</div><div class="line"></div><div class="line">#iostat 1</div><div class="line">Linux 3.10.0-327.ali2017.alios7.x86_64 (k28a11352.eu95sqa) 	05/13/2021 	_x86_64_	(64 CPU)</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           0.14    0.00    0.13    0.05    0.00   99.67</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda              49.21       554.51      2315.83    1416900    5917488</div><div class="line">nvme1n1           5.65         2.34       844.73       5989    2158468</div><div class="line">nvme2n1           0.06         1.13         0.00       2896          0</div><div class="line">nvme0n1           0.06         1.13         0.00       2900          0</div><div class="line">dm-0              0.02         0.41         0.00       1036          0</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.39    0.00    0.23    0.08    0.00   98.30</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda               8.00         0.00        60.00          0         60</div><div class="line">nvme1n1         868.00         0.00    132100.00          0     132100</div><div class="line">nvme2n1           0.00         0.00         0.00          0          0</div><div class="line">nvme0n1           0.00         0.00         0.00          0          0</div><div class="line">dm-0              0.00         0.00         0.00          0          0</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.44    0.00    0.14    0.09    0.00   98.33</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda               0.00         0.00         0.00          0          0</div><div class="line">nvme1n1         766.00         0.00    132780.00          0     132780</div><div class="line">nvme2n1           0.00         0.00         0.00          0          0</div><div class="line">nvme0n1           0.00         0.00         0.00          0          0</div><div class="line">dm-0              0.00         0.00         0.00          0          0</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.41    0.00    0.16    0.09    0.00   98.34</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda             105.00         0.00       532.00          0        532</div><div class="line">nvme1n1         760.00         0.00    122236.00          0     122236</div><div class="line">nvme2n1           0.00         0.00         0.00          0          0</div><div class="line">nvme0n1           0.00         0.00         0.00          0          0</div><div class="line">dm-0              0.00         0.00         0.00          0          0</div></pre></td></tr></table></figure>
<p>如果同样写lvm，由两块nvme组成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">nvme2n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</div><div class="line">nvme0n1           0.00   137.00    0.00 5730.00     0.00 421112.00   146.98     2.95    0.52    0.00    0.52   0.05  27.30</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.17    0.00    0.34    0.19    0.00   98.30</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">nvme2n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</div><div class="line">nvme0n1           0.00   109.00    0.00 2533.00     0.00 271236.00   214.16     1.08    0.43    0.00    0.43   0.06  15.90</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.38    0.00    0.42    0.20    0.00   98.00</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</div><div class="line">nvme2n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</div><div class="line">nvme0n1           0.00   118.00    0.00 3336.00     0.00 320708.00   192.27     1.50    0.45    0.00    0.45   0.06  20.00</div><div class="line"></div><div class="line">[root@k28a11352.eu95sqa /var/lib]</div><div class="line">#iostat  1</div><div class="line">Linux 3.10.0-327.ali2017.alios7.x86_64 (k28a11352.eu95sqa) 	05/13/2021 	_x86_64_	(64 CPU)</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           0.40    0.00    0.20    0.07    0.00   99.33</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda              38.96       334.64      1449.68    1419236    6148304</div><div class="line">nvme1n1         324.95         1.43     31201.30       6069  132329072</div><div class="line">nvme2n1           0.07         0.90         0.00       3808          0</div><div class="line">nvme0n1         256.24         1.60     22918.46       6801   97200388</div><div class="line">dm-0            266.98         1.38     22918.46       5849   97200388</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.20    0.00    0.42    0.25    0.00   98.12</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda               0.00         0.00         0.00          0          0</div><div class="line">nvme1n1           0.00         0.00         0.00          0          0</div><div class="line">nvme2n1           0.00         0.00         0.00          0          0</div><div class="line">nvme0n1        4460.00         0.00    332288.00          0     332288</div><div class="line">dm-0           4608.00         0.00    332288.00          0     332288</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">           1.35    0.00    0.38    0.22    0.00   98.06</div><div class="line"></div><div class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</div><div class="line">sda              48.00         0.00       200.00          0        200</div><div class="line">nvme1n1           0.00         0.00         0.00          0          0</div><div class="line">nvme2n1           0.00         0.00         0.00          0          0</div><div class="line">nvme0n1        4187.00         0.00    332368.00          0     332368</div><div class="line">dm-0           4348.00         0.00    332368.00          0     332368</div></pre></td></tr></table></figure>
<p>不知道为什么只有一个块ssd有流量，可能跟只写一个文件有关系</p>
<h2 id="SSD中，SATA、m2、PCIE和NVME各有什么意义"><a href="#SSD中，SATA、m2、PCIE和NVME各有什么意义" class="headerlink" title="SSD中，SATA、m2、PCIE和NVME各有什么意义"></a>SSD中，SATA、m2、PCIE和NVME各有什么意义</h2><h3 id="高速信号协议"><a href="#高速信号协议" class="headerlink" title="高速信号协议"></a>高速信号协议</h3><p> SAS，SATA，PCIe 这三个是同一个层面上的，模拟串行高速接口。</p>
<ul>
<li>SAS 对扩容比较友好，也支持双控双活。接上SAS RAID 卡，一般在阵列上用的比较多。</li>
<li>SATA 对热插拔很友好，早先台式机装机市场的 SSD基本上都是SATA的，现在的 机械硬盘也是SATA接口居多。但速率上最高只能到 6Gb/s，上限 768MB/s左右，现在已经慢慢被pcie取代。</li>
<li>PCIe 支持速率更高，也离CPU最近。很多设备 如 网卡，显卡也都走pcie接口，当然也有SSD。现在比较主流的是PCIe 3.0,8Gb/s 看起来好像也没比 SATA 高多少，但是 PCIe 支持多个LANE，每个LANE都是 8Gb/s，这样性能就倍数增加了。目前，SSD主流的是 PCIe 3.0x4 lane，性能可以做到 3500MB/s 左右。</li>
</ul>
<h3 id="传输层协议"><a href="#传输层协议" class="headerlink" title="传输层协议"></a>传输层协议</h3><p>SCSI，ATA，NVMe 都属于这一层。主要是定义命令集，数字逻辑层。</p>
<ul>
<li>SCSI 命令集 历史悠久，应用也很广泛。U盘，SAS 盘，还有手机上 UFS 之类很多设备都走的这个命令集。</li>
<li>ATA 则只是跑在SATA 协议上</li>
<li>NVMe 协议是有特意为 NAND 进行优化。相比于上面两者，效率更高。主要是跑在 PCIe 上的。当然，也有NVMe-MI，NVMe-of之类的。是个很好的传输层协议。</li>
</ul>
<h3 id="物理接口"><a href="#物理接口" class="headerlink" title="物理接口"></a>物理接口</h3><p>M.2 , U.2 , AIC, NGFF 这些属于物理接口</p>
<p>像 M.2 可以是 SATA SSD 也可以是 NVMe（PCIe） SSD。金手指上有一个 SATA/PCIe 的选择信号，来区分两者。很多笔记本的M.2 接口也是同时支持两种类型的盘的。</p>
<ul>
<li><p>M.2 , 主要用在 笔记本上，优点是体积小，缺点是散热不好。</p>
</li>
<li><p>U.2,主要用在 数据中心或者一些企业级用户，对热插拔需求高的地方。优点热插拔，散热也不错。一般主要是pcie ssd(也有sas ssd)，受限于接口，最多只能是 pcie 4lane</p>
</li>
<li><p>AIC，企业，行业用户用的比较多。通常会支持pcie 4lane/8lane，带宽上限更高</p>
</li>
</ul>
<h2 id="数据总结"><a href="#数据总结" class="headerlink" title="数据总结"></a>数据总结</h2><ul>
<li>性能排序 NVMe SSD &gt; SATA SSD &gt; SAN &gt; ESSD &gt; HDD</li>
<li>本地ssd性能最好、sas机械盘(RAID10)性能最差</li>
<li>san存储走特定的光纤网络，不是走tcp的san（至少从网卡看不到san的流量），性能居中</li>
<li>从rt来看 ssd:san:sas 大概是 1:3:15</li>
<li>san比本地sas机械盘性能要好，这也许取决于san的网络传输性能和san存储中的设备（比如用的ssd而不是机械盘）</li>
<li>NVMe SSD比SATA SSD快很多，latency更稳定</li>
<li>阿里云的云盘ESSD比本地SAS RAID10阵列性能还好</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://cizixs.com/2017/01/03/how-slow-is-disk-and-network" target="_blank" rel="external">http://cizixs.com/2017/01/03/how-slow-is-disk-and-network</a></p>
<p><a href="https://tobert.github.io/post/2014-04-17-fio-output-explained.html" target="_blank" rel="external">https://tobert.github.io/post/2014-04-17-fio-output-explained.html</a> </p>
<p><a href="https://zhuanlan.zhihu.com/p/40497397" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/40497397</a></p>
<p><a href="https://www.atatech.org/articles/167736?spm=ata.home.0.0.11fd75362qwsg7&amp;flag_data_from=home_algorithm_article" target="_blank" rel="external">块存储NVMe云盘原型实践</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247483999&amp;idx=1&amp;sn=238d3d1a8cf24443db0da4aa00c9fb7e&amp;chksm=a6e3036491948a72704e0b114790483f227b7ce82f5eece5dd870ef88a8391a03eca27e8ff61&amp;scene=178&amp;cur_album_id=1371808335259090944#rd" target="_blank" rel="external">机械硬盘随机IO慢的超乎你的想象</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247484023&amp;idx=1&amp;sn=1946b4c286ed72da023b402cc30908b6&amp;chksm=a6e3034c91948a5aa3b0e6beb31c1d3804de9a11c668400d598c2a6b12462e179cf9f1dc33e2&amp;scene=178&amp;cur_album_id=1371808335259090944#rd" target="_blank" rel="external">搭载固态硬盘的服务器究竟比搭机械硬盘快多少？</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/24/如何制作本地yum repository/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/24/如何制作本地yum repository/" itemprop="url">如何制作本地yum repository</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-24T17:30:03+08:00">
                2020-01-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何制作本地yum-repository"><a href="#如何制作本地yum-repository" class="headerlink" title="如何制作本地yum repository"></a>如何制作本地yum repository</h1><p>某些情况下在没有外网的环境需要安装一些软件，但是软件依赖比较多，那么可以提前将所有依赖下载到本地，然后将他们制作成一个yum repo，安装的时候就会自动将依赖包都安装好。</p>
<h2 id="收集所有rpm包"><a href="#收集所有rpm包" class="headerlink" title="收集所有rpm包"></a>收集所有rpm包</h2><p>创建一个文件夹，比如 Yum，将收集到的所有rpm包放在里面，比如安装ansible和docker需要的依赖文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-rwxr-xr-x 1 root root  73K 7月  12 14:22 audit-libs-python-2.8.4-4.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root 295K 7月  12 14:22 checkpolicy-2.5-8.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  23M 7月  12 14:22 containerd.io-1.2.2-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  26K 7月  12 14:22 container-selinux-2.9-4.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  37K 7月  12 14:22 container-selinux-2.74-1.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  14M 7月  12 14:22 docker-ce-cli-18.09.0-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  29K 7月  12 14:22 docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-2.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-1.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root 154K 7月  12 14:23 PyYAML-3.10-11.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  29K 7月  12 14:23 python-six-1.9.0-2.el7.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root 397K 7月  12 14:23 python-setuptools-0.9.8-7.el7.noarch.rpm</div></pre></td></tr></table></figure>
<p>收集方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//先安装yum工具</div><div class="line">yum install yum-utils -y</div><div class="line">//将 ansible 依赖包都下载下来</div><div class="line">repoquery --requires --resolve --recursive ansible | xargs -r yumdownloader --destdir=/tmp/ansible</div><div class="line">//将ansible rpm自己下载回来</div><div class="line">yumdownloader --destdir=/tmp/ansible --resolve ansible</div><div class="line">//验证一下依赖关系是完整的</div><div class="line">//repotrack ansible</div></pre></td></tr></table></figure>
<h2 id="创建仓库索引"><a href="#创建仓库索引" class="headerlink" title="创建仓库索引"></a>创建仓库索引</h2><p>需要安装工具 yum install createrepo -y：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># createrepo ./public_yum/</div><div class="line">Spawning worker 0 with 6 pkgs</div><div class="line">Spawning worker 1 with 6 pkgs</div><div class="line">Spawning worker 23 with 5 pkgs</div><div class="line">Workers Finished</div><div class="line">Saving Primary metadata</div><div class="line">Saving file lists metadata</div><div class="line">Saving other metadata</div><div class="line">Generating sqlite DBs</div><div class="line">Sqlite DBs complete</div></pre></td></tr></table></figure>
<p>会在yum文件夹下生成一个索引文件夹 repodata</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x 2 root root 4.0K 7月  12 14:25 repodata</div><div class="line">[root@az1-drds-79 yum]# ls repodata/</div><div class="line">5e15c62fec1fe43c6025ecf4d370d632f4b3f607500016e045ad94b70f87bac3-filelists.xml.gz</div><div class="line">7a314396d6e90532c5c534567f9bd34eee94c3f8945fc2191b225b2861ace2b6-other.xml.gz</div><div class="line">ce9dce19f6b426b8856747b01d51ceaa2e744b6bbd5fbc68733aa3195f724590-primary.xml.gz</div><div class="line">ee33b7d79e32fe6ad813af92a778a0ec8e5cc2dfdc9b16d0be8cff6a13e80d99-filelists.sqlite.bz2</div><div class="line">f7e8177e7207a4ff94bade329a0f6b572a72e21da106dd9144f8b1cdf0489cab-primary.sqlite.bz2</div><div class="line">ff52e1f1859790a7b573d2708b02404eb8b29aa4b0c337bda83af75b305bfb36-other.sqlite.bz2</div><div class="line">repomd.xml</div></pre></td></tr></table></figure>
<h2 id="生成iso镜像文件"><a href="#生成iso镜像文件" class="headerlink" title="生成iso镜像文件"></a>生成iso镜像文件</h2><p>非必要步骤，如果需要带到客户环境可以先生成iso，不过不够灵活。</p>
<p>也可以不用生成iso，直接在drds.repo中指定 createrepo 的目录也可以，记得要先执行 yum clean all和yum update </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#mkisofs -r -o docker_ansible.iso ./yum/</div><div class="line">I: -input-charset not specified, using utf-8 (detected in locale settings)</div><div class="line">Using PYTHO000.RPM;1 for  /python-httplib2-0.7.7-3.el7.noarch.rpm (python-httplib2-0.9.1-3.el7.noarch.rpm)</div><div class="line">Using MARIA006.RPM;1 for  /mariadb-5.5.56-2.el7.x86_64.rpm (mariadb-libs-5.5.56-2.el7.x86_64.rpm)</div><div class="line">Using LIBTO001.RPM;1 for  /libtomcrypt-1.17-25.el7.x86_64.rpm (libtomcrypt-1.17-26.el7.x86_64.rpm)</div><div class="line">  6.11% done, estimate finish Sun Jul 12 14:26:47 2020</div><div class="line"> 97.60% done, estimate finish Sun Jul 12 14:26:48 2020</div><div class="line">Total translation table size: 0</div><div class="line">Total rockridge attributes bytes: 14838</div><div class="line">Total directory bytes: 2048</div><div class="line">Path table size(bytes): 26</div><div class="line">Max brk space used 21000</div><div class="line">81981 extents written (160 MB)</div></pre></td></tr></table></figure>
<h2 id="将-生成的-iso挂载到目标机器上"><a href="#将-生成的-iso挂载到目标机器上" class="headerlink" title="将 生成的 iso挂载到目标机器上"></a>将 生成的 iso挂载到目标机器上</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mkdir /mnt/iso</div><div class="line"># mount ./docker_ansible.iso /mnt/iso</div><div class="line">mount: /dev/loop0 is write-protected, mounting read-only</div></pre></td></tr></table></figure>
<h2 id="配置本地-yum-源"><a href="#配置本地-yum-源" class="headerlink" title="配置本地 yum 源"></a>配置本地 yum 源</h2><p>yum repository不是必须要求iso挂载，直接指向rpm文件夹（必须要有 createrepo 建立索引了）也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># cat /etc/yum.repos.d/drds.repo </div><div class="line">[drds]</div><div class="line">name=drds Extra Packages for Enterprise Linux 7 - $basearch</div><div class="line">enabled=1</div><div class="line">failovermethod=priority</div><div class="line">baseurl=file:///mnt/repo #baseurl=http://192.168.1.91:8000/ 本地内网</div><div class="line">priority=1  #添加priority=1，数字越小优先级越高，也可以修改网络源的priority的值</div><div class="line">gpgcheck=0</div><div class="line">#gpgkey=file:///mnt/cdrom/RPM-GPG-KEY-CentOS-5    #注：这个你cd /mnt/cdrom/可以看到这个key，这里仅仅是个例子， 因为gpgcheck是0 ，所以gpgkey不需要了</div></pre></td></tr></table></figure>
<p>到此就可以在没有网络环境的机器上直接：yum install ansible docker -y 了 </p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的话可以指定repo 源： yum install ansible –enablerepo=drds （drds 优先级最高）</p>
<p>本地会cache一些rpm的版本信息，可以执行 yum clean all 得到一个干净的测试环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum clean all</div><div class="line">yum list</div><div class="line">yum deplist ansible</div></pre></td></tr></table></figure>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a href="https://access.redhat.com/solutions/641093" target="_blank" rel="external">Yum commands error “pycurl.so: undefined symbol”</a></p>
<h2 id="xargs-作用"><a href="#xargs-作用" class="headerlink" title="xargs 作用"></a>xargs 作用</h2><p><code>xargs</code>命令的作用，是将标准输入转为命令行参数。因为有些命令是不接受标准输入的，比如echo</p>
<p><code>xargs</code>的作用在于，大多数命令（比如<code>rm</code>、<code>mkdir</code>、<code>ls</code>）与管道一起使用时，都需要<code>xargs</code>将标准输入转为命令行参数。</p>
<h2 id="dnf-使用"><a href="#dnf-使用" class="headerlink" title="dnf 使用"></a>dnf 使用</h2><p><strong>DNF</strong> 是新一代的rpm软件包管理器。他首先出现在 Fedora 18 这个发行版中。而最近，它取代了yum，正式成为 Fedora 22 的包管理器。</p>
<p>DNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等多方面的内容。DNF使用 RPM, libsolv 和 hawkey 库进行包管理操作。尽管它没有预装在 CentOS 和 RHEL 7 中，但你可以在使用 YUM 的同时使用 DNF 。你可以在这里获得关于 DNF 的更多知识：《 DNF 代替 YUM ，你所不知道的缘由》</p>
<p>DNF 包管理器作为 YUM 包管理器的升级替代品，它能自动完成更多的操作。但在我看来，正因如此，所以 DNF 包管理器不会太受那些经验老道的 Linux 系统管理者的欢迎。举例如下：</p>
<ol>
<li>在 DNF 中没有 –skip-broken 命令，并且没有替代命令供选择。</li>
<li>在 DNF 中没有判断哪个包提供了指定依赖的 resolvedep 命令。</li>
<li>在 DNF 中没有用来列出某个软件依赖包的 deplist 命令。</li>
<li>当你在 DNF 中排除了某个软件库，那么该操作将会影响到你之后所有的操作，不像在 YUM 下那样，你的排除操作只会咋升级和安装软件时才起作用。</li>
</ol>
<h2 id="安装yum源"><a href="#安装yum源" class="headerlink" title="安装yum源"></a>安装yum源</h2><p>安装7.70版本curl yum源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -Uvh http://www.city-fan.org/ftp/contrib/yum-repo/city-fan.org-release-2-1.rhel7.noarch.rpm</div></pre></td></tr></table></figure>
<h2 id="其它技巧"><a href="#其它技巧" class="headerlink" title="其它技巧"></a>其它技巧</h2><h3 id="rpm依赖查询"><a href="#rpm依赖查询" class="headerlink" title="rpm依赖查询"></a>rpm依赖查询</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">rpm -q --whatprovides file-name //查询一个文件来自哪个rpm包</div><div class="line">rpm -qf /usr/lib/systemd/libsystemd-shared-239.so // 查询一个so lib来自哪个rpm包</div><div class="line">或者 yum whatprovides /usr/lib/systemd/libsystemd-shared-239.so</div><div class="line">yum provides */libmysqlclient.so.18</div></pre></td></tr></table></figure>
<h2 id="制作debian-仓库"><a href="#制作debian-仓库" class="headerlink" title="制作debian 仓库"></a><a href="https://rpmdeb.com/devops-articles/how-to-create-local-debian-repository/" target="_blank" rel="external">制作debian 仓库</a></h2><p>适合ubuntu、deepin、uos等, 参考：<a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">https://lework.github.io/2021/04/03/debian-kubeadm-install/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#添加新仓库</div><div class="line">sudo apt-add-repository &apos;deb http://ftp.us.debian.org/debian stretch main contrib non-free&apos;</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/2-60.png" alt="img"></p>
<p><a href="https://zh-cn.linuxcapable.com/%E5%9C%A8-debian-11-%E9%9D%B6%E5%BF%83%E4%B8%8A%E5%AE%89%E8%A3%85-rpm-%E5%8C%85/" target="_blank" rel="external">rpm转换成 dpkg</a></p>
<h3 id="apt-mirror"><a href="#apt-mirror" class="headerlink" title="apt-mirror"></a><a href="https://www.rickylss.site/os/linux/2020/05/12/debian-repositry/" target="_blank" rel="external">apt-mirror</a></h3><p>先要安装apt-mirror 工具，安装后会生成配置文件 /etc/apt/mirror.list 然后需要手工修改配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">#cat /etc/apt/mirror.list</div><div class="line">############# config ##################</div><div class="line"></div><div class="line">#下载下来的仓库文件放在哪里</div><div class="line">set base_path    /polarx/debian</div><div class="line"></div><div class="line">set mirror_path  $base_path/mirror</div><div class="line">set skel_path    $base_path/skel</div><div class="line">set var_path     $base_path/var</div><div class="line">set cleanscript $var_path/clean.sh</div><div class="line">set defaultarch  amd64</div><div class="line">#set postmirror_script $var_path/postmirror.sh</div><div class="line">set run_postmirror 0</div><div class="line">set nthreads     20</div><div class="line">set _tilde 0</div><div class="line">#</div><div class="line">############# end config ##############</div><div class="line"></div><div class="line">#从哪里镜像仓库</div><div class="line">deb http://yum.tbsite.net/mirrors/debian/ buster main non-free contrib</div><div class="line">#deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</div><div class="line"></div><div class="line">#deb http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-src http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line"># mirror additional architectures</div><div class="line">#deb-alpha http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-amd64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-armel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-hppa http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-i386 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-ia64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-m68k http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mips http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mipsel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-powerpc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-s390 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-sparc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line">#clean http://ftp.us.debian.org/debian</div><div class="line">clean http://yum.tbsite.net/mirrors/debian/</div></pre></td></tr></table></figure>
<h3 id="debian仓库介绍"><a href="#debian仓库介绍" class="headerlink" title="debian仓库介绍"></a><a href="https://wiki.debian.org/zh_CN/DebianRepository" target="_blank" rel="external">debian仓库介绍</a></h3><p>一个Debian仓库包含多个<strong>发行版</strong>。Debian 的发行版是以 “玩具总动员 “电影中的角色命名的 (wheezy, jessie, stretch, …)。 代号有别名，叫做<strong>套件</strong>(stable, oldstable, testing, unstable)。一个发行版会被分成几个<strong>组件</strong>。在 Debian 中，这些组件被命名为 <code>main</code>, <code>contrib</code>, 和 <code>non-free</code>，并表并表示它们所包含的软件的授权条款。一个版本也有各种<strong>架构</strong>(amd64, i386, mips, powerpc, s390x, …)的软件包，以及源码和架构独立的软件包。</p>
<p>仓库的<a href="https://mirrors.aliyun.com/debian/dists/" target="_blank" rel="external">根目录下有一个<code>dists</code> 目录</a>，而这个目录又有每个发行版和套件的目录，后者通常是前者的符号链接，但浏览器不会向您显示出这个区别。每个发行版子目录都包含一个加密签名的<code>Release</code>文件和每个组件的目录，里面是不同架构的目录，名为<code>binary</code>-<em>&lt;架构&gt;</em>和<code>sources</code>。而在这些文件中，<code>Packages</code>是文本文件，包含了软件包。嗯，那么实际的软件包在哪里？</p>
<p><img src="/images/951413iMgBlog/image-20220829163817671.png" alt="image-20220829163817671"></p>
<p>软件包本身在仓库根目录下的<code>pool</code>。在<code>pool</code>下面又有所有组件的目录，其中有<code>0</code>，…，<code>9</code>，<code>a</code>，<code>b</code>，.., <code>z</code>, <code>liba</code>, … , <code>libz</code>。 而在这些目录中，是以它们所包含的软件包命名的目录，这些目录最后包含实际的软件包，即<code>.deb</code>文件。这个名字不一定是软件包本身的名字，例如，软件包bsdutils在目录<code>pool/main/u/util-linux</code> 下，它是生成软件包的源码的名称。一个上游源可能会生成多个二进制软件包，而所有这些软件包最终都会在<code>pool</code>下面的同一个子目录中。额外的单字母目录只是一个技巧，以避免在一个目录中有太多的条目，因为这是很多系统传统上存在性能问题的原因。</p>
<p>在<code>pool</code>下面的子目录中，通常会有多个版本的软件包，而每个版本的软件包属于什么发行版的信息只存在于索引中。这样一来，同一个版本的包可软件以属于多个发行版，但只使用一次磁盘空间，而且不需要求助于硬链接或符号链接，所以镜像相当简单，甚至可以在没有这些概念的系统中进行。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">apt install kubeadm=1.20.12-00 //指定版本安装</div><div class="line"></div><div class="line">#查询可用版本</div><div class="line">apt-cache policy kubeadm</div><div class="line">apt list --all-versions kubeadm</div><div class="line"></div><div class="line">#清理</div><div class="line">apt clean --dry-run </div><div class="line">apt update</div><div class="line">apt list</div><div class="line">apt show kubeadm</div><div class="line"></div><div class="line">#查询安装包的所有文件</div><div class="line">dpkg-query -L kubeadm</div><div class="line"></div><div class="line">#列出所有依赖包</div><div class="line">apt-cache depends ansible</div><div class="line"></div><div class="line">#被依赖查询</div><div class="line">apt-cache rdepends kubelet</div><div class="line"></div><div class="line">dpkg -I kubernetes/pool/kubeadm_1.21.0-00_amd64.deb</div><div class="line"></div><div class="line">#下载依赖包</div><div class="line">apt-get download $(apt-rdepends kubeadm|grep -v &quot;^ &quot;)</div><div class="line">aptitude --download-only -y install $(apt-rdepends kubeadm|grep -v &quot;^ &quot;) //不能下载已经安装了的依赖包</div><div class="line"></div><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<h3 id="简单仓库"><a href="#简单仓库" class="headerlink" title="简单仓库"></a><a href="https://zhuanlan.zhihu.com/p/482592599" target="_blank" rel="external">简单仓库</a></h3><p>下载所有 deb 包以及他们的依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<p>生成 index</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dpkg-scanpackages -m . &gt; Packages</div></pre></td></tr></table></figure>
<p>apt source 指向这个目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">deb [trusted=yes] file:/polarx/test /</div></pre></td></tr></table></figure>
<h3 id="Kubernetes-仓库"><a href="#Kubernetes-仓库" class="headerlink" title="Kubernetes 仓库"></a>Kubernetes 仓库</h3><p><a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">debian 上通过kubeadm 安装 kubernetes 集群</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">//官方</div><div class="line">echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee -a /etc/apt/sources.list.d/kubernetes.list</div><div class="line"></div><div class="line">//阿里云仓库</div><div class="line">echo &apos;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&apos; &gt; /etc/apt/sources.list.d/kubernetes.list</div><div class="line">curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</div><div class="line">apt-get update</div><div class="line"></div><div class="line">export KUBE_VERSION=&quot;1.20.5&quot;</div><div class="line">apt-get install -y kubeadm=$KUBE_VERSION-00 kubelet=$KUBE_VERSION-00 kubectl=$KUBE_VERSION-00</div><div class="line">sudo apt-mark hold kubelet kubeadm kubectl</div><div class="line"></div><div class="line">[ -d /etc/bash_completion.d ] &amp;&amp; \</div><div class="line">    &#123; kubectl completion bash &gt; /etc/bash_completion.d/kubectl; \</div><div class="line">      kubeadm completion bash &gt; /etc/bash_completion.d/kubadm; &#125;</div><div class="line">      </div><div class="line">[ ! -d /usr/lib/systemd/system/kubelet.service.d ] &amp;&amp; mkdir -p /usr/lib/systemd/system/kubelet.service.d</div><div class="line">cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf</div><div class="line">[Service]</div><div class="line">CPUAccounting=true</div><div class="line">MemoryAccounting=true</div><div class="line">BlockIOAccounting=true</div><div class="line">ExecStartPre=/usr/bin/bash -c &apos;/usr/bin/mkdir -p /sys/fs/cgroup/&#123;cpuset,memory,systemd,pids,&quot;cpu,cpuacct&quot;&#125;/&#123;system,kube,kubepods&#125;.slice&apos;</div><div class="line">Slice=kube.slice</div><div class="line">EOF</div><div class="line">systemctl daemon-reload</div><div class="line"> </div><div class="line">systemctl enable kubelet.service</div></pre></td></tr></table></figure>
<h3 id="docker-仓库"><a href="#docker-仓库" class="headerlink" title="docker 仓库"></a>docker 仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apt-get install -y apt-transport-https ca-certificates curl gnupg2 lsb-release bash-completion</div><div class="line">    </div><div class="line">curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add -</div><div class="line">echo &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs)   stable&quot; &gt; /etc/apt/sources.list.d/docker-ce.list</div><div class="line">sudo apt-get update</div><div class="line">apt-get install -y docker-ce docker-ce-cli containerd.io</div><div class="line">apt-mark hold docker-ce docker-ce-cli containerd.io</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ruanyifeng.com/blog/2019/08/xargs-tutorial.html" target="_blank" rel="external">xargs 命令教程</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/24/如何制作本地软件仓库/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/24/如何制作本地软件仓库/" itemprop="url">如何制作本地软件仓库</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-24T17:30:03+08:00">
                2020-01-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何制作软件仓库"><a href="#如何制作软件仓库" class="headerlink" title="如何制作软件仓库"></a>如何制作软件仓库</h1><p>某些情况下在没有外网的环境需要安装一些软件，但是软件依赖比较多，那么可以提前将所有依赖下载到本地，然后将他们制作成一个yum repo，安装的时候就会自动将依赖包都安装好。</p>
<p>centos下是 yum 仓库，Debian、ubuntu下是apt仓库，我们先讲 yum 仓库的制作，Debian apt 仓库类似</p>
<h2 id="收集所有rpm包"><a href="#收集所有rpm包" class="headerlink" title="收集所有rpm包"></a>收集所有rpm包</h2><p>创建一个文件夹，比如 Yum，将收集到的所有rpm包放在里面，比如安装ansible和docker需要的依赖文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-rwxr-xr-x 1 root root  73K 7月  12 14:22 audit-libs-python-2.8.4-4.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root 295K 7月  12 14:22 checkpolicy-2.5-8.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  23M 7月  12 14:22 containerd.io-1.2.2-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  26K 7月  12 14:22 container-selinux-2.9-4.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  37K 7月  12 14:22 container-selinux-2.74-1.el7.noarch.rpm</div><div class="line">-rwxr-xr-x 1 root root  14M 7月  12 14:22 docker-ce-cli-18.09.0-3.el7.x86_64.rpm</div><div class="line">-rwxr-xr-x 1 root root  29K 7月  12 14:22 docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-2.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  22K 7月  12 14:23 sshpass-1.06-1.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root 154K 7月  12 14:23 PyYAML-3.10-11.el7.x86_64.rpm</div><div class="line">-r-xr-xr-x 1 root root  29K 7月  12 14:23 python-six-1.9.0-2.el7.noarch.rpm</div><div class="line">-r-xr-xr-x 1 root root 397K 7月  12 14:23 python-setuptools-0.9.8-7.el7.noarch.rpm</div></pre></td></tr></table></figure>
<p>收集方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//先安装yum工具</div><div class="line">yum install yum-utils -y</div><div class="line">//将 ansible 依赖包都下载下来</div><div class="line">repoquery --requires --resolve --recursive ansible | xargs -r yumdownloader --destdir=/tmp/ansible</div><div class="line">//将ansible rpm自己下载回来</div><div class="line">yumdownloader --destdir=/tmp/ansible --resolve ansible</div><div class="line">//验证一下依赖关系是完整的</div><div class="line">//repotrack ansible</div></pre></td></tr></table></figure>
<h2 id="创建仓库索引"><a href="#创建仓库索引" class="headerlink" title="创建仓库索引"></a>创建仓库索引</h2><p>需要安装工具 yum install createrepo -y：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># createrepo ./public_yum/</div><div class="line">Spawning worker 0 with 6 pkgs</div><div class="line">Spawning worker 1 with 6 pkgs</div><div class="line">Spawning worker 23 with 5 pkgs</div><div class="line">Workers Finished</div><div class="line">Saving Primary metadata</div><div class="line">Saving file lists metadata</div><div class="line">Saving other metadata</div><div class="line">Generating sqlite DBs</div><div class="line">Sqlite DBs complete</div></pre></td></tr></table></figure>
<p>会在yum文件夹下生成一个索引文件夹 repodata</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">drwxr-xr-x 2 root root 4.0K 7月  12 14:25 repodata</div><div class="line">[root@az1-drds-79 yum]# ls repodata/</div><div class="line">5e15c62fec1fe43c6025ecf4d370d632f4b3f607500016e045ad94b70f87bac3-filelists.xml.gz</div><div class="line">7a314396d6e90532c5c534567f9bd34eee94c3f8945fc2191b225b2861ace2b6-other.xml.gz</div><div class="line">ce9dce19f6b426b8856747b01d51ceaa2e744b6bbd5fbc68733aa3195f724590-primary.xml.gz</div><div class="line">ee33b7d79e32fe6ad813af92a778a0ec8e5cc2dfdc9b16d0be8cff6a13e80d99-filelists.sqlite.bz2</div><div class="line">f7e8177e7207a4ff94bade329a0f6b572a72e21da106dd9144f8b1cdf0489cab-primary.sqlite.bz2</div><div class="line">ff52e1f1859790a7b573d2708b02404eb8b29aa4b0c337bda83af75b305bfb36-other.sqlite.bz2</div><div class="line">repomd.xml</div></pre></td></tr></table></figure>
<h2 id="生成iso镜像文件"><a href="#生成iso镜像文件" class="headerlink" title="生成iso镜像文件"></a>生成iso镜像文件</h2><p>非必要步骤，如果需要带到客户环境可以先生成iso，不过不够灵活。</p>
<p>也可以不用生成iso，直接在drds.repo中指定 createrepo 的目录也可以，记得要先执行 yum clean all和yum update </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#mkisofs -r -o docker_ansible.iso ./yum/</div><div class="line">I: -input-charset not specified, using utf-8 (detected in locale settings)</div><div class="line">Using PYTHO000.RPM;1 for  /python-httplib2-0.7.7-3.el7.noarch.rpm (python-httplib2-0.9.1-3.el7.noarch.rpm)</div><div class="line">Using MARIA006.RPM;1 for  /mariadb-5.5.56-2.el7.x86_64.rpm (mariadb-libs-5.5.56-2.el7.x86_64.rpm)</div><div class="line">Using LIBTO001.RPM;1 for  /libtomcrypt-1.17-25.el7.x86_64.rpm (libtomcrypt-1.17-26.el7.x86_64.rpm)</div><div class="line">  6.11% done, estimate finish Sun Jul 12 14:26:47 2020</div><div class="line"> 97.60% done, estimate finish Sun Jul 12 14:26:48 2020</div><div class="line">Total translation table size: 0</div><div class="line">Total rockridge attributes bytes: 14838</div><div class="line">Total directory bytes: 2048</div><div class="line">Path table size(bytes): 26</div><div class="line">Max brk space used 21000</div><div class="line">81981 extents written (160 MB)</div></pre></td></tr></table></figure>
<h2 id="将-生成的-iso挂载到目标机器上"><a href="#将-生成的-iso挂载到目标机器上" class="headerlink" title="将 生成的 iso挂载到目标机器上"></a>将 生成的 iso挂载到目标机器上</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mkdir /mnt/iso</div><div class="line"># mount ./docker_ansible.iso /mnt/iso</div><div class="line">mount: /dev/loop0 is write-protected, mounting read-only</div></pre></td></tr></table></figure>
<h2 id="配置本地-yum-源"><a href="#配置本地-yum-源" class="headerlink" title="配置本地 yum 源"></a>配置本地 yum 源</h2><p>yum repository不是必须要求iso挂载，直接指向rpm文件夹（必须要有 createrepo 建立索引了）也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># cat /etc/yum.repos.d/drds.repo </div><div class="line">[drds]</div><div class="line">name=drds Extra Packages for Enterprise Linux 7 - $basearch</div><div class="line">enabled=1</div><div class="line">failovermethod=priority</div><div class="line">baseurl=file:///mnt/repo #baseurl=http://192.168.1.91:8000/ 本地内网</div><div class="line">priority=1  #添加priority=1，数字越小优先级越高，也可以修改网络源的priority的值</div><div class="line">gpgcheck=0</div><div class="line">#gpgkey=file:///mnt/cdrom/RPM-GPG-KEY-CentOS-5    #注：这个你cd /mnt/cdrom/可以看到这个key，这里仅仅是个例子， 因为gpgcheck是0 ，所以gpgkey不需要了</div></pre></td></tr></table></figure>
<p>到此就可以在没有网络环境的机器上直接：yum install ansible docker -y 了 </p>
<h2 id="yum-仓库代理"><a href="#yum-仓库代理" class="headerlink" title="yum 仓库代理"></a><a href="https://pshizhsysu.gitbook.io/linux/yum/wei-yum-yuan-pei-zhi-dai-li" target="_blank" rel="external">yum 仓库代理</a></h2><p>如果需要代理可以在docker.repo 最后添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">proxy=socks5://127.0.0.1:12345 //socks代理 或者 proxy=http://ip:port</div></pre></td></tr></table></figure>
<p>如果要给全局配置代理，则在 /etc/yum.conf 最后添加如上行</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的话可以指定repo 源： yum install ansible –enablerepo=drds （drds 优先级最高）</p>
<p>本地会cache一些rpm的版本信息，可以执行 yum clean all 得到一个干净的测试环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">yum clean all</div><div class="line">yum list</div><div class="line">yum deplist ansible</div></pre></td></tr></table></figure>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a href="https://access.redhat.com/solutions/641093" target="_blank" rel="external">Yum commands error “pycurl.so: undefined symbol”</a></p>
<h2 id="xargs-作用"><a href="#xargs-作用" class="headerlink" title="xargs 作用"></a>xargs 作用</h2><p><code>xargs</code>命令的作用，是将标准输入转为命令行参数。因为有些命令是不接受标准输入的，比如echo</p>
<p><code>xargs</code>的作用在于，大多数命令（比如<code>rm</code>、<code>mkdir</code>、<code>ls</code>）与管道一起使用时，都需要<code>xargs</code>将标准输入转为命令行参数。</p>
<h2 id="dnf-使用"><a href="#dnf-使用" class="headerlink" title="dnf 使用"></a>dnf 使用</h2><p><strong>DNF</strong> 是新一代的rpm软件包管理器。他首先出现在 Fedora 18 这个发行版中。而最近，它取代了yum，正式成为 Fedora 22 的包管理器。</p>
<p>DNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等多方面的内容。DNF使用 RPM, libsolv 和 hawkey 库进行包管理操作。尽管它没有预装在 CentOS 和 RHEL 7 中，但你可以在使用 YUM 的同时使用 DNF 。你可以在这里获得关于 DNF 的更多知识：《 DNF 代替 YUM ，你所不知道的缘由》</p>
<p>DNF 包管理器作为 YUM 包管理器的升级替代品，它能自动完成更多的操作。但在我看来，正因如此，所以 DNF 包管理器不会太受那些经验老道的 Linux 系统管理者的欢迎。举例如下：</p>
<ol>
<li>在 DNF 中没有 –skip-broken 命令，并且没有替代命令供选择。</li>
<li>在 DNF 中没有判断哪个包提供了指定依赖的 resolvedep 命令。</li>
<li>在 DNF 中没有用来列出某个软件依赖包的 deplist 命令。</li>
<li>当你在 DNF 中排除了某个软件库，那么该操作将会影响到你之后所有的操作，不像在 YUM 下那样，你的排除操作只会咋升级和安装软件时才起作用。</li>
</ol>
<h2 id="安装yum源"><a href="#安装yum源" class="headerlink" title="安装yum源"></a>安装yum源</h2><p>安装7.70版本curl yum源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -Uvh http://www.city-fan.org/ftp/contrib/yum-repo/city-fan.org-release-2-1.rhel7.noarch.rpm</div></pre></td></tr></table></figure>
<h2 id="其它技巧"><a href="#其它技巧" class="headerlink" title="其它技巧"></a>其它技巧</h2><h3 id="rpm依赖查询"><a href="#rpm依赖查询" class="headerlink" title="rpm依赖查询"></a>rpm依赖查询</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">rpm -q --whatprovides file-name //查询一个文件来自哪个rpm包</div><div class="line">rpm -qf /usr/lib/systemd/libsystemd-shared-239.so // 查询一个so lib来自哪个rpm包</div><div class="line">或者 yum whatprovides /usr/lib/systemd/libsystemd-shared-239.so</div><div class="line">yum provides */libmysqlclient.so.18</div><div class="line"></div><div class="line">rpm -qi //查看rpm包安装的安装信息</div></pre></td></tr></table></figure>
<h3 id="多版本问题"><a href="#多版本问题" class="headerlink" title="多版本问题"></a>多版本问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">//查看protobuf 的多个版本</div><div class="line"> yum --showduplicates list protobuf</div><div class="line">//or</div><div class="line"> repoquery --show-duplicates protobuf</div><div class="line"></div><div class="line">//指定版本安装，一般是安装教老的版本 </div><div class="line"> yum install protobuf-3.6.1-4.el7</div></pre></td></tr></table></figure>
<h2 id="制作debian-仓库"><a href="#制作debian-仓库" class="headerlink" title="制作debian 仓库"></a><a href="https://rpmdeb.com/devops-articles/how-to-create-local-debian-repository/" target="_blank" rel="external">制作debian 仓库</a></h2><p>适合ubuntu、deepin、uos等, 参考：<a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">https://lework.github.io/2021/04/03/debian-kubeadm-install/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#添加新仓库</div><div class="line">sudo apt-add-repository &apos;deb http://ftp.us.debian.org/debian stretch main contrib non-free&apos;</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/2-60.png" alt="img"></p>
<p><a href="https://zh-cn.linuxcapable.com/%E5%9C%A8-debian-11-%E9%9D%B6%E5%BF%83%E4%B8%8A%E5%AE%89%E8%A3%85-rpm-%E5%8C%85/" target="_blank" rel="external">rpm转换成 dpkg</a></p>
<h3 id="apt-mirror"><a href="#apt-mirror" class="headerlink" title="apt-mirror"></a><a href="https://www.rickylss.site/os/linux/2020/05/12/debian-repositry/" target="_blank" rel="external">apt-mirror</a></h3><p>先要安装apt-mirror 工具，安装后会生成配置文件 /etc/apt/mirror.list 然后需要手工修改配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">#cat /etc/apt/mirror.list</div><div class="line">############# config ##################</div><div class="line"></div><div class="line">#下载下来的仓库文件放在哪里</div><div class="line">set base_path    /polarx/debian</div><div class="line"></div><div class="line">set mirror_path  $base_path/mirror</div><div class="line">set skel_path    $base_path/skel</div><div class="line">set var_path     $base_path/var</div><div class="line">set cleanscript $var_path/clean.sh</div><div class="line">set defaultarch  amd64</div><div class="line">#set postmirror_script $var_path/postmirror.sh</div><div class="line">set run_postmirror 0</div><div class="line">set nthreads     20</div><div class="line">set _tilde 0</div><div class="line">#</div><div class="line">############# end config ##############</div><div class="line"></div><div class="line">#从哪里镜像仓库</div><div class="line">deb http://yum.tbsite.net/mirrors/debian/ buster main non-free contrib</div><div class="line">#deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main</div><div class="line"></div><div class="line">#deb http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-src http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line"># mirror additional architectures</div><div class="line">#deb-alpha http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-amd64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-armel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-hppa http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-i386 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-ia64 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-m68k http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mips http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-mipsel http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-powerpc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-s390 http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line">#deb-sparc http://ftp.us.debian.org/debian unstable main contrib non-free</div><div class="line"></div><div class="line">#clean http://ftp.us.debian.org/debian</div><div class="line">clean http://yum.tbsite.net/mirrors/debian/</div></pre></td></tr></table></figure>
<h3 id="debian仓库介绍"><a href="#debian仓库介绍" class="headerlink" title="debian仓库介绍"></a><a href="https://wiki.debian.org/zh_CN/DebianRepository" target="_blank" rel="external">debian仓库介绍</a></h3><p>一个Debian仓库包含多个<strong>发行版</strong>。Debian 的发行版是以 “玩具总动员 “电影中的角色命名的 (wheezy, jessie, stretch, …)。 代号有别名，叫做<strong>套件</strong>(stable, oldstable, testing, unstable)。一个发行版会被分成几个<strong>组件</strong>。在 Debian 中，这些组件被命名为 <code>main</code>, <code>contrib</code>, 和 <code>non-free</code>，并表并表示它们所包含的软件的授权条款。一个版本也有各种<strong>架构</strong>(amd64, i386, mips, powerpc, s390x, …)的软件包，以及源码和架构独立的软件包。</p>
<p>仓库的<a href="https://mirrors.aliyun.com/debian/dists/" target="_blank" rel="external">根目录下有一个<code>dists</code> 目录</a>，而这个目录又有每个发行版和套件的目录，后者通常是前者的符号链接，但浏览器不会向您显示出这个区别。每个发行版子目录都包含一个加密签名的<code>Release</code>文件和每个组件的目录，里面是不同架构的目录，名为<code>binary</code>-<em>&lt;架构&gt;</em>和<code>sources</code>。而在这些文件中，<code>Packages</code>是文本文件，包含了软件包。嗯，那么实际的软件包在哪里？</p>
<p><img src="/images/951413iMgBlog/image-20220829163817671.png" alt="image-20220829163817671"></p>
<p>软件包本身在仓库根目录下的<code>pool</code>。在<code>pool</code>下面又有所有组件的目录，其中有<code>0</code>，…，<code>9</code>，<code>a</code>，<code>b</code>，.., <code>z</code>, <code>liba</code>, … , <code>libz</code>。 而在这些目录中，是以它们所包含的软件包命名的目录，这些目录最后包含实际的软件包，即<code>.deb</code>文件。这个名字不一定是软件包本身的名字，例如，软件包bsdutils在目录<code>pool/main/u/util-linux</code> 下，它是生成软件包的源码的名称。一个上游源可能会生成多个二进制软件包，而所有这些软件包最终都会在<code>pool</code>下面的同一个子目录中。额外的单字母目录只是一个技巧，以避免在一个目录中有太多的条目，因为这是很多系统传统上存在性能问题的原因。</p>
<p>在<code>pool</code>下面的子目录中，通常会有多个版本的软件包，而每个版本的软件包属于什么发行版的信息只存在于索引中。这样一来，同一个版本的包可软件以属于多个发行版，但只使用一次磁盘空间，而且不需要求助于硬链接或符号链接，所以镜像相当简单，甚至可以在没有这些概念的系统中进行。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">apt install kubeadm=1.20.12-00 //指定版本安装</div><div class="line"></div><div class="line">#查询可用版本</div><div class="line">apt-cache policy kubeadm</div><div class="line">apt list --all-versions kubeadm</div><div class="line"></div><div class="line">#清理</div><div class="line">apt clean --dry-run </div><div class="line">apt update</div><div class="line">apt list/dpkg -l //列出本机所有已经安装的软件</div><div class="line">apt show kubeadm //显示版本、作者等信息</div><div class="line"></div><div class="line">#查询某个安装包的所有文件</div><div class="line">dpkg-query -L kubeadm</div><div class="line"></div><div class="line">#列出所有依赖包</div><div class="line">apt-cache depends ansible</div><div class="line"></div><div class="line">#被依赖查询</div><div class="line">apt-cache rdepends kubelet</div><div class="line"></div><div class="line">dpkg -I kubernetes/pool/kubeadm_1.21.0-00_amd64.deb</div><div class="line"></div><div class="line">#下载依赖包</div><div class="line">apt-get download $(apt-rdepends kubeadm|grep -v &quot;^ &quot;)</div><div class="line">aptitude --download-only -y install $(apt-rdepends kubeadm|grep -v &quot;^ &quot;) //不能下载已经安装了的依赖包</div><div class="line"></div><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<h3 id="简单仓库"><a href="#简单仓库" class="headerlink" title="简单仓库"></a><a href="https://zhuanlan.zhihu.com/p/482592599" target="_blank" rel="external">简单仓库</a></h3><p>下载所有 deb 包以及他们的依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt download $(apt-cache depends --recurse --no-recommends --no-suggests --no-conflicts --no-breaks --no-replaces --no-enhances kubeadm | grep &quot;^\w&quot; | sort -u)</div></pre></td></tr></table></figure>
<p>到deb 包所在的目录下生成 index</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dpkg-scanpackages -m . &gt; Packages</div></pre></td></tr></table></figure>
<p>/etc/apt/source.list 指向这个目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">deb [trusted=yes] file:/polarx/test /</div><div class="line">or</div><div class="line">deb [trusted=yes] http://kunpeng/pool/ ./</div></pre></td></tr></table></figure>
<p>验证：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt update //看是否能拉取你刚配置的仓库</div></pre></td></tr></table></figure>
<h3 id="Kubernetes-仓库"><a href="#Kubernetes-仓库" class="headerlink" title="Kubernetes 仓库"></a>Kubernetes 仓库</h3><p><a href="https://lework.github.io/2021/04/03/debian-kubeadm-install/" target="_blank" rel="external">debian 上通过kubeadm 安装 kubernetes 集群</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">//官方</div><div class="line">echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; | tee -a /etc/apt/sources.list.d/kubernetes.list</div><div class="line"></div><div class="line">//阿里云仓库</div><div class="line">echo &apos;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main&apos; &gt; /etc/apt/sources.list.d/kubernetes.list</div><div class="line">curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</div><div class="line">apt-get update</div><div class="line"></div><div class="line">export KUBE_VERSION=&quot;1.20.5&quot;</div><div class="line">apt-get install -y kubeadm=$KUBE_VERSION-00 kubelet=$KUBE_VERSION-00 kubectl=$KUBE_VERSION-00</div><div class="line">sudo apt-mark hold kubelet kubeadm kubectl</div><div class="line"></div><div class="line">[ -d /etc/bash_completion.d ] &amp;&amp; \</div><div class="line">    &#123; kubectl completion bash &gt; /etc/bash_completion.d/kubectl; \</div><div class="line">      kubeadm completion bash &gt; /etc/bash_completion.d/kubadm; &#125;</div><div class="line">      </div><div class="line">[ ! -d /usr/lib/systemd/system/kubelet.service.d ] &amp;&amp; mkdir -p /usr/lib/systemd/system/kubelet.service.d</div><div class="line">cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf</div><div class="line">[Service]</div><div class="line">CPUAccounting=true</div><div class="line">MemoryAccounting=true</div><div class="line">BlockIOAccounting=true</div><div class="line">ExecStartPre=/usr/bin/bash -c &apos;/usr/bin/mkdir -p /sys/fs/cgroup/&#123;cpuset,memory,systemd,pids,&quot;cpu,cpuacct&quot;&#125;/&#123;system,kube,kubepods&#125;.slice&apos;</div><div class="line">Slice=kube.slice</div><div class="line">EOF</div><div class="line">systemctl daemon-reload</div><div class="line"> </div><div class="line">systemctl enable kubelet.service</div></pre></td></tr></table></figure>
<h3 id="docker-仓库"><a href="#docker-仓库" class="headerlink" title="docker 仓库"></a>docker 仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apt-get install -y apt-transport-https ca-certificates curl gnupg2 lsb-release bash-completion</div><div class="line">    </div><div class="line">curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add -</div><div class="line">echo &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs)   stable&quot; &gt; /etc/apt/sources.list.d/docker-ce.list</div><div class="line">sudo apt-get update</div><div class="line">apt-get install -y docker-ce docker-ce-cli containerd.io</div><div class="line">apt-mark hold docker-ce docker-ce-cli containerd.io</div></pre></td></tr></table></figure>
<h3 id="锁定已安装版本"><a href="#锁定已安装版本" class="headerlink" title="锁定已安装版本"></a>锁定已安装版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">//锁定这三个软件的版本，避免意外升级导致版本错误：</div><div class="line">sudo apt-mark hold kubeadm kubelet kubectl</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ruanyifeng.com/blog/2019/08/xargs-tutorial.html" target="_blank" rel="external">xargs 命令教程</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/22/kubernetes service/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/22/kubernetes service/" itemprop="url">kubernetes service</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-22T17:30:03+08:00">
                2020-01-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-service-和-kube-proxy详解"><a href="#kubernetes-service-和-kube-proxy详解" class="headerlink" title="kubernetes service 和 kube-proxy详解"></a>kubernetes service 和 kube-proxy详解</h1><h2 id="service-模式"><a href="#service-模式" class="headerlink" title="service 模式"></a>service 模式</h2><p>根据创建Service的<code>type</code>类型不同，可分成4种模式：</p>
<ul>
<li>ClusterIP： <strong>默认方式</strong>。根据是否生成ClusterIP又可分为普通Service和Headless Service两类：<ul>
<li><code>普通Service</code>：通过为Kubernetes的Service分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问。为最常见的方式。</li>
<li><code>Headless Service</code>：该服务不会分配Cluster IP，也不通过kube-proxy做反向代理和负载均衡。而是通过DNS提供稳定的网络ID来访问，DNS会将headless service的后端直接解析为podIP列表。主要供StatefulSet中对应POD的序列用。</li>
</ul>
</li>
<li><code>NodePort</code>：除了使用Cluster IP之外，还通过将service的port映射到集群内每个节点的相同一个端口，实现通过nodeIP:nodePort从集群外访问服务。NodePort会RR转发给后端的任意一个POD，跟ClusterIP类似</li>
<li><code>LoadBalancer</code>：和nodePort类似，不过除了使用一个Cluster IP和nodePort之外，还会向所使用的公有云申请一个负载均衡器，实现从集群外通过LB访问服务。在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。</li>
<li><code>ExternalName</code>：是 Service 的特例。此模式主要面向运行在集群外部的服务，通过它可以将外部服务映射进k8s集群，且具备k8s内服务的一些特征（如具备namespace等属性），来为集群内部提供服务。此模式要求kube-dns的版本为1.7或以上。这种模式和前三种模式（除headless service）最大的不同是重定向依赖的是dns层次，而不是通过kube-proxy。</li>
</ul>
<p>service yaml案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: nginx-ren</div><div class="line">  labels:</div><div class="line">    app: web</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line"># clusterIP: None  </div><div class="line">  ports:</div><div class="line">  - port: 8080</div><div class="line">    targetPort: 80</div><div class="line">    nodePort: 30080</div><div class="line">  selector:</div><div class="line">    app: ren</div></pre></td></tr></table></figure>
<p><code>ports</code> 字段指定服务的端口信息：</p>
<ul>
<li><code>port</code>：虚拟 ip 要绑定的 port，每个 service 会创建出来一个虚拟 ip，通过访问 <code>vip:port</code> 就能获取服务的内容。这个 port 可以用户随机选取，因为每个服务都有自己的 vip，也不用担心冲突的情况</li>
<li><code>targetPort</code>：pod 中暴露出来的 port，这是运行的容器中具体暴露出来的端口，一定不能写错–一般用name来代替具体的port</li>
<li><code>protocol</code>：提供服务的协议类型，可以是 <code>TCP</code> 或者 <code>UDP</code></li>
<li><code>nodePort</code>： 仅在type为nodePort模式下有用，宿主机暴露端口</li>
</ul>
<p>但是nodePort和loadbalancer可以被外部访问，loadbalancer需要一个外部ip，流量走外部ip进出</p>
<p>NodePort向外部暴露了多个宿主机的端口，外部可以部署负载均衡将这些地址配置进去。</p>
<p>默认情况下，服务会rr转发到可用的后端。如果希望保持会话（同一个 client 永远都转发到相同的 pod），可以把 <code>service.spec.sessionAffinity</code> 设置为 <code>ClientIP</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">iptables-save | grep 3306</div><div class="line"></div><div class="line">iptables-save | grep KUBE-SERVICES</div><div class="line"></div><div class="line">#iptables-save |grep KUBE-SVC-RVEVH2XMONK6VC5O</div><div class="line">:KUBE-SVC-RVEVH2XMONK6VC5O - [0:0]</div><div class="line">-A KUBE-SERVICES -d 10.10.70.95/32 -p tcp -m comment --comment &quot;drds/mysql-read:mysql cluster IP&quot; -m tcp --dport 3306 -j KUBE-SVC-RVEVH2XMONK6VC5O</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-XC4TZYIZFYB653VI</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-MK4XPBZUIJGFXKED</div><div class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -j KUBE-SEP-AAYXWGQJBDHUJUQ3</div></pre></td></tr></table></figure>
<p>看起来 service 是个完美的方案，可以解决服务访问的所有问题，但是 service 这个方案（iptables 模式）也有自己的缺点。</p>
<p>首先，<strong>如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod</strong>，当然这个可以通过 <code>readiness probes</code> 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。</p>
<p>另外，<code>nodePort</code> 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。因为只做了NAT</p>
<h2 id="NodePort-的一些问题"><a href="#NodePort-的一些问题" class="headerlink" title="NodePort 的一些问题"></a>NodePort 的一些问题</h2><ul>
<li>首先endpoint回复不能走node 1给client，因为会被client reset（如果在node1上将src ip替换成node2的ip可能会路由不通）。回复包在 node1上要snat给node2</li>
<li>经过snat后endpoint没法拿到client ip（slb之类是通过option带过来）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">          client</div><div class="line">            \ ^</div><div class="line">             \ \</div><div class="line">              v \</div><div class="line">  node 1 &lt;--- node 2</div><div class="line">   | ^   SNAT</div><div class="line">   | |   ---&gt;</div><div class="line">   v |</div><div class="line">endpoint</div></pre></td></tr></table></figure>
<p>可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。</p>
<p>而这个机制的实现原理也非常简单：这时候，<strong>一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod</strong>。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">      client</div><div class="line">      ^ /   \</div><div class="line">     / /     \</div><div class="line">    / v       X</div><div class="line">  node 1     node 2</div><div class="line">   ^ |</div><div class="line">   | |</div><div class="line">   | v</div><div class="line">endpoint</div></pre></td></tr></table></figure>
<p>当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。</p>
<h2 id="Service和kube-proxy的工作原理"><a href="#Service和kube-proxy的工作原理" class="headerlink" title="Service和kube-proxy的工作原理"></a>Service和kube-proxy的工作原理</h2><p>kube-proxy有两种主要的实现（userspace基本没有使用了）：</p>
<ul>
<li>[[iptables使用]]来做NAT以及负载均衡</li>
<li>ipvs来做NAT以及负载均衡</li>
</ul>
<p>Service 是由 kube-proxy 组件通过监听 Pod 的变化事件，在宿主机上维护iptables规则或者ipvs规则。</p>
<p>Kube-proxy 主要监听两个对象，一个是 Service，一个是 Endpoint，监听他们启停。以及通过selector将他们绑定。</p>
<p>IPVS 是专门为LB设计的。它用hash table管理service，对service的增删查找都是<em>O(1)</em>的时间复杂度。不过IPVS内核模块没有SNAT功能，因此借用了iptables的SNAT功能。IPVS 针对报文做DNAT后，将连接信息保存在nf_conntrack中，iptables据此接力做SNAT。该模式是目前Kubernetes网络性能最好的选择。但是由于nf_conntrack的复杂性，带来了很大的性能损耗。</p>
<h3 id="iptables-实现负载均衡的工作流程"><a href="#iptables-实现负载均衡的工作流程" class="headerlink" title="iptables 实现负载均衡的工作流程"></a>iptables 实现负载均衡的工作流程</h3><p>如果kube-proxy不是用的ipvs模式，那么主要靠iptables来做DNAT和SNAT以及负载均衡</p>
<p>iptables+clusterIP工作流程：</p>
<ol>
<li>集群内访问svc 10.10.35.224:3306 命中 kube-services iptables</li>
<li>iptables 规则：KUBE-SEP-F4QDAAVSZYZMFXZQ 对应到  KUBE-SEP-F4QDAAVSZYZMFXZQ</li>
<li>KUBE-SEP-F4QDAAVSZYZMFXZQ 指示 DNAT到 宿主机：192.168.0.83:10379（在内核中将包改写了ip port）</li>
<li>从svc description中可以看到这个endpoint的地址 192.168.0.83:10379（pod使用Host network）</li>
</ol>
<p><img src="/images/oss/52e050ebb7841d70b7e3ea62e18d5b30.png" alt="image.png"></p>
<p>在对应的宿主机上可以清楚地看到容器中的mysqld进程正好监听着 10379端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@az1-drds-83 ~]# ss -lntp |grep 10379</div><div class="line">LISTEN     0      128         :::10379                   :::*                   users:((&quot;mysqld&quot;,pid=17707,fd=18))</div><div class="line">[root@az1-drds-83 ~]# ps auxff | grep 17707 -B2</div><div class="line">root     13606  0.0  0.0  10720  3764 ?        Sl   17:09   0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc</div><div class="line"></div><div class="line">root     13624  0.0  0.0 103044 10424 ?        Ss   17:09   0:00  |   \_ python /entrypoint.py</div><div class="line">root     14835  0.0  0.0  11768  1636 ?        S    17:10   0:00  |   \_ /bin/sh /u01/xcluster/bin/mysqld_safe --defaults-file=/home/mysql/my10379.cnf</div><div class="line">alidb    17707  0.6  0.0 1269128 67452 ?       Sl   17:10   0:25  |       \_ /u01/xcluster_20200303/bin/mysqld --defaults-file=/home/mysql/my10379.cnf --basedir=/u01/xcluster_20200303 --datadir=/home/mysql/data10379/dbs10379 --plugin-dir=/u01/xcluster_20200303/lib/plugin --user=mysql --log-error=/home/mysql/data10379/mysql/master-error.log --open-files-limit=8192 --pid-file=/home/mysql/data10379/dbs10379/az1-drds-83.pid --socket=/home/mysql/data10379/tmp/mysql.sock --port=10379</div></pre></td></tr></table></figure>
<p>对应的这个pod的description：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">#kubectl describe pod apsaradbcluster010-cv6w</div><div class="line">Name:         apsaradbcluster010-cv6w</div><div class="line">Namespace:    default</div><div class="line">Priority:     0</div><div class="line">Node:         az1-drds-83/192.168.0.83</div><div class="line">Start Time:   Thu, 10 Sep 2020 17:09:33 +0800</div><div class="line">Labels:       alisql.clusterName=apsaradbcluster010</div><div class="line">              alisql.pod_name=apsaradbcluster010-cv6w</div><div class="line">              alisql.pod_role=leader</div><div class="line">Annotations:  apsara.metric.pod_name: apsaradbcluster010-cv6w</div><div class="line">Status:       Running</div><div class="line">IP:           192.168.0.83</div><div class="line">IPs:</div><div class="line">  IP:           192.168.0.83</div><div class="line">Controlled By:  ApsaradbCluster/apsaradbcluster010</div><div class="line">Containers:</div><div class="line">  engine:</div><div class="line">    Container ID:   docker://ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97</div><div class="line">    Image:          reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-engine:develop-20200910140415</div><div class="line">    Image ID:       docker://sha256:7ad5cc53c87b34806eefec829d70f5f0192f4127c7ee4e867cb3da3bb6c2d709</div><div class="line">    Ports:          10379/TCP, 20383/TCP, 46846/TCP</div><div class="line">    Host Ports:     10379/TCP, 20383/TCP, 46846/TCP</div><div class="line">    State:          Running</div><div class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</div><div class="line">    Ready:          True</div><div class="line">    Restart Count:  0</div><div class="line">    Environment:</div><div class="line">      ALISQL_POD_NAME:  apsaradbcluster010-cv6w (v1:metadata.name)</div><div class="line">      ALISQL_POD_PORT:  10379</div><div class="line">    Mounts:</div><div class="line">      /dev/shm from devshm (rw)</div><div class="line">      /etc/localtime from etclocaltime (rw)</div><div class="line">      /home/mysql/data from data-dir (rw)</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</div><div class="line">  exporter:</div><div class="line">    Container ID:  docker://b49865b7798f9036b431203d54994ac8fdfcadacb01a2ab4494b13b2681c482d</div><div class="line">    Image:         reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-exporter:latest</div><div class="line">    Image ID:      docker://sha256:432cdd0a0e7c74c6eb66551b6f6af9e4013f60fb07a871445755f6577b44da19</div><div class="line">    Port:          47272/TCP</div><div class="line">    Host Port:     47272/TCP</div><div class="line">    Args:</div><div class="line">      --web.listen-address=:47272</div><div class="line">      --collect.binlog_size</div><div class="line">      --collect.engine_innodb_status</div><div class="line">      --collect.info_schema.innodb_metrics</div><div class="line">      --collect.info_schema.processlist</div><div class="line">      --collect.info_schema.tables</div><div class="line">      --collect.info_schema.tablestats</div><div class="line">      --collect.slave_hosts</div><div class="line">    State:          Running</div><div class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</div><div class="line">    Ready:          True</div><div class="line">    Restart Count:  0</div><div class="line">    Environment:</div><div class="line">      ALISQL_POD_NAME:   apsaradbcluster010-cv6w (v1:metadata.name)</div><div class="line">      DATA_SOURCE_NAME:  root:@(127.0.0.1:10379)/</div><div class="line">    Mounts:</div><div class="line">      /dev/shm from devshm (rw)</div><div class="line">      /etc/localtime from etclocaltime (rw)</div><div class="line">      /home/mysql/data from data-dir (rw)</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</div></pre></td></tr></table></figure>
<p>DNAT 规则的作用，就是<strong>在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口</strong>。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。</p>
<h4 id="哪些组件会修改iptables"><a href="#哪些组件会修改iptables" class="headerlink" title="哪些组件会修改iptables"></a>哪些组件会修改iptables</h4><p><img src="/images/oss/776d057b133692312578f01e74caca5b.png" alt="image.png"></p>
<h3 id="ipvs-实现负载均衡的原理"><a href="#ipvs-实现负载均衡的原理" class="headerlink" title="ipvs 实现负载均衡的原理"></a>ipvs 实现负载均衡的原理</h3><p>ipvs模式下，kube-proxy会先创建虚拟网卡，kube-ipvs0下面的每个ip都对应着svc的一个clusterIP：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># ip addr</div><div class="line">  ...</div><div class="line">5: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default </div><div class="line">    link/ether de:29:17:2a:8d:79 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 10.68.70.130/32 scope global kube-ipvs0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure>
<p>kube-ipvs0下面绑的这些ip就是在发包的时候让内核知道如果目标ip是这些地址的话，这些地址是自身的所以包不会发出去，而是给INPUT链，这样ipvs内核模块有机会改写包做完NAT后再发走。</p>
<p>ipvs会放置DNAT钩子在INPUT链上，因此必须要让内核识别 VIP 是本机的 IP。这样才会过INPUT 链，要不然就通过OUTPUT链出去了。k8s 通过kube-proxy将service cluster ip 绑定到虚拟网卡kube-ipvs0。</p>
<p>同时在路由表中增加一些ipvs 的路由条目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># ip route show table local</div><div class="line">local 10.68.0.1 dev kube-ipvs0 proto kernel scope host src 10.68.0.1 </div><div class="line">local 10.68.0.2 dev kube-ipvs0 proto kernel scope host src 10.68.0.2 </div><div class="line">local 10.68.70.130 dev kube-ipvs0 proto kernel scope host src 10.68.70.130 -- ipvs</div><div class="line">broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1 </div><div class="line">local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 </div><div class="line">local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 </div><div class="line">broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 </div><div class="line">broadcast 172.17.0.0 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">local 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 </div><div class="line">broadcast 172.17.255.255 dev docker0 proto kernel scope link src 172.17.0.1 </div><div class="line">local 172.20.185.192 dev tunl0 proto kernel scope host src 172.20.185.192 </div><div class="line">broadcast 172.20.185.192 dev tunl0 proto kernel scope link src 172.20.185.192 </div><div class="line">broadcast 172.26.128.0 dev eth0 proto kernel scope link src 172.26.137.117 </div><div class="line">local 172.26.137.117 dev eth0 proto kernel scope host src 172.26.137.117 </div><div class="line">broadcast 172.26.143.255 dev eth0 proto kernel scope link src 172.26.137.117</div></pre></td></tr></table></figure>
<p>而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ipvsadm -ln |grep 10.68.114.131 -A5</div><div class="line">TCP  10.68.114.131:3306 rr</div><div class="line">  -&gt; 172.20.120.143:3306          Masq    1      0          0         </div><div class="line">  -&gt; 172.20.185.209:3306          Masq    1      0          0         </div><div class="line">  -&gt; 172.20.248.143:3306          Masq    1      0          0</div></pre></td></tr></table></figure>
<p>172.20.<em>.</em> 是后端真正pod的ip， 10.68.114.131 是cluster-ip.</p>
<p>完整的工作流程如下：</p>
<ol>
<li>因为service cluster ip 绑定到虚拟网卡kube-ipvs0上，内核可以识别访问的 VIP 是本机的 IP.</li>
<li>数据包到达INPUT链.</li>
<li>ipvs监听到达input链的数据包，比对数据包请求的服务是为集群服务，修改数据包的目标IP地址为对应pod的IP，然后将数据包发至POSTROUTING链.</li>
<li>数据包经过POSTROUTING链选路由后，将数据包通过tunl0网卡(calico网络模型)发送出去。从tunl0虚拟网卡获得源IP.</li>
<li>经过tunl0后进行ipip封包，丢到物理网络，路由到目标node（目标pod所在的node）</li>
<li>目标node进行ipip解包后给pod对应的网卡</li>
<li>pod接收到请求之后，构建响应报文，改变源地址和目的地址，返回给客户端。</li>
</ol>
<p><img src="/images/oss/51695ebb1c6b30d95f8ac8d5dcb8dd7f.png" alt="image.png"></p>
<h4 id="ipvs实际案例"><a href="#ipvs实际案例" class="headerlink" title="ipvs实际案例"></a>ipvs实际案例</h4><p>ipvs负载均衡下一次完整的syn握手抓包。</p>
<p>宿主机上访问 curl clusterip+port 后因为这个ip绑定在kube-ipvs0上，本来是应该发出去的包（prerouting）但是内核认为这个包是访问自己，于是给INPUT链，接着被ipvs放置在INPUT中的DNAT钩子勾住，将dest ip根据负载均衡逻辑改成pod-ip，然后将数据包再发至POSTROUTING链。这时因为目标ip是POD-IP了，根据ip route 选择到出口网卡是tunl0。</p>
<p>可以看下内核中的路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># ip route get 10.68.70.130</div><div class="line">local 10.68.70.130 dev lo src 10.68.70.130  //这条规则指示了clusterIP是发给自己的</div><div class="line">    cache &lt;local&gt; </div><div class="line"># ip route get 172.20.185.217</div><div class="line">172.20.185.217 via 172.26.137.117 dev tunl0 src 172.20.22.192  //这条规则指示clusterIP替换成POD IP后发给本地tunl0做ipip封包</div></pre></td></tr></table></figure>
<p>于是cip变成了tunl0的IP，这个tunl0是ipip模式，于是将这个包打包成ipip，也就是外层sip、dip都是宿主机ip，再将这个包丢入到物理网络</p>
<p><img src="/images/oss/84bbd3f10de9e7ec2266a82520876c8c.png" alt=""></p>
<p>网络收包到达内核后的处理流程如下，核心都是查路由表，出包也会查路由表（判断是否本机内部通信，或者外部通信的话需要选用哪个网卡）</p>
<p>补两张内核netfilter框架的图：</p>
<p><strong>packet filtering in IPTables</strong></p>
<p><img src="/images/oss/a10e26828904310633f7bc20d587e547.png" alt="image.png"></p>
<p><a href="https://en.wikipedia.org/wiki/Iptables#/media/File:Netfilter-packet-flow.svg" target="_blank" rel="external">完整版</a>：</p>
<p><img src="/images/oss/02e4e71ea0fae4f087a233faa190d7c7.png" alt="image.png" style="zoom:150%;"></p>
<h3 id="ipvs的一些分析"><a href="#ipvs的一些分析" class="headerlink" title="ipvs的一些分析"></a>ipvs的一些分析</h3><p>ipvs是一个内核态的四层负载均衡，支持NAT以及IPIP隧道模式，但LB和RS不能跨子网，IPIP性能次之，通过ipip隧道解决跨网段传输问题，因此能够支持跨子网。而NAT模式没有限制，这也是唯一一种支持端口映射的模式。</p>
<p>但是ipvs只有NAT（也就是DNAT），NAT也俗称三角模式，要求RS和LVS 在一个二层网络，并且LVS是RS的网关，这样回包一定会到网关，网关再次做SNAT，这样client看到SNAT后的src ip是LVS ip而不是RS-ip。默认实现不支持ful-NAT，所以像公有云厂商为了适应公有云场景基本都会定制实现ful-NAT模式的lvs。</p>
<p>我们不难猜想，由于Kubernetes Service需要使用端口映射功能，因此kube-proxy必然只能使用ipvs的NAT模式。</p>
<p>如下Masq表示MASQUERADE（也就是SNAT），跟iptables里面的 MASQUERADE 是一个意思</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># ipvsadm -L -n  |grep 70.130 -A12</div><div class="line">TCP  10.68.70.130:12380 rr</div><div class="line">  -&gt; 172.20.185.217:9376          Masq    1      0          0</div></pre></td></tr></table></figure>
<h3 id="kuberletes对iptables的修改-图中黄色部分-："><a href="#kuberletes对iptables的修改-图中黄色部分-：" class="headerlink" title="kuberletes对iptables的修改(图中黄色部分)："></a>kuberletes对iptables的修改(图中黄色部分)：</h3><p><img src="/images/oss/b64e5edf67ec76613616efbd7eba20a3.png" alt="image.png"></p>
<h2 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h2><p>在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#iptables-代理模式" target="_blank" rel="external">iptables 代理模式</a>，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#ipvs-代理模式" target="_blank" rel="external">ipvs 代理模式</a></p>
<p>kube-proxy相当于service的管控方，业务流量不会走到kube-proxy，业务流量的负载均衡都是由内核层面的iptables或者ipvs来分发。</p>
<p>kube-proxy的三种模式：</p>
<p><img src="/images/oss/075e2955c5fbd08986bd34afaa5034ba.png" alt="image.png"></p>
<p><strong>一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。</strong></p>
<p>ipvs 就是用于解决在大量 Service 时，iptables 规则同步变得不可用的性能问题。与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，这也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能。</p>
<p>除了能够提升性能之外，ipvs 也提供了多种类型的负载均衡算法，除了最常见的 Round-Robin 之外，还支持最小连接、目标哈希、最小延迟等算法，能够很好地提升负载均衡的效率。</p>
<p>而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价，“将重要操作放入内核态”是提高性能的重要手段。</p>
<p><strong>IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。</strong></p>
<p>ipvs 和 iptables 都是基于 Netfilter 实现的。</p>
<p>Kubernetes 中已经使用 ipvs 作为 kube-proxy 的默认代理模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/opt/kube/bin/kube-proxy --bind-address=172.26.137.117 --cluster-cidr=172.20.0.0/16 --hostname-override=172.26.137.117 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --logtostderr=true --proxy-mode=ipvs</div></pre></td></tr></table></figure>
<p><img src="/images/oss/c44c8b3fbb1b2e0910872a6aecef790c.png" alt="image.png"></p>
<h2 id="为什么clusterIP不能ping通"><a href="#为什么clusterIP不能ping通" class="headerlink" title="为什么clusterIP不能ping通"></a>为什么clusterIP不能ping通</h2><p><a href="https://cizixs.com/2017/03/30/kubernetes-introduction-service-and-kube-proxy/" target="_blank" rel="external">集群内访问cluster ip（不能ping，只能cluster ip+port）就是在到达网卡之前被内核iptalbes做了dnat/snat</a>, cluster IP是一个虚拟ip，可以针对具体的服务固定下来，这样服务后面的pod可以随便变化。</p>
<p>iptables模式的svc会ping不通clusterIP，可以看如下iptables和route（留意：–reject-with icmp-port-unreachable）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#ping 10.96.229.40</div><div class="line">PING 10.96.229.40 (10.96.229.40) 56(84) bytes of data.</div><div class="line">^C</div><div class="line">--- 10.96.229.40 ping statistics ---</div><div class="line">2 packets transmitted, 0 received, 100% packet loss, time 999ms</div><div class="line"></div><div class="line"></div><div class="line">#iptables-save |grep 10.96.229.40</div><div class="line">-A KUBE-SERVICES -d 10.96.229.40/32 -p tcp -m comment --comment &quot;***-service:https has no endpoints&quot; -m tcp --dport 8443 -j REJECT --reject-with icmp-port-unreachable</div><div class="line"></div><div class="line">#ip route get 10.96.229.40</div><div class="line">10.96.229.40 via 11.164.219.253 dev eth0  src 11.164.219.119 </div><div class="line">    cache</div></pre></td></tr></table></figure>
<p>准确来说如果用ipvs实现的clusterIP是可以ping通的：</p>
<ul>
<li>如果用iptables 来做转发是ping不通的，因为iptables里面这条规则只处理tcp包，reject了icmp</li>
<li>ipvs实现的clusterIP都能ping通</li>
<li>ipvs下的clusterIP ping通了也不是转发到pod，ipvs负载均衡只转发tcp协议的包</li>
<li>ipvs 的clusterIP在本地配置了route路由到回环网卡，这个包是lo网卡回复的</li>
</ul>
<p>ipvs实现的clusterIP，在本地有添加路由到lo网卡</p>
<p><img src="/images/oss/1f5539eb4c5fa16b2f66f44056d80d7a.png" alt="image.png"></p>
<p>然后在本机抓包（到ipvs后端的pod上抓不到icmp包）：</p>
<p><img src="/images/oss/1caea5b0eb23a47241191d1b5d8c5001.png" alt="image.png"></p>
<p>从上面可以看出显然ipvs只会转发tcp包到后端pod，所以icmp包不会通过ipvs转发到pod上，同时在本地回环网卡lo上抓到了进去的icmp包。因为本地添加了一条路由规则，目标clusterIP被指示发到lo网卡上，lo网卡回复了这个ping包，所以通了。</p>
<h2 id="port-forward"><a href="#port-forward" class="headerlink" title="port-forward"></a>port-forward</h2><p>port-forward后外部也能够像nodePort一样访问到，但是port-forward不适合大流量，一般用于管理端口，启动的时候port-forward会固定转发到一个具体的Pod上，也没有负载均衡的能力。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#在本机监听1080端口，并转发给后端的svc/nginx-ren(总是给发给svc中的一个pod)</div><div class="line">kubectl port-forward --address 0.0.0.0 svc/nginx-ren 1080:80</div></pre></td></tr></table></figure>
<p><code>kubectl</code> looks up a Pod from the service information provided on the command line and forwards directly to a Pod rather than forwarding to the ClusterIP/Service port and allowing the cluster to load balance the service like regular service traffic.</p>
<p>The <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L225" target="_blank" rel="external">portforward.go <code>Complete</code> function</a> is where <code>kubectl portforward</code> does the first look up for a pod from options via <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L254" target="_blank" rel="external"><code>AttachablePodForObjectFn</code></a>:</p>
<p>The <code>AttachablePodForObjectFn</code> is defined as <code>attachablePodForObject</code> in <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/interface.go#L39-L40" target="_blank" rel="external">this interface</a>, then here is the <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go" target="_blank" rel="external"><code>attachablePodForObject</code> function</a>.</p>
<p>To my (inexperienced) Go eyes, it appears the <a href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go" target="_blank" rel="external"><code>attachablePodForObject</code></a> is the thing <code>kubectl</code> uses to look up a Pod to from a Service defined on the command line.</p>
<p>Then from there on everything deals with filling in the Pod specific <a href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L46-L58" target="_blank" rel="external"><code>PortForwardOptions</code></a> (which doesn’t include a service) and is passed to the kubernetes API.</p>
<h2 id="Service-和-DNS-的关系"><a href="#Service-和-DNS-的关系" class="headerlink" title="Service 和 DNS 的关系"></a>Service 和 DNS 的关系</h2><p>Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。</p>
<p>对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。</p>
<p>而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">#kubectl get pod -l app=mysql-r -o wide</div><div class="line">NAME        READY   STATUS    RESTARTS IP               NODE          </div><div class="line">mysql-r-0   2/2     Running   0        172.20.120.143   172.26.137.118</div><div class="line">mysql-r-1   2/2     Running   4        172.20.248.143   172.26.137.116</div><div class="line">mysql-r-2   2/2     Running   0        172.20.185.209   172.26.137.117</div><div class="line"></div><div class="line">/ # nslookup mysql-r-1.mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r-1.mysql-r</div><div class="line">Address 1: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</div><div class="line">/ # </div><div class="line">/ # nslookup mysql-r-2.mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r-2.mysql-r</div><div class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</div><div class="line"></div><div class="line">#如果service是headless(也就是明确指定了 clusterIP: None)</div><div class="line">/ # nslookup mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r</div><div class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</div><div class="line">Address 2: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</div><div class="line">Address 3: 172.20.120.143 mysql-r-0.mysql-r.default.svc.cluster.local</div><div class="line"></div><div class="line">#如果service 没有指定 clusterIP: None，也就是会分配一个clusterIP给集群</div><div class="line">/ # nslookup mysql-r</div><div class="line">Server:    10.68.0.2</div><div class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      mysql-r</div><div class="line">Address 1: 10.68.90.172 mysql-r.default.svc.cluster.local</div></pre></td></tr></table></figure>
<p>不是每个pod都会向DNS注册，只有：</p>
<ul>
<li>StatefulSet中的POD会向dns注册，因为他们要保证顺序行</li>
<li>POD显式指定了hostname和subdomain，说明要靠hostname/subdomain来解析</li>
<li>Headless Service代理的POD也会注册</li>
</ul>
<h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p> <code>kube-proxy</code> 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 <a href="https://jimmysong.io/kubernetes-handbook/concepts/cni.html" target="_blank" rel="external">CNI</a> 创建的外网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 <a href="https://jimmysong.io/kubernetes-handbook/concepts/ingress.html" target="_blank" rel="external">ingress</a> 这个资源对象，它由位于 Kubernetes <a href="https://jimmysong.io/kubernetes-handbook/practice/edge-node-configuration.html" target="_blank" rel="external">边缘节点</a>（这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理<strong>南北向流量</strong>，Ingress 必须对接各种 Ingress Controller 才能使用，比如 <a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="external">nginx ingress controller</a>、<a href="https://traefik.io/" target="_blank" rel="external">traefik</a>。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 <a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/" target="_blank" rel="external">nginx ingress controller</a>，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。</p>
<p>Ingress是授权入站连接到达集群服务的规则集合。 你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。 用户通过POST Ingress资源到API server的方式来请求ingress。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> internet</div><div class="line">     |</div><div class="line">[ Ingress ]</div><div class="line">--|-----|--</div><div class="line">[ Services ]</div></pre></td></tr></table></figure>
<p>可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers" target="_blank" rel="external">Ingress 控制器</a> 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。</p>
<p>Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#nodeport" target="_blank" rel="external">Service.Type=NodePort</a> 或 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#loadbalancer" target="_blank" rel="external">Service.Type=LoadBalancer</a> 类型的服务。</p>
<p>Ingress 其实不是Service的一个类型，但是它可以作用于多个Service，作为集群内部服务的入口。Ingress 能做许多不同的事，比如根据不同的路由，将请求转发到不同的Service上等等。</p>
<p><img src="/images/oss/0e100056910df8cfc45403a05838dd34.png" alt="image.png"></p>
<p> Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Ingress</div><div class="line">metadata:</div><div class="line">  name: cafe-ingress</div><div class="line">spec:</div><div class="line">  tls:</div><div class="line">  - hosts:</div><div class="line">    - cafe.example.com</div><div class="line">    secretName: cafe-secret</div><div class="line">  rules:</div><div class="line">  - host: cafe.example.com</div><div class="line">    http:</div><div class="line">      paths:</div><div class="line">      - path: /tea              --入口url路径</div><div class="line">        backend:</div><div class="line">          serviceName: tea-svc  --对应的service</div><div class="line">          servicePort: 80</div><div class="line">      - path: /coffee</div><div class="line">        backend:</div><div class="line">          serviceName: coffee-svc</div><div class="line">          servicePort: 80</div></pre></td></tr></table></figure>
<p>在实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可。然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。</p>
<p>目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。</p>
<p>一个 Ingress Controller 可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。</p>
<p>对service未来的一些探索</p>
<h2 id="eBPF（extended-Berkeley-Packet-Filter）和网络"><a href="#eBPF（extended-Berkeley-Packet-Filter）和网络" class="headerlink" title="eBPF（extended Berkeley Packet Filter）和网络"></a>eBPF（extended Berkeley Packet Filter）和网络</h2><p>eBPF 最早出现在 3.18 内核中，此后原来的 BPF 就被称为 <strong>“经典” BPF</strong>（classic BPF, cBPF），cBPF 现在基本已经废弃了。很多人知道 cBPF 是因为它是 <code>tcpdump</code> 的包过滤语言。<strong>现在，Linux 内核只运行 eBPF，内核会将加载的 cBPF 字节码 透明地转换成 eBPF 再执行</strong>。如无特殊说明，本文中所说的 BPF 都是泛指 BPF 技术。</p>
<p>2015年<strong>eBPF 添加了一个新 fast path：XDP</strong>，XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码，而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。</p>
<p>BPF 当时用于 tcpdump，在内核中尽量前面的位置抓包，它不会 crash 内核；</p>
<p>bcc 是 tracing frontend for eBPF。</p>
<p>内核添加了一个新 socket 类型 AF_XDP。它提供的能力是：在零拷贝（ zero-copy）的前提下将包从网卡驱动送到用户空间。</p>
<p>AF_XDP 提供的能力与 DPDK 有点类似，不过：</p>
<ul>
<li>DPDK 需要重写网卡驱动，需要额外维护用户空间的驱动代码。</li>
<li>AF_XDP 在复用内核网卡驱动的情况下，能达到与 DPDK 一样的性能。</li>
</ul>
<p>而且由于复用了内核基础设施，所有的网络管理工具还都是可以用的，因此非常方便， 而 DPDK 这种 bypass 内核的方案导致绝大大部分现有工具都用不了了。</p>
<p>由于所有这些操作都是发生在 XDP 层的，因此它称为 AF_XDP。插入到这里的 BPF 代码 能直接将包送到 socket。</p>
<p>Facebook 公布了生产环境 XDP+eBPF 使用案例（DDoS &amp; LB）</p>
<ul>
<li>用 XDP/eBPF 重写了原来基于 IPVS 的 L4LB，性能 10x。</li>
<li>eBPF 经受住了严苛的考验：从 2017 开始，每个进入 facebook.com 的包，都是经过了 XDP &amp; eBPF 处理的。</li>
</ul>
<p><strong>Cilium 1.6 发布</strong> 第一次支持完全干掉基于 iptables 的 kube-proxy，全部功能基于 eBPF。Cilium 1.8 支持基于 XDP 的 Service 负载均衡和 host network policies。</p>
<p>传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：</p>
<p><img src="/images/oss/67851ecb88fca18b9745dae4948947a5.png" alt="image.png"></p>
<p>步骤：</p>
<ol>
<li>网卡收到一个包（通过 DMA 放到 ring-buffer）。</li>
<li>包经过 XDP hook 点。</li>
<li>内核给包分配内存，此时才有了大家熟悉的 skb（包的内核结构体表示），然后 送到内核协议栈。</li>
<li>包经过 GRO 处理，对分片包进行重组。</li>
<li>包进入 tc（traffic control）的 ingress hook。接下来，所有橙色的框都是 Netfilter 处理点。</li>
<li>Netfilter：在 PREROUTING hook 点处理 raw table 里的 iptables 规则。</li>
<li>包经过内核的连接跟踪（conntrack）模块。</li>
<li>Netfilter：在 PREROUTING hook 点处理 mangle table 的 iptables 规则。</li>
<li>Netfilter：在 PREROUTING hook 点处理 nat table 的 iptables 规则。</li>
<li>进行路由判断（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。</li>
<li>Netfilter：在 FORWARD hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 FORWARD hook 点处理 filter table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 nat table 里的iptables 规则。</li>
<li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。</li>
<li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：发送到一个本机 veth 设备，或者一个本机 service endpoint， 或者，如果目的 IP 是主机外，就通过网卡发出去。</li>
</ol>
<h3 id="Cilium-如何处理POD之间的流量（东西向流量）"><a href="#Cilium-如何处理POD之间的流量（东西向流量）" class="headerlink" title="Cilium 如何处理POD之间的流量（东西向流量）"></a>Cilium 如何处理POD之间的流量（东西向流量）</h3><p><img src="/images/oss/f6efb2e51abbd2c88a099ee9dc942d37.png" alt="image.png"></p>
<p>如上图所示，Socket 层的 BPF 程序主要处理 Cilium 节点的东西向流量（E-W）。</p>
<ul>
<li>将 Service 的 IP:Port 映射到具体的 backend pods，并做负载均衡。</li>
<li>当应用发起 connect、sendmsg、recvmsg 等请求（系统调用）时，拦截这些请求， 并根据请求的IP:Port 映射到后端 pod，直接发送过去。反向进行相反的变换。</li>
</ul>
<p>这里实现的好处：性能更高。</p>
<ul>
<li>不需要包级别（packet leve）的地址转换（NAT）。在系统调用时，还没有创建包，因此性能更高。</li>
<li>省去了 kube-proxy 路径中的很多中间节点（intermediate node hops） 可以看出，应用对这种拦截和重定向是无感知的（符合 Kubernetes Service 的设计）。</li>
</ul>
<h3 id="Cilium处理外部流量（南北向流量）"><a href="#Cilium处理外部流量（南北向流量）" class="headerlink" title="Cilium处理外部流量（南北向流量）"></a>Cilium处理外部流量（南北向流量）</h3><p><img src="/images/oss/e013d356145d1be6d6a69e2f1b32bdc8.png" alt="image.png"></p>
<p>集群外来的流量到达 node 时，由 XDP 和 tc 层的 BPF 程序进行处理， 它们做的事情与 socket 层的差不多，将 Service 的 IP:Port 映射到后端的 PodIP:Port，如果 backend pod 不在本 node，就通过网络再发出去。发出去的流程我们 在前面 Cilium eBPF 包转发路径 讲过了。</p>
<p>这里 BPF 做的事情：执行 DNAT。这个功能可以在 XDP 层做，也可以在 TC 层做，但 在XDP 层代价更小，性能也更高。</p>
<p>总结起来，Cilium的核心理念就是：</p>
<ul>
<li>将东西向流量放在离 socket 层尽量近的地方做。</li>
<li>将南北向流量放在离驱动（XDP 和 tc）层尽量近的地方做。</li>
</ul>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p>测试环境：两台物理节点，一个发包，一个收包，收到的包做 Service loadbalancing 转发给后端 Pods。</p>
<p><img src="/images/oss/1b69dfd206a91dc4007781163fd55f41.png" alt="image.png"></p>
<p>可以看出：</p>
<ul>
<li>Cilium XDP eBPF 模式能处理接收到的全部 10Mpps（packets per second）。</li>
<li>Cilium tc eBPF 模式能处理 3.5Mpps。</li>
<li>kube-proxy iptables 只能处理 2.3Mpps，因为它的 hook 点在收发包路径上更后面的位置。</li>
<li>kube-proxy ipvs 模式这里表现更差，它相比 iptables 的优势要在 backend 数量很多的时候才能体现出来。</li>
</ul>
<p>cpu：</p>
<ul>
<li>XDP 性能最好，是因为 XDP BPF 在驱动层执行，不需要将包 push 到内核协议栈。</li>
<li>kube-proxy 不管是 iptables 还是 ipvs 模式，都在处理软中断（softirq）上消耗了大量 CPU。</li>
</ul>
<h2 id="标签和选择算符"><a href="#标签和选择算符" class="headerlink" title="标签和选择算符"></a>标签和选择算符</h2><p><em>标签（Labels）</em> 是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。</p>
<h3 id="标签选择符"><a href="#标签选择符" class="headerlink" title="标签选择符"></a>标签选择符</h3><p>selector要和template中的labels一致</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">spec:</div><div class="line">  serviceName: &quot;nginx-test&quot;</div><div class="line">  replicas: 2</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: ren</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: web</div></pre></td></tr></table></figure>
<p>selector就是要找别人的label和自己匹配的，label是给别人来寻找的。如下case，svc中的 Selector:                 app=ren 是表示这个svc要绑定到app=ren的deployment/statefulset上.</p>
<p>被 selector 选中的 Pod，就称为 Service 的 Endpoints</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">[root@poc117 mysql-cluster]# kubectl describe svc nginx-ren </div><div class="line">Name:                     nginx-ren</div><div class="line">Namespace:                default</div><div class="line">Labels:                   app=web</div><div class="line">Annotations:              &lt;none&gt;</div><div class="line">Selector:                 app=ren</div><div class="line">Type:                     NodePort</div><div class="line">IP:                       10.68.34.173</div><div class="line">Port:                     &lt;unset&gt;  8080/TCP</div><div class="line">TargetPort:               80/TCP</div><div class="line">NodePort:                 &lt;unset&gt;  30080/TCP</div><div class="line">Endpoints:                172.20.22.226:80,172.20.56.169:80</div><div class="line">Session Affinity:         None</div><div class="line">External Traffic Policy:  Cluster</div><div class="line">Events:                   &lt;none&gt;</div><div class="line">[root@poc117 mysql-cluster]# kubectl get svc -l app=web</div><div class="line">NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</div><div class="line">nginx-ren   NodePort   10.68.34.173   &lt;none&gt;        8080:30080/TCP   13m</div><div class="line">[root@poc117 mysql-cluster]# kubectl get svc -l app=ren</div><div class="line">No resources found in default namespace.</div><div class="line">[root@poc117 mysql-cluster]# kubectl get svc -l app=web</div><div class="line">NAME        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</div><div class="line">nginx-ren   NodePort   10.68.34.173   &lt;none&gt;        8080:30080/TCP   14m</div></pre></td></tr></table></figure>
<h2 id="service-mesh"><a href="#service-mesh" class="headerlink" title="service mesh"></a>service mesh</h2><ul>
<li>Kubernetes 的本质是应用的生命周期管理，具体来说就是部署和管理（扩缩容、自动恢复、发布）。</li>
<li>Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。</li>
<li>Service Mesh 的基础是透明代理，通过 sidecar proxy 拦截到微服务间流量后再通过控制平面配置管理微服务的行为。</li>
<li>Service Mesh 将流量管理从 Kubernetes 中解耦，Service Mesh 内部的流量无需 <code>kube-proxy</code> 组件的支持，通过为更接近微服务应用层的抽象，管理服务间的流量、安全性和可观察性。</li>
<li>xDS 定义了 Service Mesh 配置的协议标准。</li>
<li>Service Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。</li>
</ul>
<h3 id="Sidecar-注入及流量劫持步骤概述"><a href="#Sidecar-注入及流量劫持步骤概述" class="headerlink" title="Sidecar 注入及流量劫持步骤概述"></a>Sidecar 注入及流量劫持步骤概述</h3><p>下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。</p>
<p><strong>1.</strong> Kubernetes 通过 Admission Controller 自动注入，或者用户使用 <code>istioctl</code> 命令手动注入 sidecar 容器。</p>
<p><strong>2.</strong> 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。</p>
<p><strong>3.</strong> 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式） 将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。</p>
<p><strong>4.</strong> 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考<a href="https://zhaohuabing.com/post/2018-09-25-istio-traffic-management-impl-intro/#通过管理接口获取完整配置" target="_blank" rel="external">通过管理接口获取完整配置</a>。</p>
<p><strong>5.</strong> 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。</p>
<p><strong>6.</strong> Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/" target="_blank" rel="external">https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/</a> Kubernetes 网络疑难杂症排查方法</p>
<p><a href="https://blog.csdn.net/qq_36183935/article/details/90734936" target="_blank" rel="external">https://blog.csdn.net/qq_36183935/article/details/90734936</a>  kube-proxy ipvs模式详解</p>
<p><a href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/ebpf-and-k8s-zh/</a>  大规模微服务利器：eBPF 与 Kubernetes</p>
<p><a href="http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/</a>  Life of a Packet in Cilium：实地探索 Pod-to-Service 转发路径及 BPF 处理逻辑</p>
<p><a href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/" target="_blank" rel="external">http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/</a>  深入理解 Cilium 的 eBPF 收发包路径（datapath）（KubeCon, 2019）</p>
<p><a href="https://jiayu0x.com/2014/12/02/iptables-essential-summary/" target="_blank" rel="external">https://jiayu0x.com/2014/12/02/iptables-essential-summary/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/21/kubernetes 多集群管理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/21/kubernetes 多集群管理/" itemprop="url">kubernetes 多集群管理</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-21T17:30:03+08:00">
                2020-01-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-多集群管理"><a href="#kubernetes-多集群管理" class="headerlink" title="kubernetes 多集群管理"></a>kubernetes 多集群管理</h1><h2 id="kubectl-管理多集群"><a href="#kubectl-管理多集群" class="headerlink" title="kubectl 管理多集群"></a>kubectl 管理多集群</h2><p>指定config配置文件的方式访问不同的集群</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes</div></pre></td></tr></table></figure>
<p>一个kubectl可以管理多个集群，主要是 ~/.kube/config 里面的配置，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">clusters:</div><div class="line">- cluster:</div><div class="line">    certificate-authority: /root/k8s-cluster.ca</div><div class="line">    server: https://192.168.0.80:6443</div><div class="line">  name: context-az1</div><div class="line">- cluster:</div><div class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCQl0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</div><div class="line">    server: https://192.168.0.97:6443</div><div class="line">  name: context-az3</div><div class="line"></div><div class="line">- context:</div><div class="line">    cluster: context-az1</div><div class="line">    namespace: default</div><div class="line">    user: az1-admin</div><div class="line">  name: az1</div><div class="line">- context:</div><div class="line">    cluster: context-az3</div><div class="line">    namespace: default</div><div class="line">    user: az3-read</div><div class="line">  name: az3</div><div class="line">current-context: az3  //当前使用的集群</div><div class="line"></div><div class="line">kind: Config</div><div class="line">preferences: &#123;&#125;</div><div class="line">users:</div><div class="line">- name: az1-admin</div><div class="line">  user:</div><div class="line">    client-certificate: /root/k8s.crt  //key放在配置文件中</div><div class="line">    client-key: /root/k8s.key</div><div class="line">- name: az3-read</div><div class="line">  user:</div><div class="line">    client-certificate-data: LS0tLS1CRUQ0FURS0tLS0tCg==</div><div class="line">    client-key-data: LS0tLS1CRUdJThuL2VPM0YxSWpEcXBQdmRNbUdiU2c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=</div></pre></td></tr></table></figure>
<p>多个集群中切换的话 ： kubectl config use-context az3</p>
<h3 id="快速合并两个cluster"><a href="#快速合并两个cluster" class="headerlink" title="快速合并两个cluster"></a>快速合并两个cluster</h3><p>简单来讲就是把两个集群的 .kube/config 文件合并，注意context、cluster name别重复了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 必须提前保证两个config文件中的cluster、context名字不能重复</div><div class="line">export KUBECONFIG=~/.kube/config:~/someotherconfig </div><div class="line">kubectl config view --flatten</div><div class="line"></div><div class="line">#激活这个上下文</div><div class="line">kubectl config use-context az1 </div><div class="line"></div><div class="line">#查看所有context</div><div class="line">kubectl config get-contexts </div><div class="line">CURRENT   NAME   CLUSTER       AUTHINFO           NAMESPACE</div><div class="line">          az1    context-az1   az1-admin          default</div><div class="line">*         az2    kubernetes    kubernetes-admin   </div><div class="line">          az3    context-az3   az3-read           default</div></pre></td></tr></table></figure>
<p>背后的原理类似于这个流程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 添加集群 集群地址上一步有获取 ，需要指定ca文件，上一步有获取 </div><div class="line">kubectl config set-cluster cluster-az1 --server https://192.168.146.150:6444  --certificate-authority=/usr/program/k8s-certs/k8s-cluster.ca</div><div class="line"></div><div class="line"># 添加用户 需要指定crt，key文件，上一步有获取</div><div class="line">kubectl config set-credentials az1-admin --client-certificate=/usr/program/k8s-certs/k8s.crt --client-key=/usr/program/k8s-certs/k8s.key</div><div class="line"></div><div class="line"># 指定一个上下文的名字，我这里叫做 az1，随便你叫啥 关联刚才的用户</div><div class="line">kubectl config set-context az1 --cluster=context-az1  --namespace=default --user=az1-admin</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://coreos.com/blog/kubectl-tips-and-tricks" target="_blank" rel="external">http://coreos.com/blog/kubectl-tips-and-tricks</a></p>
<p><a href="https://stackoverflow.com/questions/46184125/how-to-merge-kubectl-config-file-with-kube-config" target="_blank" rel="external">https://stackoverflow.com/questions/46184125/how-to-merge-kubectl-config-file-with-kube-config</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/15/Linux 内存问题汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/15/Linux 内存问题汇总/" itemprop="url">Linux 内存问题汇总</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-15T16:30:03+08:00">
                2020-01-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-内存问题汇总"><a href="#Linux-内存问题汇总" class="headerlink" title="Linux 内存问题汇总"></a>Linux 内存问题汇总</h1><p>本系列有如下几篇</p>
<p><a href="/2020/01/15/Linux 内存问题汇总/">Linux 内存问题汇总</a></p>
<p><a href="/2020/11/15/Linux内存--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux内存--管理和碎片/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux内存--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux内存--零拷贝/">Linux内存–零拷贝</a></p>
<h2 id="内存使用观察"><a href="#内存使用观察" class="headerlink" title="内存使用观察"></a>内存使用观察</h2><pre><code># free -m
         total       used       free     shared    buffers     cached
Mem:          7515       1115       6400          0        189        492
-/+ buffers/cache:        432       7082
Swap:            0          0          0
</code></pre><p>其中，<a href="https://spongecaptain.cool/SimpleClearFileIO/1.%20page%20cache.html" target="_blank" rel="external">cached 列表示当前的页缓存（Page Cache）占用量</a>，buffers 列表示当前的块缓存（buffer cache）占用量。用一句话来解释：<strong>Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。</strong>页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。</p>
<p><img src="/images/oss/f8d944e2c7a8611384acb820c4471007.png" alt="image.png" style="zoom:80%;"></p>
<p><strong>上图中-/+ buffers/cache: -是指userd去掉buffers/cached后真正使用掉的内存; +是指free加上buffers和cached后真正free的内存大小。</strong></p>
<h2 id="free"><a href="#free" class="headerlink" title="free"></a>free</h2><p>free是从 /proc/meminfo 读取数据然后展示：</p>
<blockquote>
<p>buff/cache = Buffers + Cached + SReclaimable</p>
<p>Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@az1-drds-79 ~]# cat /proc/meminfo |egrep -i &quot;buff|cach|SReclai&quot;</div><div class="line">Buffers:          817764 kB</div><div class="line">Cached:         76629252 kB</div><div class="line">SwapCached:            0 kB</div><div class="line">SReclaimable:    7202264 kB</div><div class="line">[root@az1-drds-79 ~]# free -k</div><div class="line">             total       used       free     shared    buffers     cached</div><div class="line">Mem:      97267672   95522336    1745336          0     817764   76629352</div><div class="line">-/+ buffers/cache:   18075220   79192452</div><div class="line">Swap:            0          0          0</div></pre></td></tr></table></figure>
<p>在内核启动时，物理页面将加入到伙伴系统 （Buddy System）中，用户申请内存时分配，释放时回收。为了照顾慢速设备及兼顾多种 workload，Linux 将页面类型分为匿名页（Anon Page）和文件页 （Page Cache），及 swapness，使用 Page Cache 缓存文件 （慢速设备），通过 swap cache 和 swapness 交由用户根据负载特征决定内存不足时回收二者的比例。</p>
<h2 id="cached过高回收"><a href="#cached过高回收" class="headerlink" title="cached过高回收"></a>cached过高回收</h2><p>系统内存大体可分为三块，应用程序使用内存、系统Cache 使用内存（包括page cache、buffer，内核slab 等）和Free 内存。</p>
<ul>
<li>应用程序使用内存：应用使用都是虚拟内存，应用申请内存时只是分配了地址空间，并未真正分配出物理内存，等到应用真正访问内存时会触发内核的缺页中断，这时候才真正的分配出物理内存，映射到用户的地址空间，因此应用使用内存是不需要连续的，内核有机制将非连续的物理映射到连续的进程地址空间中（mmu），缺页中断申请的物理内存，内核优先给低阶碎内存。</li>
<li><p>系统Cache 使用内存：使用的也是虚拟内存，申请机制与应用程序相同。</p>
</li>
<li><p>Free 内存，未被使用的物理内存，这部分内存以4k 页的形式被管理在内核伙伴算法结构中，相邻的2^n 个物理页会被伙伴算法组织到一起，形成一块连续物理内存，所谓的阶内存就是这里的n (0&lt;= n &lt;=10)，高阶内存指的就是一块连续的物理内存，在OSS 的场景中，如果3阶内存个数比较小的情况下，如果系统有吞吐burst 就会触发Drop cache 情况。</p>
</li>
</ul>
<p>cache回收：<br>    echo 1/2/3 &gt;/proc/sys/vm/drop_caches</p>
<p>查看回收后：</p>
<pre><code>cat /proc/meminfo
</code></pre><p>手动回收系统Cache、Buffer，这个文件可以设置的值分别为1、2、3。它们所表示的含义为：</p>
<p><strong>echo 1 &gt; /proc/sys/vm/drop_caches</strong>:表示清除pagecache。</p>
<p><strong>echo 2 &gt; /proc/sys/vm/drop_caches</strong>:表示清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。</p>
<p><strong>echo 3 &gt; /proc/sys/vm/drop_caches</strong>:表示清除pagecache和slab分配器中的缓存对象。</p>
<h2 id="cached无法回收"><a href="#cached无法回收" class="headerlink" title="cached无法回收"></a>cached无法回收</h2><p>可能是正打开的文件占用了cached，比如 vim 打开了一个巨大的文件；比如 mount的 tmpfs； 比如 journald 日志等等</p>
<h3 id="通过vmtouch-查看"><a href="#通过vmtouch-查看" class="headerlink" title="通过vmtouch 查看"></a>通过<a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a> 查看</h3><pre><code># vmtouch -v test.x86_64.rpm 
test.x86_64.rpm
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 10988/10988

           Files: 1
     Directories: 0
  Resident Pages: 10988/10988  42M/42M  100%
         Elapsed: 0.000594 seconds

# ls -lh test.x86_64.rpm
-rw-r--r-- 1 root root 43M 10月  8 14:11 test.x86_64.rpm
</code></pre><p>如上，表示整个文件 test.x86_64.rpm 都被cached了，回收的话执行：</p>
<pre><code>vmtouch -e test.x86_64.rpm // 或者： echo 3 &gt;/proc/sys/vm/drop_cached
</code></pre><h3 id="遍历某个目录下的所有文件被cached了多少"><a href="#遍历某个目录下的所有文件被cached了多少" class="headerlink" title="遍历某个目录下的所有文件被cached了多少"></a>遍历某个目录下的所有文件被cached了多少</h3><pre><code># vmtouch -vt /var/log/journal/
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000011ba49-00059979e0926f43.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000152f41-00059b2c88eb4344.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-00000000000f2181-000598335fcd492f.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000129aea-000599e83996db80.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000009f171-000595a722ead670.journal
…………
           Files: 48
 Directories: 2
 Touched Pages: 468992 (1G)
 Elapsed: 13.274 seconds
</code></pre><h2 id="消失的内存"><a href="#消失的内存" class="headerlink" title="消失的内存"></a>消失的内存</h2><p>OS刚启动后就报内存不够了，什么都没跑就500G没了，cached和buffer基本没用，纯粹就是used占用高，top按内存排序没有超过0.5%的进程</p>
<p>参考： <a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">[aliyun@uos15 18:40 /u02/backup_15/leo/benchmark/run]</div><div class="line">$free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503         501           1           0           0           1</div><div class="line">Swap:            15          12           3</div><div class="line"></div><div class="line">$cat /proc/meminfo </div><div class="line">MemTotal:       528031512 kB</div><div class="line">MemFree:         1469632 kB</div><div class="line">MemAvailable:          0 kB</div><div class="line">VmallocTotal:   135290290112 kB</div><div class="line">VmallocUsed:           0 kB</div><div class="line">VmallocChunk:          0 kB</div><div class="line">Percpu:            81920 kB</div><div class="line">AnonHugePages:    950272 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:   252557   ----- 预分配太多，一个2M，加起来刚好500G了</div><div class="line">HugePages_Free:    252557</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:        517236736 kB</div><div class="line"></div><div class="line">以下是一台正常的机器对比：</div><div class="line">Percpu:            41856 kB</div><div class="line">AnonHugePages:  11442176 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:       0            ----没有做预分配</div><div class="line">HugePages_Free:        0</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:               0 kB</div><div class="line"></div><div class="line">[aliyun@uos16 18:43 /home/aliyun]</div><div class="line">$free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503          20         481           0           1         480</div><div class="line">Swap:            15           0          15</div><div class="line"></div><div class="line">对有问题的机器执行：</div><div class="line"># echo 1024 &gt; /proc/sys/vm/nr_hugepages</div><div class="line">可以看到内存恢复正常了 </div><div class="line">root@uos15:/u02/backup_15/leo/benchmark/run# free -g</div><div class="line">              total        used        free      shared  buff/cache   available</div><div class="line">Mem:            503          10         492           0           0         490</div><div class="line">Swap:            15          12           3</div><div class="line">root@uos15:/u02/backup_15/leo/benchmark/run# cat /proc/meminfo </div><div class="line">MemTotal:       528031512 kB</div><div class="line">MemFree:        516106832 kB</div><div class="line">MemAvailable:   514454408 kB</div><div class="line">VmallocTotal:   135290290112 kB</div><div class="line">VmallocUsed:           0 kB</div><div class="line">VmallocChunk:          0 kB</div><div class="line">Percpu:            81920 kB</div><div class="line">AnonHugePages:    313344 kB</div><div class="line">ShmemHugePages:        0 kB</div><div class="line">ShmemPmdMapped:        0 kB</div><div class="line">HugePages_Total:    1024</div><div class="line">HugePages_Free:     1024</div><div class="line">HugePages_Rsvd:        0</div><div class="line">HugePages_Surp:        0</div><div class="line">Hugepagesize:       2048 kB</div><div class="line">Hugetlb:         2097152 kB</div></pre></td></tr></table></figure>
<h2 id="定制内存"><a href="#定制内存" class="headerlink" title="定制内存"></a>定制内存</h2><p>物理内存700多G，要求OS只能用512G：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">24条32G的内存条，总内存768G</div><div class="line"># dmidecode -t memory |grep &quot;Size: 32 GB&quot;</div><div class="line">  Size: 32 GB</div><div class="line">…………</div><div class="line">  Size: 32 GB</div><div class="line">  Size: 32 GB</div><div class="line">root@uos15:/etc# dmidecode -t memory |grep &quot;Size: 32 GB&quot; | wc -l</div><div class="line">24</div><div class="line"></div><div class="line"># cat /boot/grub/grub.cfg  |grep 512</div><div class="line">  linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</div><div class="line">    linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</div></pre></td></tr></table></figure>
<p>​    </p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="external">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="external">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/13/kubernetes 卷和volume/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/13/kubernetes 卷和volume/" itemprop="url">kubernetes volume and storage</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-13T17:30:03+08:00">
                2020-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-volume-and-storage"><a href="#kubernetes-volume-and-storage" class="headerlink" title="kubernetes volume and storage"></a>kubernetes volume and storage</h1><p>通常部署应用需要一些永久存储，kubernetes提供了PersistentVolume （PV，实际存储）、PersistentVolumeClaim （PVC，Pod访问PV的接口）、StorageClass来支持。</p>
<p>它为 PersistentVolume 定义了 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class" target="_blank" rel="external">StorageClass 名称</a> 为 <code>manual</code>，StorageClass 名称用来将 PersistentVolumeClaim 请求绑定到该 PersistentVolume。</p>
<p>PVC是用来描述希望使用什么样的或者说是满足什么条件的存储，它的全称是Persistent Volume Claim，也就是持久化存储声明。开发人员使用这个来描述该容器需要一个什么存储。</p>
<p>PVC就相当于是容器和PV之间的一个接口，使用人员只需要和PVC打交道即可。另外你可能也会想到如果当前环境中没有合适的PV和我的PVC绑定，那么我创建的POD不就失败了么？的确是这样的，不过如果发现这个问题，那么就赶快创建一个合适的PV，那么这时候持久化存储循环控制器会不断的检查PVC和PV，当发现有合适的可以绑定之后它会自动给你绑定上然后被挂起的POD就会自动启动，而不需要你重建POD。</p>
<p>创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。 如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume，则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。<strong>PVC的大小可以小于PV的大小</strong>。</p>
<p>一旦 PV 和 PVC 绑定后，<code>PersistentVolumeClaim</code> 绑定是排他性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射。</p>
<p><strong>注意</strong>：PV必须先于POD创建，而且只能是网络存储不能属于任何Node，虽然它支持HostPath类型但由于你不知道POD会被调度到哪个Node上，所以你要定义HostPath类型的PV就要保证所有节点都要有HostPath中指定的路径。</p>
<h2 id="PV-和PVC的关系"><a href="#PV-和PVC的关系" class="headerlink" title="PV 和PVC的关系"></a>PV 和PVC的关系</h2><p>PVC就会和PV进行绑定，绑定的一些原则：</p>
<ol>
<li>PV和PVC中的spec关键字段要匹配，比如存储（storage）大小。</li>
<li>PV和PVC中的storageClassName字段必须一致，这个后面再说。</li>
<li>上面的labels中的标签只是增加一些描述，对于PVC和PV的绑定没有关系</li>
</ol>
<p>PV的accessModes：支持三种类型</p>
<ul>
<li>ReadWriteMany 多路读写，卷能被集群多个节点挂载并读写</li>
<li>ReadWriteOnce 单路读写，卷只能被单一集群节点挂载读写</li>
<li>ReadOnlyMany 多路只读，卷能被多个集群节点挂载且只能读</li>
</ul>
<p>PV状态：</p>
<ul>
<li>Available – 资源尚未被claim使用</li>
<li>Bound – 卷已经被绑定到claim了</li>
<li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li>
<li><p>Failed – 卷自动回收失败</p>
<p>PV<strong>回收Recycling</strong>—pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</p>
</li>
<li><p>保留（Retain）： 当删除与之绑定的PVC时候，这个PV被标记为released（PVC与PV解绑但还没有执行回收策略）且之前的数据依然保存在该PV上，但是该PV不可用，需要手动来处理这些数据并删除该PV。</p>
</li>
<li>删除（Delete）：当删除与之绑定的PVC时候</li>
<li>回收（Recycle）：这个在1.14版本中以及被废弃，取而代之的是推荐使用动态存储供给策略，它的功能是当删除与该PV关联的PVC时，自动删除该PV中的所有数据</li>
</ul>
<h3 id="更改-PersistentVolume-的回收策略"><a href="#更改-PersistentVolume-的回收策略" class="headerlink" title="更改 PersistentVolume 的回收策略"></a>更改 PersistentVolume 的回收策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#kubectl patch pv wordpress-data -p &apos;&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Delete&quot;&#125;&#125;&apos;</div><div class="line">persistentvolume/wordpress-data patched</div></pre></td></tr></table></figure>
<p>本地卷（hostPath）也就是LPV不支持动态供给的方式，延迟绑定，就是为了综合考虑所有因素再进行POD调度。其根本原因是动态供给是先调度POD到节点，然后动态创建PV以及绑定PVC最后运行POD；而LPV是先创建与某一节点关联的PV，然后在调度的时候综合考虑各种因素而且要包括PV在哪个节点，然后再进行调度，到达该节点后在进行PVC的绑定。也就说动态供给不考虑节点，LPV必须考虑节点。所以这两种机制有冲突导致无法在动态供给策略下使用LPV。换句话说动态供给是PV跟着POD走，而LPV是POD跟着PV走。</p>
<h2 id="PV-和-PVC"><a href="#PV-和-PVC" class="headerlink" title="PV 和 PVC"></a>PV 和 PVC</h2><p>创建 pv controller 和pvc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#cat mysql-pv.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: simple-pv-volume</div><div class="line">  labels:</div><div class="line">    type: local</div><div class="line">spec:</div><div class="line">  storageClassName: manual</div><div class="line">  capacity:</div><div class="line">    storage: 20Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  hostPath:</div><div class="line">    path: &quot;/mnt/simple&quot;</div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolumeClaim</div><div class="line">metadata:</div><div class="line">  name: pv-claim</div><div class="line">spec:</div><div class="line">  storageClassName: manual</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 20Gi</div></pre></td></tr></table></figure>
<h3 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h3><p>PV是运维人员来创建的，开发操作PVC，可是大规模集群中可能会有很多PV，如果这些PV都需要运维手动来处理这也是一件很繁琐的事情，所以就有了动态供给概念，也就是Dynamic Provisioning。而我们上面的创建的PV都是静态供给方式，也就是Static Provisioning。而动态供给的关键就是StorageClass，它的作用就是创建PV模板。</p>
<p>创建StorageClass里面需要定义PV属性比如存储类型、大小等；另外创建这种PV需要用到存储插件。最终效果是，用户提交PVC，里面指定存储类型，如果符合我们定义的StorageClass，则会为其自动创建PV并进行绑定。</p>
<p><strong>简单可以把storageClass理解为名字，只是这个名字可以重复，然后pvc和pv之间通过storageClass来绑定。</strong></p>
<p>如下case中两个pv和两个pvc的绑定就是通过storageClass(一致)来实现的（当然pvc要求的大小也必须和pv一致）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">#kubectl get pv</div><div class="line">NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS   REASON   AGE</div><div class="line">mariadb-pv       8Gi        RWO            Retain           Bound    default/data-wordpress-mariadb-0   db                      3m54s</div><div class="line">wordpress-data   10Gi       RWO            Retain           Bound    default/wordpress                  wordpress               3m54s</div><div class="line"></div><div class="line">[root@az3-k8s-11 15:35 /root/charts/bitnami/wordpress]</div><div class="line">#kubectl get pvc</div><div class="line">NAME                       STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE</div><div class="line">data-wordpress-mariadb-0   Bound    mariadb-pv       8Gi        RWO            db             4m21s</div><div class="line">wordpress                  Bound    wordpress-data   10Gi       RWO            wordpress      4m21s</div><div class="line"></div><div class="line">#cat create-pv.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: mariadb-pv</div><div class="line">spec:</div><div class="line">  capacity:</div><div class="line">    storage: 8Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  persistentVolumeReclaimPolicy: Retain</div><div class="line">  storageClassName: db</div><div class="line">  hostPath:</div><div class="line">    path: /mnt/mariadb-pv</div><div class="line"></div><div class="line">---</div><div class="line"></div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: wordpress-data</div><div class="line">spec:</div><div class="line">  capacity:</div><div class="line">    storage: 10Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  persistentVolumeReclaimPolicy: Retain</div><div class="line">  storageClassName: wordpress</div><div class="line">  hostPath:</div><div class="line">    path: /mnt/wordpress-pv</div><div class="line"></div><div class="line">----对应 pvc的定义参数：</div><div class="line">persistence:</div><div class="line">  enabled: true</div><div class="line">  storageClass: &quot;wordpress&quot;</div><div class="line">  accessMode: ReadWriteOnce</div><div class="line">  size: 10Gi</div><div class="line">  </div><div class="line">  persistence:</div><div class="line">    enabled: true</div><div class="line">    mountPath: /bitnami/mariadb</div><div class="line">    storageClass: &quot;db&quot;</div><div class="line">    annotations: &#123;&#125;</div><div class="line">    accessModes:</div><div class="line">      - ReadWriteOnce</div><div class="line">    size: 8Gi</div></pre></td></tr></table></figure>
<h4 id="定义StorageClass"><a href="#定义StorageClass" class="headerlink" title="定义StorageClass"></a>定义StorageClass</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">kind: StorageClass</div><div class="line">apiVersion: storage.k8s.io/v1</div><div class="line">metadata:</div><div class="line">  name: local-storage</div><div class="line">provisioner: kubernetes.io/no-provisioner</div><div class="line">volumeBindingMode: WaitForFirstConsumer</div></pre></td></tr></table></figure>
<h4 id="定义PVC"><a href="#定义PVC" class="headerlink" title="定义PVC"></a>定义PVC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">kind: PersistentVolumeClaim</div><div class="line">apiVersion: v1</div><div class="line">metadata:</div><div class="line">  name: local-claim</div><div class="line">spec:</div><div class="line">  accessModes:</div><div class="line">  - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 5Gi</div><div class="line">  storageClassName: local-storage</div></pre></td></tr></table></figure>
<h2 id="delete-pv-卡住"><a href="#delete-pv-卡住" class="headerlink" title="delete pv 卡住"></a>delete pv 卡住</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">#kubectl describe pv wordpress-pv</div><div class="line">Name:            wordpress-pv</div><div class="line">Labels:          &lt;none&gt;</div><div class="line">Annotations:     pv.kubernetes.io/bound-by-controller: yes</div><div class="line">Finalizers:      [kubernetes.io/pv-protection]  --- 问题在finalizers</div><div class="line">StorageClass:    </div><div class="line">Status:          Terminating (lasts 18h)</div><div class="line">Claim:           default/wordpress</div><div class="line">Reclaim Policy:  Retain</div><div class="line">Access Modes:    RWO</div><div class="line">VolumeMode:      Filesystem</div><div class="line">Capacity:        10Gi</div><div class="line">Node Affinity:   &lt;none&gt;</div><div class="line">Message:         </div><div class="line">Source:</div><div class="line">    Type:      NFS (an NFS mount that lasts the lifetime of a pod)</div><div class="line">    Server:    192.168.0.111</div><div class="line">    Path:      /mnt/wordpress-pv</div><div class="line">    ReadOnly:  false</div><div class="line">Events:        &lt;none&gt;</div><div class="line"></div><div class="line">先执行后就能自动删除了：</div><div class="line">kubectl patch pv wordpress-pv -p &apos;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;: []&#125;&#125;&apos; --type=merge</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/12/kubernetes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/12/kubernetes/" itemprop="url">kubernetes 集群部署</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-12T17:30:03+08:00">
                2020-01-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="kubernetes-集群部署"><a href="#kubernetes-集群部署" class="headerlink" title="kubernetes 集群部署"></a>kubernetes 集群部署</h1><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>系统参数修改</p>
<p>docker部署</p>
<p>kubeadm install</p>
<p><a href="https://www.kubernetes.org.cn/4256.html" target="_blank" rel="external">https://www.kubernetes.org.cn/4256.html</a> </p>
<p><a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster" target="_blank" rel="external">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a></p>
<p>镜像源被墙，可以用阿里云镜像源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 配置源</div><div class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</div><div class="line">[kubernetes]</div><div class="line">name=Kubernetes</div><div class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</div><div class="line">enabled=1</div><div class="line">gpgcheck=1</div><div class="line">repo_gpgcheck=1</div><div class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</div><div class="line">EOF</div><div class="line"></div><div class="line"># 安装</div><div class="line">yum install -y kubelet kubeadm kubectl ipvsadm</div></pre></td></tr></table></figure>
<h2 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h2><p>多网卡情况下有必要指定网卡：–apiserver-advertise-address=192.168.0.80</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 使用本地 image repository</span></div><div class="line">kubeadm init --kubernetes-version=1.18.0  --apiserver-advertise-address=192.168.0.110   --image-repository registry:5000/registry.aliyuncs.com/google_containers  --service-cidr=10.10.0.0/16 --pod-network-cidr=10.122.0.0/16 </div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> 给api-server 指定外网地址，在服务器有内网、外网多个ip的时候适用</span></div><div class="line">kubeadm init --control-plane-endpoint 外网-ip:6443 --image-repository=registry:5000/registry.aliyuncs.com/google_containers --kubernetes-version=v1.21.0  --pod-network-cidr=172.16.0.0/16</div><div class="line"><span class="meta">#</span><span class="bash">--apiserver-advertise-address=30.1.1.1，设置 apiserver 的 IP 地址，对于多网卡服务器来说很重要（比如 VirtualBox 虚拟机就用了两块网卡），可以指定 apiserver 在哪个网卡上对外提供服务。</span></div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> node join <span class="built_in">command</span></span></div><div class="line"><span class="meta">#</span><span class="bash">kubeadm token create --<span class="built_in">print</span>-join-command</span></div><div class="line">kubeadm join 192.168.0.110:6443 --token 1042rl.b4qn9iuz6xv1ri7b     --discovery-token-ca-cert-hash sha256:341a4bcfde9668077ef29211c2a151fe6e9334eea8955f645698706b3bf47a49 </div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"><span class="comment"># 查看集群配置</span></span></div><div class="line">kubectl get configmap -n kube-system kubeadm-config -o yaml</div></pre></td></tr></table></figure>
<p>将一个node设置为不可调度，隔离出来，比如master 默认是不可调度的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl cordon &lt;node-name&gt;</div><div class="line">kubectl uncordon &lt;node-name&gt;</div></pre></td></tr></table></figure>
<h2 id="kubectl-管理多集群"><a href="#kubectl-管理多集群" class="headerlink" title="kubectl 管理多集群"></a>kubectl 管理多集群</h2><p>一个kubectl可以管理多个集群，主要是 ~/.kube/config 里面的配置，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">clusters:</div><div class="line">- cluster:</div><div class="line">    certificate-authority: /root/k8s-cluster.ca</div><div class="line">    server: https://192.168.0.80:6443</div><div class="line">  name: context-az1</div><div class="line">- cluster:</div><div class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCQl0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</div><div class="line">    server: https://192.168.0.97:6443</div><div class="line">  name: context-az3</div><div class="line"></div><div class="line">- context:</div><div class="line">    cluster: context-az1</div><div class="line">    namespace: default</div><div class="line">    user: az1-admin</div><div class="line">  name: az1</div><div class="line">- context:</div><div class="line">    cluster: context-az3</div><div class="line">    namespace: default</div><div class="line">    user: az3-read</div><div class="line">  name: az3</div><div class="line">current-context: az3  //当前使用的集群</div><div class="line"></div><div class="line">kind: Config</div><div class="line">preferences: &#123;&#125;</div><div class="line">users:</div><div class="line">- name: az1-admin</div><div class="line">  user:</div><div class="line">    client-certificate: /root/k8s.crt  //key放在配置文件中</div><div class="line">    client-key: /root/k8s.key</div><div class="line">- name: az3-read</div><div class="line">  user:</div><div class="line">    client-certificate-data: LS0tLS1CRUQ0FURS0tLS0tCg==</div><div class="line">    client-key-data: LS0tLS1CRUdJThuL2VPM0YxSWpEcXBQdmRNbUdiU2c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=</div></pre></td></tr></table></figure>
<p>多个集群中切换的话 ： kubectl config use-context az3</p>
<h3 id="快速合并两个cluster"><a href="#快速合并两个cluster" class="headerlink" title="快速合并两个cluster"></a>快速合并两个cluster</h3><p>简单来讲就是把两个集群的 .kube/config 文件合并，注意context、cluster name别重复了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 必须提前保证两个config文件中的cluster、context名字不能重复</div><div class="line">export KUBECONFIG=~/.kube/config:~/someotherconfig </div><div class="line">kubectl config view --flatten</div><div class="line"></div><div class="line">#激活这个上下文</div><div class="line">kubectl config use-context az1 </div><div class="line"></div><div class="line">#查看所有context</div><div class="line">kubectl config get-contexts </div><div class="line">CURRENT   NAME   CLUSTER       AUTHINFO           NAMESPACE</div><div class="line">          az1    context-az1   az1-admin          default</div><div class="line">*         az2    kubernetes    kubernetes-admin   </div><div class="line">          az3    context-az3   az3-read           default</div></pre></td></tr></table></figure>
<p>背后的原理类似于这个流程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 添加集群 集群地址上一步有获取 ，需要指定ca文件，上一步有获取 </div><div class="line">kubectl config set-cluster cluster-az1 --server https://192.168.146.150:6444  --certificate-authority=/usr/program/k8s-certs/k8s-cluster.ca</div><div class="line"></div><div class="line"># 添加用户 需要指定crt，key文件，上一步有获取</div><div class="line">kubectl config set-credentials az1-admin --client-certificate=/usr/program/k8s-certs/k8s.crt --client-key=/usr/program/k8s-certs/k8s.key</div><div class="line"></div><div class="line"># 指定一个上下文的名字，我这里叫做 az1，随便你叫啥 关联刚才的用户</div><div class="line">kubectl config set-context az1 --cluster=context-az1  --namespace=default --user=az1-admin</div></pre></td></tr></table></figure>
<h2 id="apiserver高可用"><a href="#apiserver高可用" class="headerlink" title="apiserver高可用"></a>apiserver高可用</h2><p>默认只有一个apiserver，可以考虑用haproxy和keepalive来做一组apiserver的负载均衡：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">docker run -d --name kube-haproxy \</div><div class="line">-v /etc/haproxy:/usr/local/etc/haproxy:ro \</div><div class="line">-p 8443:8443 \</div><div class="line">-p 1080:1080 \</div><div class="line">--restart always \</div><div class="line">haproxy:1.7.8-alpine</div></pre></td></tr></table></figure>
<p>haproxy配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">#cat /etc/haproxy/haproxy.cfg </div><div class="line">global</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  maxconn 50000</div><div class="line">  uid 99</div><div class="line">  gid 99</div><div class="line">  #daemon</div><div class="line">  nbproc 1</div><div class="line">  pidfile haproxy.pid</div><div class="line"></div><div class="line">defaults</div><div class="line">  mode http</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  maxconn 50000</div><div class="line">  retries 3</div><div class="line">  timeout connect 5s</div><div class="line">  timeout client 30s</div><div class="line">  timeout server 30s</div><div class="line">  timeout check 2s</div><div class="line"></div><div class="line">listen admin_stats</div><div class="line">  mode http</div><div class="line">  bind 0.0.0.0:1080</div><div class="line">  log 127.0.0.1 local0 err</div><div class="line">  stats refresh 30s</div><div class="line">  stats uri     /haproxy-status</div><div class="line">  stats realm   Haproxy\ Statistics</div><div class="line">  stats auth    will:will</div><div class="line">  stats hide-version</div><div class="line">  stats admin if TRUE</div><div class="line"></div><div class="line">frontend k8s-https</div><div class="line">  bind 0.0.0.0:8443</div><div class="line">  mode tcp</div><div class="line">  #maxconn 50000</div><div class="line">  default_backend k8s-https</div><div class="line"></div><div class="line">backend k8s-https</div><div class="line">  mode tcp</div><div class="line">  balance roundrobin</div><div class="line">  server lab1 192.168.1.81:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div><div class="line">  server lab2 192.168.1.82:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div><div class="line">  server lab3 192.168.1.83:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</div></pre></td></tr></table></figure>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</div><div class="line"></div><div class="line">#或者老版本的calico</div><div class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</div></pre></td></tr></table></figure>
<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>在所有node节点都在一个二层网络时候，flannel提供hostgw实现，避免vxlan实现的udp封装开销，估计是目前最高效的；calico也针对L3 Fabric，推出了IPinIP的选项，利用了GRE隧道封装；因此这些插件都能适合很多实际应用场景。</p>
<p>Service cluster IP尽可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问</p>
<p>网络插件由 containernetworking-plugins rpm包来提供，一般里面会有flannel、vlan等，安装在 /usr/libexec/cni/ 下（老版本没有带calico）</p>
<p>kubelet启动参数会配置 KUBELET_NETWORK_ARGS=–network-plugin=cni –cni-conf-dir=/etc/cni/net.d –cni-bin-dir=/usr/libexec/cni </p>
<h2 id="kubectl-启动容器"><a href="#kubectl-启动容器" class="headerlink" title="kubectl 启动容器"></a>kubectl 启动容器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl run -i --tty busybox --image=registry:5000/busybox -- sh</div><div class="line">kubectl attach busybox -c busybox -i -t</div></pre></td></tr></table></figure>
<h2 id="dashboard"><a href="#dashboard" class="headerlink" title="dashboard"></a>dashboard</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl apply -f  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc7/aio/deploy/recommented.yaml</div><div class="line"></div><div class="line">#暴露 dashboard 服务端口 (recommended中如果已经定义了 30000这个nodeport，所以这个命令不需要了)</div><div class="line">kubectl port-forward -n kubernetes-dashboard  svc/kubernetes-dashboard 30000:443 --address 0.0.0.0</div></pre></td></tr></table></figure>
<p>dashboard login token：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">#kubectl describe secrets -n kubernetes-dashboard   | grep token | awk &apos;NR==3&#123;print $2&#125;&apos;</div><div class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IndRc0hiMkdpWHRwN1FObTcyeUdhOHI0eUxYLTlvODd2U0NBcU1GY0t1Sk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLXRia3o5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYzM2MzBhOS0xMjBjLTRhNmYtYjM0ZS0zM2JhMTE1OWU1OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6ZGVmYXVsdCJ9.SP4JEw0kGDmyxrtcUC3HALq99Xr99E-tie5fk4R8odLJBAYN6HxEx80RbTSnkeSMJNApbtwXBLrp4I_w48kTkr93HJFM-oxie3RVLK_mEpZBF2JcfMk6qhfz4RjPiqmG6mGyW47mmY4kQ4fgpYSmZYR4LPJmVMw5W2zo5CGhZT8rKtgmi5_ROmYpWcd2ZUORaexePgesjjKwY19bLEXFOwdsqekwEvj1_zaJhKAehF_dBdgW9foFXkbXOX0xAC0QNnKUwKPanuFOVZDg1fhyV-eyi6c9-KoTYqZMJTqZyIzscIwruIRw0oauJypcdgi7ykxAubMQ4sWEyyFafSEYWg</div></pre></td></tr></table></figure>
<p>dashboard 显示为空的话(留意报错信息，一般是用户权限，重新授权即可)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kubectl delete clusterrolebinding kubernetes-dashboard</div><div class="line">kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard --user=&quot;system:serviceaccount:kubernetes-dashboard:default&quot;</div></pre></td></tr></table></figure>
<p>其中：system:serviceaccount:kubernetes-dashboard:default 来自于报错信息中的用户名</p>
<p>默认dashboard login很快expired，可以设置不过期：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ kubectl -n kubernetes-dashboard edit deployments kubernetes-dashboard</div><div class="line">...</div><div class="line">spec:</div><div class="line">      containers:</div><div class="line">      - args:</div><div class="line">        - --auto-generate-certificates</div><div class="line">        - --token-ttl=0                //增加这行表示不expire</div><div class="line">        </div><div class="line">        --enable-skip-login            //增加这行表示不需要token 就能login，不推荐</div></pre></td></tr></table></figure>
<p>kubectl proxy –address 0.0.0.0 –accept-hosts ‘.*’</p>
<h2 id="node管理调度"><a href="#node管理调度" class="headerlink" title="node管理调度"></a>node管理调度</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">//如何优雅删除node</div><div class="line">kubectl drain my-node        # 对 my-node 节点进行清空操作，为节点维护做准备</div><div class="line">kubectl drain ky4 --ignore-daemonsets --delete-local-data # 驱逐pod</div><div class="line">kubectl delete node ky4			 # 删除node</div><div class="line"></div><div class="line">kubectl cordon my-node       # 标记 my-node 节点为不可调度</div><div class="line">kubectl uncordon my-node     # 标记 my-node 节点为可以调度</div><div class="line">kubectl top node my-node     # 显示给定节点的度量值</div><div class="line">kubectl cluster-info         # 显示主控节点和服务的地址</div><div class="line">kubectl cluster-info dump    # 将当前集群状态转储到标准输出</div><div class="line">kubectl cluster-info dump --output-directory=/path/to/cluster-state   # 将当前集群状态输出到 /path/to/cluster-state</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> 如果已存在具有指定键和效果的污点，则替换其值为指定值</span></div><div class="line">kubectl taint nodes foo dedicated=special-user:NoSchedule</div><div class="line">kubectl taint nodes poc65 node-role.kubernetes.io/master:NoSchedule-</div></pre></td></tr></table></figure>
<h3 id="地址"><a href="#地址" class="headerlink" title="地址 "></a>地址<a href="https://kubernetes.io/zh/docs/concepts/architecture/nodes/#addresses" target="_blank" rel="external"> </a></h3><p>这些字段的用法取决于你的云服务商或者物理机配置。</p>
<ul>
<li>HostName：由节点的内核设置。可以通过 kubelet 的 <code>--hostname-override</code> 参数覆盖。</li>
<li>ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。</li>
<li>InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。</li>
</ul>
<h3 id="状况"><a href="#状况" class="headerlink" title="状况"></a>状况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># kubectl get node -o wide</div><div class="line">NAME             STATUS                     ROLES    AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</div><div class="line">172.26.137.114   Ready                      master   6d1h   v1.19.0   172.26.137.114   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</div><div class="line">172.26.137.115   Ready                      node     6d1h   v1.19.0   172.26.137.115   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</div><div class="line">172.26.137.116   Ready,SchedulingDisabled   node     6d1h   v1.19.0   172.26.137.116   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</div></pre></td></tr></table></figure>
<p>如果 Ready 条件处于 <code>Unknown</code> 或者 <code>False</code> 状态的时间超过了 <code>pod-eviction-timeout</code> 值， （一个传递给 <a href="https://kubernetes.io/docs/reference/generated/kube-controller-manager/" target="_blank" rel="external">kube-controller-manager</a> 的参数）， 节点上的所有 Pod 都会被节点控制器计划删除。默认的逐出超时时长为 <strong>5 分钟</strong>。 某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。 删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。 与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。</p>
<h2 id="node-cidr-缺失"><a href="#node-cidr-缺失" class="headerlink" title="node cidr 缺失"></a>node cidr 缺失</h2><p>flannel pod 运行正常，pod无法创建，检查flannel日志发现该node cidr缺失</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">I0818 08:06:38.951132       1 main.go:733] Defaulting external v6 address to interface address (&lt;nil&gt;)</div><div class="line">I0818 08:06:38.951231       1 vxlan.go:137] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false</div><div class="line">E0818 08:06:38.951550       1 main.go:325] Error registering network: failed to acquire lease: node &quot;ky3&quot; pod cidr not assigned</div><div class="line">I0818 08:06:38.951604       1 main.go:439] Stopping shutdownHandler...</div></pre></td></tr></table></figure>
<p>正常来说describe node会看到如下的cidr信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"> Kube-Proxy Version:         v1.15.8-beta.0</div><div class="line">PodCIDR:                     172.19.1.0/24</div><div class="line">Non-terminated Pods:         (3 in total)</div></pre></td></tr></table></figure>
<p>可以手工给node添加cidr</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl patch node ky3 -p &apos;&#123;&quot;spec&quot;:&#123;&quot;podCIDR&quot;:&quot;172.19.3.0/24&quot;&#125;&#125;&apos;</div></pre></td></tr></table></figure>
<h2 id="prometheus"><a href="#prometheus" class="headerlink" title="prometheus"></a>prometheus</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/coreos/kube-prometheus.git</div><div class="line">kubectl apply -f manifests/setup</div><div class="line">kubectl apply -f manifests/</div></pre></td></tr></table></figure>
<p>暴露grafana端口：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl port-forward --address 0.0.0.0 svc/grafana -n monitoring 3000:3000</div></pre></td></tr></table></figure>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><h3 id="DRDS-deployment"><a href="#DRDS-deployment" class="headerlink" title="DRDS deployment"></a>DRDS deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: drds</div><div class="line"></div><div class="line">---</div><div class="line"></div><div class="line">apiVersion: apps/v1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: drds-deployment</div><div class="line">  namespace: drds</div><div class="line">  labels:</div><div class="line">    app: drds-server</div><div class="line">spec:</div><div class="line">  # 创建2个nginx容器</div><div class="line">  replicas: 3</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: drds-server</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: drds-server</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: drds-server</div><div class="line">        image: registry:5000/drds-image:v5_wisp_5.4.5-15940932</div><div class="line">        ports:</div><div class="line">        - containerPort: 8507</div><div class="line">        - containerPort: 8607</div><div class="line">        env:</div><div class="line">        - name: diamond_server_port</div><div class="line">          value: &quot;8100&quot;</div><div class="line">        - name: diamond_server_list</div><div class="line">          value: &quot;192.168.0.79,192.168.0.82&quot;</div><div class="line">        - name: drds_server_id</div><div class="line">          value: &quot;1&quot;</div></pre></td></tr></table></figure>
<h3 id="DRDS-Service"><a href="#DRDS-Service" class="headerlink" title="DRDS Service"></a>DRDS Service</h3><p>每个 drds 容器会通过8507提供服务，service通过3306来为一组8507做负载均衡，这个service的3306是在cluster-ip上，外部无法访问</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: drds-service</div><div class="line">  namespace: drds</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: drds-server</div><div class="line">  ports:</div><div class="line">    - protocol: TCP</div><div class="line">      port: 3306</div><div class="line">      targetPort: 8507</div></pre></td></tr></table></figure>
<p>通过node port来访问 drds service（同时会有负载均衡）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl port-forward --address 0.0.0.0 svc/drds-service -n drds 3306:3306</div></pre></td></tr></table></figure>
<h3 id="部署mysql-statefulset应用"><a href="#部署mysql-statefulset应用" class="headerlink" title="部署mysql statefulset应用"></a>部署mysql statefulset应用</h3><p>drds-pv-mysql-0 后面的mysql 会用来做存储，下面用到了三个mysql(需要三个pvc)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">#cat mysql-deployment.yaml </div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">  - port: 3306</div><div class="line">  selector:</div><div class="line">    app: mysql</div><div class="line">  clusterIP: None</div><div class="line">---</div><div class="line">apiVersion: apps/v1 </div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      app: mysql</div><div class="line">  strategy:</div><div class="line">    type: Recreate</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: mysql</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - image: mysql:5.7</div><div class="line">        name: mysql</div><div class="line">        env:</div><div class="line">          # Use secret in real usage</div><div class="line">        - name: MYSQL_ROOT_PASSWORD</div><div class="line">          value: &quot;123456&quot;</div><div class="line">        ports:</div><div class="line">        - containerPort: 3306</div><div class="line">          name: mysql</div><div class="line">        volumeMounts:</div><div class="line">        - name: mysql-persistent-storage</div><div class="line">          mountPath: /var/lib/mysql</div><div class="line">      volumes:</div><div class="line">      - name: mysql-persistent-storage</div><div class="line">        persistentVolumeClaim:</div><div class="line">          claimName: pv-claim</div></pre></td></tr></table></figure>
<p>清理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl delete deployment,svc mysql</div><div class="line">kubectl delete pvc mysql-pv-claim</div><div class="line">kubectl delete pv mysql-pv-volume</div></pre></td></tr></table></figure>
<p>查看所有pod ip以及node ip：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get pods -o wide</div></pre></td></tr></table></figure>
<h2 id="配置-Pod-使用-ConfigMap"><a href="#配置-Pod-使用-ConfigMap" class="headerlink" title="配置 Pod 使用 ConfigMap"></a>配置 Pod 使用 ConfigMap</h2><p>ConfigMap 允许你将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"># cat mysql-configmap.yaml  //mysql配置文件放入： configmap</div><div class="line">apiVersion: v1</div><div class="line">kind: ConfigMap</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">  labels:</div><div class="line">    app: mysql</div><div class="line">data:</div><div class="line">  master.cnf: |</div><div class="line">    # Apply this config only on the master.</div><div class="line">    [mysqld]</div><div class="line">    log-bin</div><div class="line"></div><div class="line">  mysqld.cnf: |</div><div class="line">    [mysqld]</div><div class="line">    pid-file        = /var/run/mysqld/mysqld.pid</div><div class="line">    socket          = /var/run/mysqld/mysqld.sock</div><div class="line">    datadir         = /var/lib/mysql</div><div class="line">    #log-error      = /var/log/mysql/error.log</div><div class="line">    # By default we only accept connections from localhost</div><div class="line">    #bind-address   = 127.0.0.1</div><div class="line">    # Disabling symbolic-links is recommended to prevent assorted security risks</div><div class="line">    symbolic-links=0</div><div class="line">   sql_mode=&apos;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&apos;</div><div class="line">    # 慢查询阈值，查询时间超过阈值时写入到慢日志中</div><div class="line">    long_query_time = 2</div><div class="line">    innodb_buffer_pool_size = 257M</div><div class="line"></div><div class="line"></div><div class="line">  slave.cnf: |</div><div class="line">    # Apply this config only on slaves.</div><div class="line">    [mysqld]</div><div class="line">    super-read-only</div><div class="line"></div><div class="line">  786  26/08/20 15:27:00 kubectl create configmap game-config-env-file --from-env-file=configure-pod-container/configmap/game-env-file.properties</div><div class="line">  787  26/08/20 15:28:10 kubectl get configmap -n kube-system kubeadm-config -o yaml</div><div class="line">  788  26/08/20 15:28:11 kubectl get configmap game-config-env-file -o yaml</div></pre></td></tr></table></figure>
<p>将mysql root密码放入secret并查看 secret密码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> cat mysql-secret.yaml</span></div><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: mysql-root-password</div><div class="line">type: Opaque</div><div class="line">data:</div><div class="line">  password: MTIz</div><div class="line"><span class="meta"></span></div><div class="line">#<span class="bash"> <span class="built_in">echo</span> -n <span class="string">'123'</span> | base64  //生成密码编码  </span></div><div class="line"><span class="meta">#</span><span class="bash"> kubectl get secret mysql-root-password -o jsonpath=<span class="string">'&#123;.data.password&#125;'</span> | base64 --decode -</span></div><div class="line"></div><div class="line">或者创建一个新的 secret：</div><div class="line">kubectl create secret generic my-secret --from-literal=password="Password"</div></pre></td></tr></table></figure>
<p>在mysql容器中使用以上configmap中的参数： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">spec:</div><div class="line">  volumes:</div><div class="line">  - name: conf</div><div class="line">    emptyDir: &#123;&#125;</div><div class="line">  - name: myconf</div><div class="line">    emptyDir: &#123;&#125;</div><div class="line">  - name: config-map</div><div class="line">    configMap:</div><div class="line">      name: mysql</div><div class="line">  initContainers:</div><div class="line">  - name: init-mysql</div><div class="line">    image: mysql:5.7</div><div class="line">    command:</div><div class="line">    - bash</div><div class="line">    - &quot;-c&quot;</div><div class="line">    - |</div><div class="line">      set -ex</div><div class="line">      # Generate mysql server-id from pod ordinal index.</div><div class="line">      [[ `hostname` =~ -([0-9]+)$ ]] || exit 1</div><div class="line">      ordinal=$&#123;BASH_REMATCH[1]&#125;</div><div class="line">      echo [mysqld] &gt; /mnt/conf.d/server-id.cnf</div><div class="line">      # Add an offset to avoid reserved server-id=0 value.</div><div class="line">      echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf</div><div class="line">      #echo &quot;innodb_buffer_pool_size=512m&quot; &gt; /mnt/rds.cnf</div><div class="line">      # Copy appropriate conf.d files from config-map to emptyDir.</div><div class="line">      #if [[ $ordinal -eq 0 ]]; then</div><div class="line">      cp /mnt/config-map/master.cnf /mnt/conf.d/</div><div class="line">      cp /mnt/config-map/mysqld.cnf /mnt/mysql.conf.d/</div><div class="line">      #else</div><div class="line">      #  cp /mnt/config-map/slave.cnf /mnt/conf.d/</div><div class="line">      #fi</div><div class="line">    volumeMounts:</div><div class="line">    - name: conf</div><div class="line">      mountPath: /mnt/conf.d</div><div class="line">    - name: myconf</div><div class="line">      mountPath: /mnt/mysql.conf.d</div><div class="line">    - name: config-map</div><div class="line">      mountPath: /mnt/config-map</div><div class="line">  containers:</div><div class="line">  - name: mysql</div><div class="line">    image: mysql:5.7</div><div class="line">    env:</div><div class="line">    #- name: MYSQL_ALLOW_EMPTY_PASSWORD</div><div class="line">    #  value: &quot;1&quot;</div><div class="line">    - name: MYSQL_ROOT_PASSWORD</div><div class="line">      valueFrom:</div><div class="line">        secretKeyRef:</div><div class="line">          name: mysql-root-password</div><div class="line">          key: password</div></pre></td></tr></table></figure>
<p><strong>通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。</strong></p>
<p>集群会自动创建一个 default-token-<em>**</em> 的secret，然后所有pod都会自动将这个 secret通过 Porjected Volume挂载到容器，也叫 ServiceAccountToken，是一种特殊的Secret</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">    Environment:    &lt;none&gt;</div><div class="line">    Mounts:</div><div class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ncgdl (ro)</div><div class="line">Conditions:</div><div class="line">  Type              Status</div><div class="line">  Initialized       True </div><div class="line">  Ready             True </div><div class="line">  ContainersReady   True </div><div class="line">  PodScheduled      True </div><div class="line">Volumes:</div><div class="line">  default-token-ncgdl:</div><div class="line">    Type:        Secret (a volume populated by a Secret)</div><div class="line">    SecretName:  default-token-ncgdl</div><div class="line">    Optional:    false</div><div class="line">QoS Class:       BestEffort</div></pre></td></tr></table></figure>
<h2 id="apply-create操作"><a href="#apply-create操作" class="headerlink" title="apply create操作"></a>apply create操作</h2><p>先 kubectl create，再 replace 的操作，我们称为命令式配置文件操作</p>
<p>kubectl apply 命令才是“声明式 API”</p>
<blockquote>
<p>kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；</p>
<p>而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。</p>
<p>kubectl set image 和 kubectl edit 也是对已有 API 对象的修改</p>
</blockquote>
<p> kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力</p>
<p>声明式 API，相当于对外界所有操作（并发接收）串行merge，才是 Kubernetes 项目编排能力“赖以生存”的核心所在</p>
<blockquote>
<p>如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。</p>
</blockquote>
<h2 id="label"><a href="#label" class="headerlink" title="label"></a>label</h2><p>给多个节点加标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">kubectl label  --overwrite=true nodes 10.0.0.172 10.0.1.192 10.0.2.48 topology.kubernetes.io/region=cn-hangzhou</div><div class="line"></div><div class="line">//查看</div><div class="line">kubectl get nodes --show-labels</div></pre></td></tr></table></figure>
<h2 id="helm"><a href="#helm" class="headerlink" title="helm"></a>helm</h2><p>Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。</p>
<p>建立local repo index：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">helm repo index [DIR] [flags]</div></pre></td></tr></table></figure>
<p>仓库只能index 到 helm package 发布后的tgz包，意义不大。每次index后需要 helm repo update</p>
<p>然后可以启动一个http服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python -m SimpleHTTPServer 8089 &amp;</div></pre></td></tr></table></figure>
<p>将local repo加入到仓库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> helm repo add local http://127.0.0.1:8089</div><div class="line"> </div><div class="line"> # helm repo list</div><div class="line">NAME 	URL                  </div><div class="line">local	http://127.0.0.1:8089</div></pre></td></tr></table></figure>
<p>install chart：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//helm3 默认不自动创建namespace，不带参数就报没有 ame 的namespace错误</div><div class="line">helm install -name wordpress -n test --create-namespace .</div><div class="line"></div><div class="line">helm list -n test</div><div class="line"></div><div class="line">&#123;&#123; .Release.Name &#125;&#125; 这种是helm内部自带的值，都是一些内建的变量，所有人都可以访问</div><div class="line"></div><div class="line">image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag | default .Chart.AppVersion &#125;&#125;&quot;  这种是我们从values.yaml文件中获取或者从命令行中获取的值。</div></pre></td></tr></table></figure>
<p>quote是一个模板方法，可以将输入的参数添加双引号</p>
<h3 id="模板片段"><a href="#模板片段" class="headerlink" title="模板片段"></a>模板片段</h3><p>之前我们看到有个文件叫做_helpers.tpl，我们介绍是说存储模板片段的地方。</p>
<p>模板片段其实也可以在文件中定义，但是为了更好管理，可以在_helpers.tpl中定义，使用时直接调用即可。</p>
<h2 id="自动补全"><a href="#自动补全" class="headerlink" title="自动补全"></a>自动补全</h2><p>kubernetes自动补全：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">source &lt;(kubectl completion bash) </div><div class="line"></div><div class="line">echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc</div></pre></td></tr></table></figure>
<p>helm自动补全：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd ~</div><div class="line">helm completion bash &gt; .helmrc &amp;&amp; echo &quot;source .helmrc&quot; &gt;&gt; .bashrc &amp;&amp; source .bashrc</div></pre></td></tr></table></figure>
<p>两者都需要依赖 auto-completion，所以得先：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># yum install -y bash-completion</div><div class="line"># source /usr/share/bash-completion/bash_completion</div></pre></td></tr></table></figure>
<p>kubectl -s polarx-test-ackk8s-atp-3826.adbgw.alibabacloud.test exec -it bushu016polarx282bc7216f-5161 bash</p>
<h2 id="启动时间排序"><a href="#启动时间排序" class="headerlink" title="启动时间排序"></a>启动时间排序</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">532  [2021-08-24 18:37:19] kubectl get po --sort-by=.status.startTime -ndrds</div><div class="line">533  [2021-08-24 18:37:41] kubectl get pods --sort-by=.metadata.creationTimestamp -ndrds</div></pre></td></tr></table></figure>
<h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><p>初始化集群的时候第一看kubelet能否起来（cgroup配置），第二就是看kubelet静态起pod，kubelet参数指定yaml目录，然后kubelet拉起这个目录下的所有yaml。</p>
<p>kubeadm启动集群就是如此。kubeadm生成证书、etcd.yaml等yaml、然后拉起kubelet，kubelet拉起etcd、apiserver等pod，kubeadm init 的时候主要是在轮询等待apiserver的起来。</p>
<p>可以通过kubelet –v 256来看详细日志，kubeadm本身所做的事情并不多，所以日志没有太多的信息，主要是等待轮询apiserver的拉起。</p>
<h3 id="Kubeadm-config"><a href="#Kubeadm-config" class="headerlink" title="Kubeadm config"></a>Kubeadm config</h3><p>Init 可以指定仓库以及版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubeadm init --image-repository=registry:5000/registry.aliyuncs.com/google_containers --kubernetes-version=v1.14.6  --pod-network-cidr=10.244.0.0/16</div></pre></td></tr></table></figure>
<p>查看并修改配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sudo kubeadm config view &gt; kubeadm-config.yaml</div><div class="line">edit kubeadm-config.yaml and replace k8s.gcr.io with your repo</div><div class="line">sudo kubeadm upgrade apply --config kubeadm-config.yaml</div><div class="line"></div><div class="line">kubeadm config images pull --config="/root/kubeadm-config.yaml"</div><div class="line"></div><div class="line">kubectl get cm -n kube-system kubeadm-config -o yaml</div></pre></td></tr></table></figure>
<p>pod镜像拉取不到的话可以在kebelet启动参数中写死pod镜像（pod_infra_container_image）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash">cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span></div><div class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --pod_infra_container_image=registry:5000/registry.aliyuncs.com/google_containers/pause:3.1</div></pre></td></tr></table></figure>
<h3 id="构建离线镜像库"><a href="#构建离线镜像库" class="headerlink" title="构建离线镜像库"></a>构建离线镜像库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubeadm config images list &gt;1.24.list</div><div class="line"></div><div class="line">cat 1.24.list | awk -F / &apos;&#123; print $0 &quot;    &quot; $3&#125;&apos; &gt; 1.24.aarch.list</div></pre></td></tr></table></figure>
<h3 id="cni-报x509-certificate-signed-by-unknown-authority"><a href="#cni-报x509-certificate-signed-by-unknown-authority" class="headerlink" title="cni 报x509: certificate signed by unknown authority"></a><a href="https://www.cnblogs.com/huiyichanmian/p/15760579.html" target="_blank" rel="external">cni 报x509: certificate signed by unknown authority</a></h3><p>一个集群下反复部署calico/flannel插件后，在 /etc/cni/net.d/ 下会有cni 网络配置文件残留，导致 flannel 创建容器网络的时候报证书错误。其实这不只是证书错误，还可能报其它cni配置错误，总之这是因为 10-calico.conflist 不符合 flannel要求所导致的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># find /etc/cni/net.d/</div><div class="line">/etc/cni/net.d/</div><div class="line">/etc/cni/net.d/calico-kubeconfig</div><div class="line">/etc/cni/net.d/10-calico.conflist   //默认读取了这个配置文件，不符合flannel</div><div class="line">/etc/cni/net.d/10-flannel.conflist</div></pre></td></tr></table></figure>
<p>因为calico 排在 flannel前面，所以即使用flannel配置文件也是用的 10-calico.conflist。每次 kubeadm reset 的时候是不会去做 cni 的reset 的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</div><div class="line">[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]</div><div class="line"></div><div class="line">The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d</div></pre></td></tr></table></figure>
<h2 id="kubernetes-API-案例"><a href="#kubernetes-API-案例" class="headerlink" title="kubernetes API 案例"></a><a href="https://mp.weixin.qq.com/s/1ouLZbw-Z7G-fKz53uJZag" target="_blank" rel="external">kubernetes API 案例</a></h2><p>用kubeadm部署kubernetes集群，会生成如下证书：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#ls /etc/kubernetes/pki/</div><div class="line">apiserver-etcd-client.crt  apiserver-kubelet-client.crt  apiserver.crt  ca.crt  etcd  front-proxy-ca.key      front-proxy-client.key  sa.pub</div><div class="line">apiserver-etcd-client.key  apiserver-kubelet-client.key  apiserver.key  ca.key  front-proxy-ca.crt  front-proxy-client.crt  sa.key</div></pre></td></tr></table></figure>
<p>curl访问api必须提供证书</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://ip:6443/apis/apps/v1/deployments</div></pre></td></tr></table></figure>
<p>/etc/kubernetes/pki/ca.crt —- CA机构</p>
<p>由CA机构签发：/etc/kubernetes/pki/apiserver-kubelet-client.crt </p>
<p><img src="/images/951413iMgBlog/640-5609125.jpeg" alt="Image"></p>
<p><a href="https://kubernetes.io/docs/reference/using-api/api-concepts/" target="_blank" rel="external">获取default namespace下的deployment</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># JWT_TOKEN_DEFAULT_DEFAULT=$(kubectl get secrets \</div><div class="line">    $(kubectl get serviceaccounts/default -o jsonpath=&apos;&#123;.secrets[0].name&#125;&apos;) \</div><div class="line">    -o jsonpath=&apos;&#123;.data.token&#125;&apos; | base64 --decode)</div><div class="line"></div><div class="line">#curl --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/apis/apps/v1/namespaces/default/deployments --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;</div><div class="line">&#123;</div><div class="line">  &quot;kind&quot;: &quot;DeploymentList&quot;,</div><div class="line">  &quot;apiVersion&quot;: &quot;apps/v1&quot;,</div><div class="line">  &quot;metadata&quot;: &#123;</div><div class="line">    &quot;resourceVersion&quot;: &quot;1233307&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;items&quot;: [</div><div class="line">    &#123;</div><div class="line">      &quot;metadata&quot;: &#123;</div><div class="line">        &quot;name&quot;: &quot;nginx-deployment&quot;,</div><div class="line"> </div><div class="line">//列出default namespace下所有的pod </div><div class="line">#curl  --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/api/v1/namespaces/default/pods --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;       </div><div class="line"></div><div class="line">//对应的kubectl生成的curl命令</div><div class="line">curl  --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -v -XGET  -H &quot;Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json&quot; -H &quot;User-Agent: kubectl/v1.23.3 (linux/arm64) kubernetes/816c97a&quot; &apos;https://11.158.239.200:6443/api/v1/namespaces/default/pods?limit=500&apos;</div></pre></td></tr></table></figure>
<p>对应地可以通过 kubectl -v 256 get pods 来看kubectl的处理过程，以及具体访问的api、参数、返回结果等。实际kubectl最终也是通过libcurl来访问的这些api。这样也不用对api-server抓包分析了。</p>
<p>或者将kube api-server 代理成普通http服务</p>
<blockquote>
<p><em># Make Kubernetes API available on localhost:8080</em><br><em># to bypass the auth step in subsequent queries:</em><br>$ kubectl proxy –port=8080 </p>
<p>然后</p>
<p>curl <a href="http://localhost:8080/api/v1/namespaces" target="_blank" rel="external">http://localhost:8080/api/v1/namespaces</a></p>
</blockquote>
<p><img src="/images/951413iMgBlog/640-5609622.png" alt="Image"></p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>用curl调用kubernetes api-server来调试，需要抓包，先在执行curl的服务器上配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export SSLKEYLOGFILE=/root/ssllog/apiserver-ssl.log</div></pre></td></tr></table></figure>
<p>然后执行tcpdump对api-server的6443端口抓包，然后将/root/ssllog/apiserver-ssl.log和抓包文件下载到本地，wireshark打开抓包文件，同时配置tls。</p>
<p>以下是个完整case（技巧指定curl的本地端口为12345，然后tcpdump只抓12345，所得的请求、response结果都会解密–如果抓api-server的6443则只能看到请求被解密）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curl --local-port 12345 --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/apis/apps/v1/namespaces/default/deployments --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;</div><div class="line"></div><div class="line">#cat $JWT_TOKEN_DEFAULT_DEFAULT eyJhbGciOiJSUzI1NiIsImtpZCI6ImlNVVFVNmxUM2t4c3Y2Q3IyT1BzV2hDZGRVSmVxTHc5RV8wUXZ4RVM5REEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJ: File name too long</div></pre></td></tr></table></figure>
<p><img src="/images/951413iMgBlog/image-20220223170008311.png" alt="image-20220223170008311"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/" target="_blank" rel="external">https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/04/获取一直FullGC下的java进程HeapDump的小技巧/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/获取一直FullGC下的java进程HeapDump的小技巧/" itemprop="url">获取一直FullGC下的java进程HeapDump的小技巧</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T17:30:03+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="获取一直FullGC下的java进程HeapDump的小技巧"><a href="#获取一直FullGC下的java进程HeapDump的小技巧" class="headerlink" title="获取一直FullGC下的java进程HeapDump的小技巧"></a>获取一直FullGC下的java进程HeapDump的小技巧</h1><p>就是小技巧，操作步骤需要查询，随手记录</p>
<ul>
<li>找到java进程，gdb attach上去， 例如 <code>gdb -p 12345</code></li>
<li>找到这个<code>HeapDumpBeforeFullGC</code>的地址（这个flag如果为true，会在FullGC之前做HeapDump，默认是false）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) p &amp;HeapDumpBeforeFullGC</div><div class="line">$2 = (&lt;data variable, no debug info&gt; *) 0x7f7d50fc660f &lt;HeapDumpBeforeFullGC&gt;</div></pre></td></tr></table></figure>
<ul>
<li>Copy 地址：0x7f7d50fc660f</li>
<li>然后把他设置为true，这样下次FGC之前就会生成一份dump文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) set *0x7f7d50fc660f = 1</div><div class="line">(gdb) quit</div></pre></td></tr></table></figure>
<ul>
<li>最后，等一会，等下次FullGC触发，你就有HeapDump了！<br>(如果没有指定heapdump的名字，默认是 java_pidxxx.hprof)</li>
</ul>
<p>(PS. <code>jstat -gcutil pid</code> 可以查看gc的概况)</p>
<p>(操作完成后记得gdb上去再设置回去，不然可能一直fullgc，导致把磁盘打满).</p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>在jvm还有响应的时候可以： jinfo -flag +HeapDumpBeforeFullGC pid 设置HeapDumpBeforeFullGC 为true（- 为false，+-都不要为只打印值）</p>
<p>kill -3 产生coredump  存放在 kernel.core_pattern=/root/core （/etc/sysctl.conf , 先 ulimit -c unlimited；或者 gcore id 获取coredump)</p>
<p>得到core文件后，采用 gdb -c 执行文件 core文件 进入调试模式，对于java，有以下2个技巧：</p>
<p>进入gdb调试模式后，输入如下命令： info threads，观察异常的线程，定位到异常的线程后，则可以输入如下命令：thread 线程编号，则会打印出当前java代码的工作流程。</p>
<p> 而对于这个core，亦可以用jstack jmap打印出堆信息，线程信息，具体命令：</p>
<p>  jmap -heap 执行文件 core文件   jstack -F -l 执行文件 core文件</p>
<p><strong>容器中的进程的话需要到宿主机操作，并且将容器中的 jdk文件夹复制到宿主机对应的位置。</strong></p>
<p>  <strong>ps auxff |grep 容器id -A10 找到JVM在宿主机上的进程id</strong></p>
<h2 id="coredump"><a href="#coredump" class="headerlink" title="coredump"></a>coredump</h2><p>kill -3 产生coredump  存放在 kernel.core_pattern=/root/core （/etc/sysctl.conf , 先 ulimit -c unlimited；）</p>
<p>或者 gcore id 获取coredump</p>
<p><a href="https://www.baeldung.com/linux/managing-core-dumps" target="_blank" rel="external">coredump 所在位置</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$cat /proc/sys/kernel/core_pattern</div><div class="line">/home/admin/</div></pre></td></tr></table></figure>
<h3 id="coredump-分析"><a href="#coredump-分析" class="headerlink" title="coredump 分析"></a><a href="https://zhuanlan.zhihu.com/p/46605905" target="_blank" rel="external">coredump 分析</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">//打开 coredump</div><div class="line">$gdb /opt/taobao/java/bin/java core.24086</div><div class="line">[New LWP 27184]</div><div class="line">[New LWP 27186]</div><div class="line">[New LWP 24086]</div><div class="line">[Thread debugging using libthread_db enabled]</div><div class="line">Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.</div><div class="line">Core was generated by `/opt/tt/java_coroutine/bin/java&apos;.</div><div class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">Missing separate debuginfos, use: debuginfo-install jdk-8.9.14-20200203164153.alios7.x86_64</div><div class="line">(gdb) info threads  //查看所有thread</div><div class="line">  Id   Target Id         Frame</div><div class="line">  583  Thread 0x7f2fa56177c0 (LWP 24086) 0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</div><div class="line">  582  Thread 0x7f2f695f3700 (LWP 27186) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  581  Thread 0x7f2f6cbfb700 (LWP 27184) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  580  Thread 0x7f2f691ef700 (LWP 27176) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  579  Thread 0x7f2f698f6700 (LWP 27174) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  </div><div class="line"></div><div class="line">(gdb) thread apply all bt  //查看所有线程堆栈</div><div class="line">Thread 583 (Thread 0x7f2fa56177c0 (LWP 24086)):</div><div class="line">#0  0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</div><div class="line">#1  0x00007f2fa4b85085 in ContinueInNewThread0 (continuation=continuation@entry=0x7f2fa4b7fd70 &lt;JavaMain&gt;, stack_size=1048576, args=args@entry=0x7ffe529432d0)</div><div class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1044</div><div class="line">#2  0x00007f2fa4b81877 in ContinueInNewThread (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=0x7f2fa3c163a8, mode=mode@entry=1,</div><div class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:2033</div><div class="line">#3  0x00007f2fa4b8513b in JVMInit (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, mode=mode@entry=1,</div><div class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=ret@entry=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1091</div><div class="line">#4  0x00007f2fa4b8254d in JLI_Launch (argc=0, argv=0x7f2fa3c163a8, jargc=&lt;optimized out&gt;, jargv=&lt;optimized out&gt;, appclassc=1, appclassv=0x0, fullversion=0x400885 &quot;1.8.0_232-b604&quot;,</div><div class="line">    dotversion=0x400881 &quot;1.8&quot;, pname=0x40087c &quot;java&quot;, lname=0x40087c &quot;java&quot;, javaargs=0 &apos;\000&apos;, cpwildcard=1 &apos;\001&apos;, javaw=0 &apos;\000&apos;, ergo=0)</div><div class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:304</div><div class="line">#5  0x0000000000400635 in main ()</div><div class="line"></div><div class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</div><div class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#3  0x00007f2f9343b44a in ?? ()</div><div class="line">#4  0x000000008082e778 in ?? ()</div><div class="line">#5  0x0000000000000003 in ?? ()</div><div class="line">#6  0x00007f2f88e32758 in ?? ()</div><div class="line">#7  0x00007f2f6f532800 in ?? ()</div><div class="line"></div><div class="line">(gdb) thread apply 582 bt //查看582这个线程堆栈，LWP 27186(0x6a32)对应jstack 线程10进程id</div><div class="line"></div><div class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</div><div class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#3  0x00007f2f9343b44a in ?? ()</div><div class="line">#35 0x00000000f26fc738 in ?? ()</div><div class="line">#36 0x00007f2fa51cec5b in arena_run_split_remove (arena=0x7f2f6ab09c34, chunk=0x80, run_ind=0, flag_dirty=0, flag_decommitted=&lt;optimized out&gt;, need_pages=0) at src/arena.c:398</div><div class="line">#37 0x00007f2f695f2980 in ?? ()</div><div class="line">#38 0x0000000000000001 in ?? ()</div><div class="line">#39 0x00007f2f88e32758 in ?? ()</div><div class="line">#40 0x00007f2f695f2920 in ?? ()</div><div class="line">#41 0x00007f2fa32f46b8 in CallInfo::set_common(KlassHandle, KlassHandle, methodHandle, methodHandle, CallInfo::CallKind, int, Thread*) ()</div><div class="line">   from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#42 0x00007f2f7d800000 in ?? ()</div></pre></td></tr></table></figure>
<h3 id="coredump-转-jmap-hprof"><a href="#coredump-转-jmap-hprof" class="headerlink" title="coredump 转 jmap hprof"></a>coredump 转 jmap hprof</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jmap -dump:format=b,file=24086.hprof /opt/taobao/java/bin/java core.24086</div></pre></td></tr></table></figure>
<p>以上命令输入是 core.24086 这个 coredump，输出是一个 jmap 的dump 24086.hprof</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">$jmap -J-d64 /opt/taobao/java/bin/java core.24086</div><div class="line"></div><div class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</div><div class="line">Debugger attached successfully.</div><div class="line">Server compiler detected.</div><div class="line">JVM version is 25.232-b604</div><div class="line">0x0000000000400000      8K      /opt/taobao/java/bin/java</div><div class="line">0x00007f2fa51be000      6679K   /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/libjemalloc.so.2</div><div class="line">0x00007f2fa4fa2000      138K    /lib64/libpthread.so.0</div><div class="line">0x00007f2fa4d8c000      88K     /lib64/libz.so.1</div><div class="line">0x00007f2fa4b7d000      280K    /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/jli/libjli.so</div><div class="line">0x00007f2fa4979000      18K     /lib64/libdl.so.2</div><div class="line">0x00007f2fa45ab000      2105K   /lib64/libc.so.6</div><div class="line">0x00007f2fa43a3000      42K     /lib64/librt.so.1</div><div class="line">0x00007f2fa40a1000      1110K   /lib64/libm.so.6</div><div class="line">0x00007f2fa5406000      159K    /lib64/ld-linux-x86-64.so.2</div><div class="line">0x00007f2fa2af1000      17898K  /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">0x00007f2fa25f1000      64K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libverify.so</div><div class="line">0x00007f2fa23c2000      228K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libjava.so</div><div class="line">0x00007f2fa21af000      60K     /lib64/libnss_files.so.2</div><div class="line">0x00007f2fa1fa5000      47K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libzip.so</div><div class="line">0x00007f2f80ded000      96K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnio.so</div><div class="line">0x00007f2f80bd4000      119K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnet.so</div><div class="line">0x00007f2f7e1f6000      50K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libmanagement.so</div><div class="line">0x00007f2f75dc8000      209K    /home/admin/drds-server/lib/native/libsigar-amd64-linux.so</div><div class="line">0x00007f2f6d8ad000      293K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libsunec.so</div><div class="line">0x00007f2f6d697000      86K     /lib64/libgcc_s.so.1</div><div class="line">0x00007f2f6bdf9000      30K     /lib64/libnss_dns.so.2</div><div class="line">0x00007f2f6bbdf000      107K    /lib64/libresolv.so.2</div></pre></td></tr></table></figure>
<h3 id="coredump-生成-java-stack"><a href="#coredump-生成-java-stack" class="headerlink" title="coredump 生成 java stack"></a><a href="https://www.javacodegeeks.com/2013/02/analysing-a-java-core-dump.html" target="_blank" rel="external">coredump 生成 java stack</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">jstack -J-d64 /opt/taobao/java/bin/java core.24086 </div><div class="line"></div><div class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</div><div class="line">Debugger attached successfully.</div><div class="line">Server compiler detected.</div><div class="line">JVM version is 25.232-b604</div><div class="line">Deadlock Detection:</div><div class="line"></div><div class="line">No deadlocks found.</div><div class="line"></div><div class="line">Thread 27186: (state = BLOCKED)</div><div class="line"> - sun.misc.Unsafe.park0(boolean, long) @bci=0 (Compiled frame; information may be imprecise)</div><div class="line"> - sun.misc.Unsafe.park(boolean, long) @bci=63, line=1038 (Compiled frame)</div><div class="line"> - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=176 (Compiled frame)</div><div class="line"> - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=2047 (Compiled frame)</div><div class="line"> - java.util.concurrent.LinkedBlockingQueue.take() @bci=29, line=446 (Compiled frame)</div><div class="line"> - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=149, line=1074 (Compiled frame)</div><div class="line"> - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Compiled frame)</div><div class="line"> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Compiled frame)</div><div class="line"> - java.lang.Thread.run() @bci=11, line=858 (Compiled frame)</div></pre></td></tr></table></figure>
<h3 id="gdb-coredump-with-java-symbol"><a href="#gdb-coredump-with-java-symbol" class="headerlink" title="gdb coredump with java symbol"></a><a href="https://mail.openjdk.org/pipermail/hotspot-dev/2016-May/023255.html" target="_blank" rel="external">gdb coredump with java symbol</a></h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/04/Java技巧合集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/Java技巧合集/" itemprop="url">Java 技巧合集</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T17:30:03+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Java-技巧合集"><a href="#Java-技巧合集" class="headerlink" title="Java 技巧合集"></a>Java 技巧合集</h1><h2 id="获取一直FullGC下的java进程HeapDump的小技巧"><a href="#获取一直FullGC下的java进程HeapDump的小技巧" class="headerlink" title="获取一直FullGC下的java进程HeapDump的小技巧"></a>获取一直FullGC下的java进程HeapDump的小技巧</h2><p>就是小技巧，操作步骤需要查询，随手记录</p>
<ul>
<li>找到java进程，gdb attach上去， 例如 <code>gdb -p 12345</code></li>
<li>找到这个<code>HeapDumpBeforeFullGC</code>的地址（这个flag如果为true，会在FullGC之前做HeapDump，默认是false）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) p &amp;HeapDumpBeforeFullGC</div><div class="line">$2 = (&lt;data variable, no debug info&gt; *) 0x7f7d50fc660f &lt;HeapDumpBeforeFullGC&gt;</div></pre></td></tr></table></figure>
<ul>
<li>Copy 地址：0x7f7d50fc660f</li>
<li>然后把他设置为true，这样下次FGC之前就会生成一份dump文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) set *0x7f7d50fc660f = 1</div><div class="line">(gdb) quit</div></pre></td></tr></table></figure>
<ul>
<li>最后，等一会，等下次FullGC触发，你就有HeapDump了！<br>(如果没有指定heapdump的名字，默认是 java_pidxxx.hprof)</li>
</ul>
<p>(PS. <code>jstat -gcutil pid</code> 可以查看gc的概况)</p>
<p>(操作完成后记得gdb上去再设置回去，不然可能一直fullgc，导致把磁盘打满).</p>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>在jvm还有响应的时候可以： jinfo -flag +HeapDumpBeforeFullGC pid 设置HeapDumpBeforeFullGC 为true（- 为false，+-都不要为只打印值）</p>
<p>kill -3 产生coredump  存放在 kernel.core_pattern=/root/core （/etc/sysctl.conf , 先 ulimit -c unlimited；或者 gcore id 获取coredump)</p>
<p>得到core文件后，采用 gdb -c 执行文件 core文件 进入调试模式，对于java，有以下2个技巧：</p>
<p>进入gdb调试模式后，输入如下命令： info threads，观察异常的线程，定位到异常的线程后，则可以输入如下命令：thread 线程编号，则会打印出当前java代码的工作流程。</p>
<p> 而对于这个core，亦可以用jstack jmap打印出堆信息，线程信息，具体命令：</p>
<p>  jmap -heap 执行文件 core文件   jstack -F -l 执行文件 core文件</p>
<p><strong>容器中的进程的话需要到宿主机操作，并且将容器中的 jdk文件夹复制到宿主机对应的位置。</strong></p>
<p>  <strong>ps auxff |grep 容器id -A10 找到JVM在宿主机上的进程id</strong></p>
<h2 id="coredump"><a href="#coredump" class="headerlink" title="coredump"></a>coredump</h2><blockquote>
<p>Coredump叫做核心转储，它是进程运行时在突然崩溃的那一刻的一个内存快照。操作系统在程序发生异常而异常在进程内部又没有被捕获的情况下，会把进程此刻内存、寄存器状态、运行堆栈等信息转储保存在一个文件里。</p>
</blockquote>
<p>kill -3 产生coredump  存放在 kernel.core_pattern=/root/core （/etc/sysctl.conf , 先 ulimit -c unlimited；）</p>
<p>或者 gcore id 获取coredump</p>
<p><a href="https://www.baeldung.com/linux/managing-core-dumps" target="_blank" rel="external">coredump 所在位置</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$cat /proc/sys/kernel/core_pattern</div><div class="line">/home/admin/</div></pre></td></tr></table></figure>
<h3 id="coredump-分析"><a href="#coredump-分析" class="headerlink" title="coredump 分析"></a><a href="https://zhuanlan.zhihu.com/p/46605905" target="_blank" rel="external">coredump 分析</a></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">//打开 coredump</div><div class="line">$gdb /opt/taobao/java/bin/java core.24086</div><div class="line">[New LWP 27184]</div><div class="line">[New LWP 27186]</div><div class="line">[New LWP 24086]</div><div class="line">[Thread debugging using libthread_db enabled]</div><div class="line">Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.</div><div class="line">Core was generated by `/opt/tt/java_coroutine/bin/java&apos;.</div><div class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">Missing separate debuginfos, use: debuginfo-install jdk-8.9.14-20200203164153.alios7.x86_64</div><div class="line">(gdb) info threads  //查看所有thread</div><div class="line">  Id   Target Id         Frame</div><div class="line">  583  Thread 0x7f2fa56177c0 (LWP 24086) 0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</div><div class="line">  582  Thread 0x7f2f695f3700 (LWP 27186) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  581  Thread 0x7f2f6cbfb700 (LWP 27184) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  580  Thread 0x7f2f691ef700 (LWP 27176) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  579  Thread 0x7f2f698f6700 (LWP 27174) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">  </div><div class="line"></div><div class="line">(gdb) thread apply all bt  //查看所有线程堆栈</div><div class="line">Thread 583 (Thread 0x7f2fa56177c0 (LWP 24086)):</div><div class="line">#0  0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</div><div class="line">#1  0x00007f2fa4b85085 in ContinueInNewThread0 (continuation=continuation@entry=0x7f2fa4b7fd70 &lt;JavaMain&gt;, stack_size=1048576, args=args@entry=0x7ffe529432d0)</div><div class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1044</div><div class="line">#2  0x00007f2fa4b81877 in ContinueInNewThread (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=0x7f2fa3c163a8, mode=mode@entry=1,</div><div class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:2033</div><div class="line">#3  0x00007f2fa4b8513b in JVMInit (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, mode=mode@entry=1,</div><div class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=ret@entry=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1091</div><div class="line">#4  0x00007f2fa4b8254d in JLI_Launch (argc=0, argv=0x7f2fa3c163a8, jargc=&lt;optimized out&gt;, jargv=&lt;optimized out&gt;, appclassc=1, appclassv=0x0, fullversion=0x400885 &quot;1.8.0_232-b604&quot;,</div><div class="line">    dotversion=0x400881 &quot;1.8&quot;, pname=0x40087c &quot;java&quot;, lname=0x40087c &quot;java&quot;, javaargs=0 &apos;\000&apos;, cpwildcard=1 &apos;\001&apos;, javaw=0 &apos;\000&apos;, ergo=0)</div><div class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:304</div><div class="line">#5  0x0000000000400635 in main ()</div><div class="line"></div><div class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</div><div class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#3  0x00007f2f9343b44a in ?? ()</div><div class="line">#4  0x000000008082e778 in ?? ()</div><div class="line">#5  0x0000000000000003 in ?? ()</div><div class="line">#6  0x00007f2f88e32758 in ?? ()</div><div class="line">#7  0x00007f2f6f532800 in ?? ()</div><div class="line"></div><div class="line">(gdb) thread apply 582 bt //查看582这个线程堆栈，LWP 27186(0x6a32)对应jstack 线程10进程id</div><div class="line"></div><div class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</div><div class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</div><div class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#3  0x00007f2f9343b44a in ?? ()</div><div class="line">#35 0x00000000f26fc738 in ?? ()</div><div class="line">#36 0x00007f2fa51cec5b in arena_run_split_remove (arena=0x7f2f6ab09c34, chunk=0x80, run_ind=0, flag_dirty=0, flag_decommitted=&lt;optimized out&gt;, need_pages=0) at src/arena.c:398</div><div class="line">#37 0x00007f2f695f2980 in ?? ()</div><div class="line">#38 0x0000000000000001 in ?? ()</div><div class="line">#39 0x00007f2f88e32758 in ?? ()</div><div class="line">#40 0x00007f2f695f2920 in ?? ()</div><div class="line">#41 0x00007f2fa32f46b8 in CallInfo::set_common(KlassHandle, KlassHandle, methodHandle, methodHandle, CallInfo::CallKind, int, Thread*) ()</div><div class="line">   from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">#42 0x00007f2f7d800000 in ?? ()</div></pre></td></tr></table></figure>
<p>以上堆栈涉及到Java代码部分都是看不到函数，需要进一步把Java 符号替换进去</p>
<h3 id="coredump-转-jmap-hprof"><a href="#coredump-转-jmap-hprof" class="headerlink" title="coredump 转 jmap hprof"></a>coredump 转 jmap hprof</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jmap -dump:format=b,file=24086.hprof /opt/taobao/java/bin/java core.24086</div></pre></td></tr></table></figure>
<p>以上命令输入是 core.24086 这个 coredump，输出是一个 jmap 的dump 24086.hprof</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">$jmap -J-d64 /opt/taobao/java/bin/java core.24086</div><div class="line"></div><div class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</div><div class="line">Debugger attached successfully.</div><div class="line">Server compiler detected.</div><div class="line">JVM version is 25.232-b604</div><div class="line">0x0000000000400000      8K      /opt/taobao/java/bin/java</div><div class="line">0x00007f2fa51be000      6679K   /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/libjemalloc.so.2</div><div class="line">0x00007f2fa4fa2000      138K    /lib64/libpthread.so.0</div><div class="line">0x00007f2fa4d8c000      88K     /lib64/libz.so.1</div><div class="line">0x00007f2fa4b7d000      280K    /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/jli/libjli.so</div><div class="line">0x00007f2fa4979000      18K     /lib64/libdl.so.2</div><div class="line">0x00007f2fa45ab000      2105K   /lib64/libc.so.6</div><div class="line">0x00007f2fa43a3000      42K     /lib64/librt.so.1</div><div class="line">0x00007f2fa40a1000      1110K   /lib64/libm.so.6</div><div class="line">0x00007f2fa5406000      159K    /lib64/ld-linux-x86-64.so.2</div><div class="line">0x00007f2fa2af1000      17898K  /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</div><div class="line">0x00007f2fa25f1000      64K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libverify.so</div><div class="line">0x00007f2fa23c2000      228K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libjava.so</div><div class="line">0x00007f2fa21af000      60K     /lib64/libnss_files.so.2</div><div class="line">0x00007f2fa1fa5000      47K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libzip.so</div><div class="line">0x00007f2f80ded000      96K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnio.so</div><div class="line">0x00007f2f80bd4000      119K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnet.so</div><div class="line">0x00007f2f7e1f6000      50K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libmanagement.so</div><div class="line">0x00007f2f75dc8000      209K    /home/admin/drds-server/lib/native/libsigar-amd64-linux.so</div><div class="line">0x00007f2f6d8ad000      293K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libsunec.so</div><div class="line">0x00007f2f6d697000      86K     /lib64/libgcc_s.so.1</div><div class="line">0x00007f2f6bdf9000      30K     /lib64/libnss_dns.so.2</div><div class="line">0x00007f2f6bbdf000      107K    /lib64/libresolv.so.2</div></pre></td></tr></table></figure>
<h2 id="coredump-生成-java-stack"><a href="#coredump-生成-java-stack" class="headerlink" title="coredump 生成 java stack"></a><a href="https://www.javacodegeeks.com/2013/02/analysing-a-java-core-dump.html" target="_blank" rel="external">coredump 生成 java stack</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">jstack -J-d64 /opt/taobao/java/bin/java core.24086 </div><div class="line"></div><div class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</div><div class="line">Debugger attached successfully.</div><div class="line">Server compiler detected.</div><div class="line">JVM version is 25.232-b604</div><div class="line">Deadlock Detection:</div><div class="line"></div><div class="line">No deadlocks found.</div><div class="line"></div><div class="line">Thread 27186: (state = BLOCKED)</div><div class="line"> - sun.misc.Unsafe.park0(boolean, long) @bci=0 (Compiled frame; information may be imprecise)</div><div class="line"> - sun.misc.Unsafe.park(boolean, long) @bci=63, line=1038 (Compiled frame)</div><div class="line"> - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=176 (Compiled frame)</div><div class="line"> - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=2047 (Compiled frame)</div><div class="line"> - java.util.concurrent.LinkedBlockingQueue.take() @bci=29, line=446 (Compiled frame)</div><div class="line"> - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=149, line=1074 (Compiled frame)</div><div class="line"> - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Compiled frame)</div><div class="line"> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Compiled frame)</div><div class="line"> - java.lang.Thread.run() @bci=11, line=858 (Compiled frame)</div></pre></td></tr></table></figure>
<h2 id="gdb-coredump-with-java-symbol"><a href="#gdb-coredump-with-java-symbol" class="headerlink" title="gdb coredump with java symbol"></a><a href="https://mail.openjdk.org/pipermail/hotspot-dev/2016-May/023255.html" target="_blank" rel="external">gdb coredump with java symbol</a></h2><p>需要安装JVM debug info包，同时要求gdb版本在7.10以上</p>
<p>设置：</p>
<p>home 目录下创建 .gdbinit 然后放入如下内容，libjvm.so-gdb.py 就是 dbg.py 脚本，gdb启动的时候会自动加载这个脚本 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$cat ~/.gdbinit</div><div class="line">add-auto-load-safe-path /opt/install/jdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so-gdb.py</div></pre></td></tr></table></figure>
<p>使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gdb -iex &quot;set auto-load safe-path /&quot; /opt/install/java/bin/java ./core.24086</div></pre></td></tr></table></figure>
<h2 id="G1-GC为什么快"><a href="#G1-GC为什么快" class="headerlink" title="G1 GC为什么快"></a>G1 GC为什么快</h2><p><a href="https://ata.alibaba-inc.com/articles/199497" target="_blank" rel="external">https://ata.alibaba-inc.com/articles/199497</a></p>
<p>G1比CMS GC 暂停短、更稳定，但是最终吞吐大概率是CMS要好，这是因为G1编译后代码更大</p>
<p><code>-XX:InlineSmallCode=3000</code>告诉编译器, 汇编3000字节以内的函数需要被inline, 这个值默认是2000</p>
<p>另外CMS用的是Dirty Card，而G1 为了降低GC时间在Remeber Set（类似Dirty Card）的维护上花了更多的代价</p>
<p>Dirty Card维护代价：</p>
<ul>
<li>会影响code size<pre><code>Code size影响了inline机会
Code size增大则instruction cache miss几率变大 (几十倍的执行时间差距)
</code></pre></li>
<li>本身执行mark dirty动作耗时, 这是一个写内存+GC/mutator线程同步的操作, 可以很复杂, 也可以很简单</li>
</ul>
<p>比如G1为了降低暂停时间，就要尽量控制Remeber Set的更新，所以还需要判断write动作是否真的有必要更新Remeber Set(类似<code>old.ref = null</code>这种写操作是不需要更新Remeber Set的)</p>
<p>简单说CMS的每次 Dirty Card维护只需要3条汇编，而G1的Remember Set维护需要十多条、几十条汇编</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/02/Linux 问题总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/02/Linux 问题总结/" itemprop="url">Linux 问题总结</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-02T17:30:03+08:00">
                2020-01-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-问题总结"><a href="#Linux-问题总结" class="headerlink" title="Linux 问题总结"></a>Linux 问题总结</h1><h2 id="crond文件权限的坑"><a href="#crond文件权限的坑" class="headerlink" title="crond文件权限的坑"></a>crond文件权限的坑</h2><p>crond第一次加载的时候（刚启动）会去检查文件属性，不是644的话以后都不会执行了，即使后面chmod改成了644. </p>
<p>手工随便修改一下该文件的内容就能触发自动执行了，或者重启crond, 或者 sudo service crond reload， 或者 /etc/cron.d/下有任何修改都会触发crond reload配置(包含 touch )。</p>
<p>总之 crond会每分钟去检查job有没有change，有的话才触发reload，这个change看的时候change time有没有变化，不看权限的变化，仅仅是权限的变化不会触发crond reload。</p>
<p> crond会每分钟去检查一下job有没有修改，有修改的话会reload，但是这个<strong>修改不包含权限的修改</strong>。可以简单地理解这个修改是指文件的change time。</p>
<h2 id="cgroup目录报No-space-left-on-device"><a href="#cgroup目录报No-space-left-on-device" class="headerlink" title="cgroup目录报No space left on device"></a><a href="https://rotadev.com/cgroup-no-space-left-on-device-server-fault/" target="_blank" rel="external">cgroup目录报No space left on device</a></h2><p>可能是因为某个规则下的 cpuset.cpus 文件是空导致的</p>
<h2 id="容器中root用户执行-su-admin-切换失败"><a href="#容器中root用户执行-su-admin-切换失败" class="headerlink" title="容器中root用户执行 su - admin 切换失败"></a>容器中root用户执行 su - admin 切换失败</h2><p>问题原因：<a href="https://access.redhat.com/solutions/30316" target="_blank" rel="external">https://access.redhat.com/solutions/30316</a></p>
<p><img src="/images/oss/63a4ac6669f820156bff035e7dc49ac2.png" alt="image.png"></p>
<p>如上图去掉 admin nproc限制就可以了</p>
<p>这是因为root用户的nproc是unlimited，但是admin的是65535，所以切不过去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@i22h08323 /home/admin]</div><div class="line">#ulimit -u</div><div class="line">unlimited</div></pre></td></tr></table></figure>
<h2 id="容器中ulimit限制了sudo的执行"><a href="#容器中ulimit限制了sudo的执行" class="headerlink" title="容器中ulimit限制了sudo的执行"></a>容器中ulimit限制了sudo的执行</h2><p>容器启动的时候默认nofile为65535（可以通过 docker run –ulimit nofile=655360 来设置），如果容器中的 /etc/security/limits.conf 中设置的nofile大于 65535就会报错，因为容器的1号进程就是65535了，比如在容器中用root用户执行sudo ls报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#sudo ls</div><div class="line">sudo: pam_open_session: Permission denied</div><div class="line">sudo: policy plugin failed session initialization</div></pre></td></tr></table></figure>
<p>可以修改容器中的 ulimit 不要超过默认的65535或者修改容器的启动参数来解决。</p>
<p>子进程都会继承父进程的一些环境变量，比如 limits.conf, sudo/su/crond/passwd等都会触发重新加载limits, </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">grep -rin pam_limit /etc/pam.d //可以看到触发重新加载的场景</div></pre></td></tr></table></figure>
<h2 id="systemd-limits"><a href="#systemd-limits" class="headerlink" title="systemd limits"></a>systemd limits</h2><p>/etc/security/limits.conf 的配置，只适用于通过PAM 认证登录用户的资源限制，它对systemd 的service 的资源限制不生效。</p>
<p>因此登录用户的限制，通过/etc/security/limits.conf 与/etc/security/limits.d 下的文件设置即可。</p>
<p>对于systemd service 的资源设置，则需修改全局配置，全局配置文件放在/etc/systemd/system.conf 和/etc/systemd/user.conf，同时也会加载两个对应目录中的所有.conf 文件/etc/systemd/system.conf.d/.conf 和/etc/systemd/user.conf.d/.conf。</p>
<h2 id="open-files-限制在1024"><a href="#open-files-限制在1024" class="headerlink" title="open files 限制在1024"></a>open files 限制在1024</h2><p>docker 容器内 nofile只有1024，检查：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cat /etc/sysconfig/docker</div><div class="line">或者</div><div class="line">cat /usr/lib/systemd/system/docker.service</div><div class="line">LimitNOFILE=1048576</div><div class="line">LimitNPROC=1048576</div></pre></td></tr></table></figure>
<h3 id="关于ulimit的一些知识点"><a href="#关于ulimit的一些知识点" class="headerlink" title="关于ulimit的一些知识点"></a>关于ulimit的一些知识点</h3><p>参考 <a href="https://feichashao.com/ulimit_demo/" target="_blank" rel="external">Ulimit</a> <a href="http://blog.yufeng.info/archives/2568" target="_blank" rel="external">http://blog.yufeng.info/archives/2568</a></p>
<ul>
<li>limit的设定值是 per-process 的</li>
<li>在 Linux 中，每个普通进程可以调用 getrlimit() 来查看自己的 limits，也可以调用 setrlimit() 来改变自身的 soft limits</li>
<li>要改变 hard limit, 则需要进程有 CAP_SYS_RESOURCE 权限</li>
<li>进程 fork() 出来的子进程，会继承父进程的 limits 设定</li>
<li><code>ulimit</code> 是 shell 的内置命令。在执行<code>ulimit</code>命令时，其实是 shell 自身调用 getrlimit()/setrlimit() 来获取/改变自身的 limits. 当我们在 shell 中执行应用程序时，相应的进程就会继承当前 shell 的 limits 设定</li>
<li>shell 的初始 limits 通常是 pam_limits 设定的。顾名思义，pam_limits 是一个 PAM 模块，用户登录后，pam_limits 会给用户的 shell 设定在 limits.conf 定义的值</li>
</ul>
<p>ulimit, limits.conf 和 pam_limits模块 的关系，大致是这样的：</p>
<ol>
<li>用户进行登录，触发 pam_limits;</li>
<li>pam_limits 读取 limits.conf，相应地设定用户所获得的 shell 的 limits；</li>
<li>用户在 shell 中，可以通过 ulimit 命令，查看或者修改当前 shell 的 limits;</li>
<li>当用户在 shell 中执行程序时，该程序进程会继承 shell 的 limits 值。于是，limits 在进程中生效了</li>
</ol>
<p>判断要分配的句柄号是不是超过了 limits.conf 中 nofile 的限制。fd 是当前进程相关的，是一个从 0 开始的整数<br>结论1：soft nofile 和 fs.nr_open的作用一样，它两都是限制的单个进程的最大文件数量。区别是 soft nofile 可以按用户来配置，而 fs.nr_open 所有用户只能配一个。注意 hard nofile 一定要比 fs.nr_open 要小，否则可能导致用户无法登陆。<br>结论2：fs.file-max: 整个系统上可打开的最大文件数，但不限制 root 用户</p>
<h2 id="pam-权限报错"><a href="#pam-权限报错" class="headerlink" title="pam 权限报错"></a>pam 权限报错</h2><p><img src="/images/oss/b646979272e71e015de4a47c62b89747.png" alt="image.png"></p>
<p>从debug信息看如果是pam权限报错的话，需要将 required 改成 sufficient</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$cat /etc/pam.d/crond </div><div class="line">#</div><div class="line"># The PAM configuration file for the cron daemon</div><div class="line">#</div><div class="line">#</div><div class="line"># No PAM authentication called, auth modules not needed</div><div class="line">account    required   pam_access.so</div><div class="line">account    include    system-auth</div><div class="line">session    required   pam_loginuid.so //required 改成 sufficient</div><div class="line">session    include    system-auth</div><div class="line">auth       include    system-auth</div></pre></td></tr></table></figure>
<p>PAM 提供四个安全领域的特性，但是应用程序不太可能同时需要所有这些方面。例如，<code>passwd</code> 命令只需要下面列表中的第三组：</p>
<ul>
<li><code>account</code> 处理账户限制。对于有效的用户，允许他做什么？</li>
<li><code>auth</code> 处理用户识别 — 例如，通过输入用户名和密码。</li>
<li><code>password</code> 只处理与密码相关的问题，比如设置新密码。</li>
<li><code>session</code> 处理连接管理，包括日志记录。</li>
</ul>
<p>在 /etc/pam.d 目录中为将使用 PAM 的每个应用程序创建一个配置文件，文件名与应用程序名相同。例如，<code>login</code> 命令的配置文件是 /etc/pam.d/login。</p>
<p>必须定义将应用哪些模块，创建一个动作 “堆”。PAM 运行堆中的所有模块，根据它们的结果允许或拒绝用户的请求。还必须定义检查是否是必需的。最后，<em>other</em> 文件为没有特殊规则的所有应用程序提供默认规则。</p>
<ul>
<li><code>optional</code> 模块可以成功，也可以失败；PAM 根据模块是否最终成功返回 <code>success</code> 或 <code>failure</code>。</li>
<li><code>required</code> 模块必须成功。如果失败，PAM 返回 <code>failure</code>，但是会在运行堆中的其他模块之后返回。</li>
<li><code>requisite</code> 模块也必须成功。但是，如果失败，PAM 立即返回 <code>failure</code>，不再运行其他模块。</li>
<li><code>sufficient</code> 模块在成功时导致 PAM 立即返回 <code>success</code>，不再运行其他模块。</li>
</ul>
<p>当pam安装之后有两大部分：在/lib64/security目录下的各种pam模块以及/etc/pam.d和/etc/pam.d目录下的针对各种服务和应用已经定义好的pam配置文件。当某一个有认证需求的应用程序需要验证的时候，一般在应用程序中就会定义负责对其认证的PAM配置文件。以vsftpd为例，在它的配置文件/etc/vsftpd/vsftpd.conf中就有这样一行定义：</p>
<blockquote>
<p>pam_service_name=vsftpd</p>
</blockquote>
<p>表示登录FTP服务器的时候进行认证是根据/etc/pam.d/vsftpd文件定义的内容进行。</p>
<h3 id="PAM-认证过程"><a href="#PAM-认证过程" class="headerlink" title="PAM 认证过程"></a>PAM 认证过程</h3><p>当程序需要认证的时候已经找到相关的pam配置文件，认证过程是如何进行的？下面我们将通过解读/etc/pam.d/system-auth文件予以说明。</p>
<p>首先要声明一点的是：system-auth是一个非常重要的pam配置文件，主要负责用户登录系统的认证工作。而且该文件不仅仅只是负责用户登录系统认证，其它的程序和服务通过include接口也可以调用到它，从而节省了很多重新自定义配置的工作。所以应该说该文件是系统安全的总开关和核心的pam配置文件。</p>
<p>下面是/etc/pam.d/system-auth文件的全部内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">$cat /etc/pam.d/system-auth</div><div class="line">#%PAM-1.0</div><div class="line"># This file is auto-generated.</div><div class="line"># User changes will be destroyed the next time authconfig is run.</div><div class="line">auth        required      pam_env.so</div><div class="line">auth        required      pam_faildelay.so delay=2000000</div><div class="line">auth        sufficient    pam_unix.so nullok try_first_pass</div><div class="line">auth        requisite     pam_succeed_if.so uid &gt;= 1000 quiet_success</div><div class="line">auth        required      pam_deny.so</div><div class="line"></div><div class="line">account     required      pam_unix.so</div><div class="line">account     sufficient    pam_localuser.so</div><div class="line">account     sufficient    pam_succeed_if.so uid &lt; 1000 quiet</div><div class="line">account     required      pam_permit.so</div><div class="line"></div><div class="line">password    requisite     pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type=</div><div class="line">password    sufficient    pam_unix.so sha512 shadow nullok try_first_pass use_authtok</div><div class="line">password    required      pam_deny.so</div><div class="line"></div><div class="line">session     optional      pam_keyinit.so revoke</div><div class="line">session     required      pam_limits.so</div><div class="line">-session     optional      pam_systemd.so</div><div class="line">session     [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid</div><div class="line">session     required      pam_unix.so</div></pre></td></tr></table></figure>
<h4 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h4><p>当用户登录的时候，首先会通过auth类接口对用户身份进行识别和密码认证。所以在该过程中验证会经过几个带auth的配置项。</p>
<p>其中的第一步是通过pam_env.so模块来定义用户登录之后的环境变量， pam_env.so允许设置和更改用户登录时候的环境变量，默认情况下，若没有特别指定配置文件，将依据/etc/security/pam_env.conf进行用户登录之后环境变量的设置。</p>
<p>然后通过pam_unix.so模块来提示用户输入密码，并将用户密码与/etc/shadow中记录的密码信息进行对比，如果密码比对结果正确则允许用户登录，而且<strong>该配置项的使用的是“sufficient”控制位，即表示只要该配置项的验证通过，用户即可完全通过认证而不用再去走下面的认证项</strong>。不过在特殊情况下，用户允许使用空密码登录系统，例如当将某个用户在/etc/shadow中的密码字段删除之后，该用户可以只输入用户名直接登录系统。</p>
<p>下面的配置项中，通过pam_succeed_if.so对用户的登录条件做一些限制，表示允许uid大于500的用户在通过密码验证的情况下登录，在Linux系统中，一般系统用户的uid都在500之内，所以该项即表示允许使用useradd命令以及默认选项建立的普通用户直接由本地控制台登录系统。</p>
<p>最后通过pam_deny.so模块对所有不满足上述任意条件的登录请求直接拒绝，pam_deny.so是一个特殊的模块，该模块返回值永远为否，类似于大多数安全机制的配置准则，在所有认证规则走完之后，对不匹配任何规则的请求直接拒绝。</p>
<h4 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h4><p>三个配置项主要表示通过account账户类接口来识别账户的合法性以及登录权限。</p>
<p>第一行仍然使用pam_unix.so模块来声明用户需要通过密码认证。第二行承认了系统中uid小于500的系统用户的合法性。之后对所有类型的用户登录请求都开放控制台。</p>
<h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a>第三部分</h4><p>会通过password口令类接口来确认用户使用的密码或者口令的合法性。第一行配置项表示需要的情况下将调用pam_cracklib来验证用户密码复杂度。如果用户输入密码不满足复杂度要求或者密码错，最多将在三次这种错误之后直接返回密码错误的提示，否则期间任何一次正确的密码验证都允许登录。需要指出的是，pam_cracklib.so是一个常用的控制密码复杂度的pam模块，关于其用法举例我们会在之后详细介绍。之后带pam_unix.so和pam_deny.so的两行配置项的意思与之前类似。都表示需要通过密码认证并对不符合上述任何配置项要求的登录请求直接予以拒绝。不过用户如果执行的操作是单纯的登录，则这部分配置是不起作用的。</p>
<h4 id="第四部分"><a href="#第四部分" class="headerlink" title="第四部分"></a>第四部分</h4><p>主要将通过session会话类接口为用户初始化会话连接。其中几个比较重要的地方包括，使用pam_keyinit.so表示当用户登录的时候为其建立相应的密钥环，并在用户登出的时候予以撤销。不过该行配置的控制位使用的是optional，表示这并非必要条件。之后通过pam_limits.so限制用户登录时的会话连接资源，相关pam_limit.so配置文件是/etc/security/limits.conf，默认情况下对每个登录用户都没有限制。关于该模块的配置方法在后面也会详细介绍。</p>
<h3 id="常用的PAM模块介绍"><a href="#常用的PAM模块介绍" class="headerlink" title="常用的PAM模块介绍"></a>常用的PAM模块介绍</h3><table>
<thead>
<tr>
<th>PAM模块</th>
<th>结合管理类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>pam_unix.so</td>
<td>auth</td>
<td>提示用户输入密码,并与/etc/shadow文件相比对.匹配返回0</td>
</tr>
<tr>
<td>pam_unix.so</td>
<td>account</td>
<td>检查用户的账号信息(包括是否过期等).帐号可用时,返回0.</td>
</tr>
<tr>
<td>pam_unix.so</td>
<td>password</td>
<td>修改用户的密码. 将用户输入的密码,作为用户的新密码更新shadow文件</td>
</tr>
<tr>
<td>pam_shells.so</td>
<td>auth、account</td>
<td>如果用户想登录系统，那么它的shell必须是在/etc/shells文件中之一的shell</td>
</tr>
<tr>
<td>pam_deny.so</td>
<td>account、auth、password、session</td>
<td>该模块可用于拒绝访问</td>
</tr>
<tr>
<td>pam_permit.so</td>
<td>account、auth、password、session</td>
<td>模块任何时候都返回成功.</td>
</tr>
<tr>
<td>pam_securetty.so</td>
<td>auth</td>
<td>如果用户要以root登录时,则登录的tty必须在/etc/securetty之中.</td>
</tr>
<tr>
<td>pam_listfile.so</td>
<td>account、auth、password、session</td>
<td>访问应用程的控制开关</td>
</tr>
<tr>
<td>pam_cracklib.so</td>
<td>password</td>
<td>这个模块可以插入到一个程序的密码栈中,用于检查密码的强度.</td>
</tr>
<tr>
<td>pam_limits.so</td>
<td>session</td>
<td>定义使用系统资源的上限，root用户也会受此限制，可以通过/etc/security/limits.conf或/etc/security/limits.d/*.conf来设定</td>
</tr>
</tbody>
</table>
<h2 id="debug-crond"><a href="#debug-crond" class="headerlink" title="debug crond"></a>debug crond</h2><p>先停掉 crond service，然后开启debug参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl stop crond</div><div class="line">crond -x proc //不想真正执行的话：test</div></pre></td></tr></table></figure>
<p>或者增加更多的debug信息， debug sudo/sudoers , 在 /etc/sudo.conf 中增加了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Debug sudo /var/log/sudo_debug all@warn</div><div class="line">Debug sudoers.so /var/log/sudoers_debug all@debug</div></pre></td></tr></table></figure>
<h2 id="crond-ERROR-getpwnam-failed"><a href="#crond-ERROR-getpwnam-failed" class="headerlink" title="crond ERROR (getpwnam() failed)"></a>crond ERROR (getpwnam() failed)</h2><p><a href="https://www.ibm.com/support/pages/cron-job-fails-error-message-getpwnam-failed-no-such-file-or-directory" target="_blank" rel="external">报错信息</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">crond[246590]: (/usr/bin/ssh) ERROR (getpwnam() failed)</div></pre></td></tr></table></figure>
<p>要特别注意crond格式是 时间  <strong>用户</strong>  命令</p>
<p>有时候我们可以省略用户，但是在 <strong>/etc/cron.d/</strong> 中省略用户后报错如上</p>
<h2 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h2><p>把进程看做是资源分配的单位，把线程才看成一个具体的执行实体。</p>
<h2 id="deleted-文件"><a href="#deleted-文件" class="headerlink" title="deleted 文件"></a>deleted 文件</h2><p><code>lsof +L1</code> 或者<code>lsof | grep delete</code> 发现有被删除的文件，且占用大量磁盘空间</p>
<h2 id="No-route-to-host"><a href="#No-route-to-host" class="headerlink" title="No route to host"></a>No route to host</h2><p>如果ping ip能通,但是curl/telnet 访问 ip+port 报not route to host 错误,这肯定不是route问题(因为ping能通), 一般都是目标机器防火墙的问题</p>
<p>可以停掉防火墙验证,或者添加端口到防火墙:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#firewall-cmd --permanent --add-port=8090/tcp</div><div class="line">success</div><div class="line">#firewall-cmd --reload</div></pre></td></tr></table></figure>
<h2 id="强制重启系统"><a href="#强制重启系统" class="headerlink" title="强制重启系统"></a>强制重启系统</h2><p><img src="/images/oss/ee2e438907fa72c70d5393a651dc9113.png" alt="image.png"></p>
<h2 id="hostname"><a href="#hostname" class="headerlink" title="hostname"></a>hostname</h2><p>hostname -i 是根据机器的hostname去解析ip，如果 /etc/hosts里面没有指定hostname对应的ip就会走dns 流程然后libnss_myhostname 返回所有ip</p>
<p>getHostName获取的机器名如果对应的ip不是127.0.0.1，那么就用这个ip，否则就需要通过getHostByName获取所有网卡选择一个</p>
<h2 id="tsar-Floating-point-execption"><a href="#tsar-Floating-point-execption" class="headerlink" title="tsar Floating point execption"></a>tsar Floating point execption</h2><p><img src="/images/oss/72197d600425656ec9a8ed18bcc5853b.png" alt="image.png"></p>
<p>因为 /etc/localtime 是deleted状态</p>
<h2 id="奇怪的文件大小-sparse-file"><a href="#奇怪的文件大小-sparse-file" class="headerlink" title="奇怪的文件大小 sparse file"></a>奇怪的文件大小 <a href="https://unix.stackexchange.com/questions/259932/strange-discrepancy-of-file-sizes-from-ls" target="_blank" rel="external">sparse file</a></h2><p><img src="/images/oss/720f618d-2911-4bfd-a63e-33399532b6e5.png" alt="img"></p>
<p>如上图 gc.log 实际为5.6M，但是通过 ls -lh 就变成74G了，但实际上总文件夹才63M。因为写文件的时候lseek了74G的地方写入5.6M的内容就看到是这个样子了，而前面lseek的74G是不需要从磁盘上分配出来的.</p>
<p><a href="https://www.lisenet.com/2014/so-what-is-the-size-of-that-file/" target="_blank" rel="external">而 ls -s 中的 -s就是只看实际大小</a></p>
<p><img src="/images/oss/19b5f6cc-6fc4-4ad6-854c-6164705d343a.png" alt="img"></p>
<p><a href="https://www.systutorials.com/handling-sparse-files-on-linux/" target="_blank" rel="external">图片来源</a></p>
<p><a href="https://www.mankier.com/1/fallocate" target="_blank" rel="external">回收文件中的空洞</a>：sudo fallocate -c –length 70G gc.log</p>
<p>如果文件一直打开写入中是没法回收的，因为一回收又被重新lseek到之前的末尾重新写入了！</p>
<h2 id="增加dmesg-buffer"><a href="#增加dmesg-buffer" class="headerlink" title="增加dmesg buffer"></a>增加dmesg buffer</h2><p>If dmesg does not show any information about NUMA, then increase the Ring Buffer size:<br>Boot with ‘log_buf_len=16M’ (or some other big value). Refer the following kbase article <a href="https://access.redhat.com/solutions/47276" target="_blank" rel="external">How do I increase the kernel log ring buffer size?</a> for steps on how to increase the ring buffer</p>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a href="https://access.redhat.com/solutions/641093" target="_blank" rel="external">Yum commands error “pycurl.so: undefined symbol”</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># yum check update</div><div class="line">There was a problem importing one of the Python modules</div><div class="line">required to run yum. The error leading to this problem was:</div><div class="line"></div><div class="line">/usr/lib64/python2.6/site-packages/pycurl.so: undefined symbol: CRYPTO_set_locking_callback</div><div class="line"></div><div class="line">Please install a package which provides this module, or</div><div class="line">verify that the module is installed correctly.</div><div class="line"></div><div class="line">It&apos;s possible that the above module doesn&apos;t match the</div><div class="line">current version of Python, which is:</div><div class="line">2.6.6 (r266:84292, Sep  4 2013, 07:46:00)</div><div class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)]</div><div class="line"></div><div class="line">If you cannot solve this problem yourself, please go to</div><div class="line">the yum faq at:</div><div class="line">http://yum.baseurl.org/wiki/Faq</div></pre></td></tr></table></figure>
<ul>
<li>Check and fix the related library paths or remove 3rd party libraries, usually <code>libcurl</code> or <code>libssh2</code>. On a x86_64 system, the standard paths for those libraries are <code>/usr/lib64/libcurl.so.4</code> and <code>/usr/lib64/libssh2.so.1</code></li>
</ul>
<h2 id="软中断、系统调用和上下文切换"><a href="#软中断、系统调用和上下文切换" class="headerlink" title="软中断、系统调用和上下文切换"></a>软中断、系统调用和上下文切换</h2><p>“你可以把内核看做是不断对请求进行响应的服务器，这些请求可能来自在CPU上执行的进程，也可能来自发出中断的外部设备。老板的请求相当于中断，而顾客的请求相当于用户态进程发出的系统调用”。</p>
<p>软中断和系统调用一样，都是CPU停止掉当前用户态上下文，保存工作现场，然后陷入到内核态继续工作。二者的唯一区别是系统调用是切换到同进程的内核态上下文，而软中断是则是切换到了另外一个内核进程ksoftirqd上。</p>
<blockquote>
<p>系统调用开销是200ns起步</p>
<p>从实验数据来看，一次软中断CPU开销大约3.4us左右</p>
<p>实验结果显示进程上下文切换平均耗时 3.5us，lmbench工具显示的进程上下文切换耗时从2.7us到5.48之间</p>
<p>大约每次线程切换开销大约是3.8us左右。<strong>从上下文切换的耗时上来看，Linux线程（轻量级进程）其实和进程差别不太大</strong>。</p>
</blockquote>
<p>软中断和进程上下文切换比较起来，进程上下文切换是从用户进程A切换到了用户进程B。而软中断切换是从用户进程A切换到了内核线程ksoftirqd上。而ksoftirqd作为一个内核控制路径，其处理程序比一个用户进程要轻量，所以上下文切换开销相对比进程切换要少一些（实际数据基本差不多）。</p>
<p>系统调用只是在进程内将用户态切换到内核态，然后再切回来，而上下文切换可是直接从进程A切换到了进程B。显然这个上下文切换需要完成的工作量更大。</p>
<h3 id="软中断开销计算"><a href="#软中断开销计算" class="headerlink" title="软中断开销计算"></a><a href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&amp;mid=2247483827&amp;idx=3&amp;sn=8b897c8d6d3038ea79bd156a0e88db10&amp;scene=21#wechat_redirect" target="_blank" rel="external">软中断开销计算</a></h3><ul>
<li><strong>查看软中断总耗时</strong>， 首先用top命令可以看出每个核上软中断的开销占比，是在si列（1.2%–1秒[1000ms]中的1.2%）</li>
<li><strong>查看软中断次数</strong>，再用vmstat命令可以看到软中断的次数（in列 56000）</li>
<li><strong>计算每次软中断的耗时</strong>，该机器是16核的物理实机，故可以得出每个软中断需要的CPU时间是=12ms/(56000/16)次=3.428us。从实验数据来看，一次软中断CPU开销大约3.4us左右</li>
</ul>
<h2 id="Linux-启动进入紧急模式"><a href="#Linux-启动进入紧急模式" class="headerlink" title="Linux 启动进入紧急模式"></a>Linux 启动进入紧急模式</h2><p>可能是因为磁盘挂载不上，检查 /etc/fstab 中需要挂载的磁盘，尝试 mount -a 是否能全部挂载，麒麟下容易出现弄丢磁盘的标签和uuid</p>
<p>否则的话debug为啥，比如检查设备标签（e2label）是否冲突之类的</p>
<h2 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h2><p><a href="https://zhuanlan.zhihu.com/p/401910162" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/401910162</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">PROCESS STATE CODES</div><div class="line">  Here are the different values that the s, stat and state output specifiers(header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of a process:</div><div class="line"> </div><div class="line">    D    uninterruptible sleep (usually IO)  #不可中断睡眠 不接受任何信号，因此kill对它无效，一般是磁盘io,网络io读写时出现</div><div class="line">    R    running or runnable (on run queue)  #可运行状态或者运行中，可运行状态表明进程所需要的资源准备就绪，待内核调度</div><div class="line">    S    interruptible sleep (waiting for an event to complete) #可中断睡眠，等待某事件到来而进入睡眠状态</div><div class="line">    T    stopped by job control signal #进程暂停状态 平常按下的ctrl+z,实际上是给进程发了SIGTSTP 信号 （kill -l可查看系统所有的信号量）</div><div class="line">    t    stopped by debugger during the tracing #进程被ltrace、strace attach后就是这种状态</div><div class="line">    W    paging (not valid since the 2.6.xx kernel) #没有用了</div><div class="line">    X    dead (should never be seen) #进程退出时的状态</div><div class="line">    Z    defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent #进程退出后父进程没有正常回收，俗称僵尸进程</div></pre></td></tr></table></figure>
<h3 id="D状态的进程"><a href="#D状态的进程" class="headerlink" title="D状态的进程"></a><a href="https://gohalo.me/post/linux-kernel-hang-task-panic-introduce.html" target="_blank" rel="external">D状态的进程</a></h3><blockquote>
<p> Process D是指进程处于不可中断状态。即uninterruptible sleep，通常我们比较常遇到的就是进程自旋等到进入竞争区等，刷脏页，进程同步等</p>
<p>D： Disk sleep（task_uninterruptible)–比如，磁盘满，导致进程D，无法kill</p>
</blockquote>
<p>相关设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">echo 1 &gt;  /proc/sys/kernel/hung_task_panic  </div><div class="line"></div><div class="line">----- 处于D状态的超时时间，默认是120s</div><div class="line">$ cat /proc/sys/kernel/hung_task_timeout_secs</div><div class="line"></div><div class="line">----- 发现hung task之后是否触发panic操作</div><div class="line">$ cat /proc/sys/kernel/hung_task_panic</div><div class="line"></div><div class="line">----- 每次检查的进程数</div><div class="line">$ cat /proc/sys/kernel/hung_task_check_count</div><div class="line"></div><div class="line">----- 为了防止日志被刷爆，设置最多的打印次数</div><div class="line">$ cat /proc/sys/kernel/hung_task_warnings</div></pre></td></tr></table></figure>
<p>这个参数可以用来处理 D 状态进程 </p>
<p>内核在 3.10.0 版本之后提供了 hung task 机制，用来检测系统中长期处于 D 状体的进程，如果存在，则打印相关警告和进程堆栈。</p>
<p>如果配置了 <code>hung_task_panic</code> ，则会直接发起 panic 操作，然后结合 kdump 可以搜集到相关的 vmcore 文件，用于定位分析。</p>
<p>其基本原理也很简单，系统启动时会创建一个内核线程 <code>khungtaskd</code>，定期遍历系统中的所有进程，检查是否存在处于 D 状态且超过 120s 的进程，如果存在，则打印相关警告和进程堆栈，并根据参数配置决定是否发起 panic 操作。</p>
<p>查看D进程出现的原因：</p>
<p><img src="/images/951413iMgBlog/image-20230814112500971.png" alt="image-20230814112500971"></p>
<p>这个堆栈能看到进程在哪里 D 住了，但不一定是根本原因，有可能是被动进入 D 状态的</p>
<p><img src="/images/951413iMgBlog/image-20230814112742429.png" alt="image-20230814112742429"></p>
<h3 id="T-状态进程"><a href="#T-状态进程" class="headerlink" title="T 状态进程"></a>T 状态进程</h3><p>kill -CONT pid 来恢复</p>
<p>jmap -heap/histo和大家使用-F参数是一样的，底层都是通过serviceability agent来实现的，并不是jvm attach的方式，通过sa连上去之后会挂起进程，在serviceability agent里存在bug可能<strong>导致detach的动作不会被执行</strong>，从而会让进程一直挂着，可以通过top命令验证进程是否处于T状态，如果是说明进程被挂起了，如果进程被挂起了，可以通过kill -CONT [pid]来恢复。</p>
<h2 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h2><p>『你所规划的路由必须要是你的网卡 (如 eth0) 或 IP 可以直接沟通 (broadcast) 的情况』才行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">$sudo route add -net 11.164.191.0  gw 11.164.191.247 netmask 255.255.255.0 bond0</div><div class="line">SIOCDELRT: No such process // 从bond0没法广播到 11.164.191.247</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  gw 100.81.183.247 netmask 255.255.255.0 bond0.700</div><div class="line">SIOCADDRT: Network is unreachable //从bond0.700 没法广播到 100.81.183.247，实际目前从bond0.700没法广播到任何地方</div><div class="line"></div><div class="line">$sudo route add** **11.164.191.247** **dev** **bond0.700</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  **gw 100.81.183.247** netmask 255.255.255.0 bond0.700</div><div class="line">SIOCADDRT: Network is unreachable  //从bond0.700 没法广播到 100.81.183.247</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  gw 11.164.191.247 netmask 255.255.255.0 bond0</div><div class="line">SIOCADDRT: Network is unreachable//从bond0没法广播到 11.164.191.247但是从bond0.700可以</div><div class="line"></div><div class="line">$sudo route add -net 11.164.191.0  **gw 11.164.191.247** netmask 255.255.255.0 bond0.700</div></pre></td></tr></table></figure>
<p><a href="https://serverfault.com/questions/581159/unable-to-add-a-static-route-sioaddrt-network-is-unreachable" target="_blank" rel="external">https://serverfault.com/questions/581159/unable-to-add-a-static-route-sioaddrt-network-is-unreachable</a></p>
<h2 id="linux-2-6-32内核高精度定时器带来的cpu-sy暴涨的“问题”"><a href="#linux-2-6-32内核高精度定时器带来的cpu-sy暴涨的“问题”" class="headerlink" title="linux 2.6.32内核高精度定时器带来的cpu sy暴涨的“问题”"></a>linux 2.6.32内核高精度定时器带来的cpu sy暴涨的“问题”</h2><p>在 2.6.32 以前的内核里，即使你在java里写queue.await(1ns)之类的代码，其实都是需要1ms左右才会执行的，但.32以后则可以支持ns级的调度，对于实时性要求非常非常高的性能而言，这本来是个好特性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat /proc/timer_list | grep .resolution</div></pre></td></tr></table></figure>
<p>可以通过在 /boot/grub2/grub.cfg 中相应的kernel行的最后增加highres=off nohz=off来关闭高精度（不建议这样做，最好还是程序本身做相应的修改）</p>
<h2 id="后台执行"><a href="#后台执行" class="headerlink" title="后台执行"></a>后台执行</h2><p>将任务放到后台，断开ssh后还能运行：</p>
<ol>
<li>“ctrl-Z”将当前任务挂起；</li>
<li>“disown -h”让该任务忽略 SIGHUP 信号（不会因为掉线而终止执行）；</li>
<li>“bg”让该任务在后台恢复运行。</li>
</ol>
<h2 id="Linux-进程调度"><a href="#Linux-进程调度" class="headerlink" title="Linux 进程调度"></a>Linux 进程调度</h2><p>Linux的进程调度有一个不太为人熟知的特性，叫做<strong>wakeup affinity</strong>，它的初衷是这样的：如果两个进程频繁互动，那么它们很有可能共享同样的数据，把它们放到亲缘性更近的scheduling domain有助于提高缓存和内存的访问性能，所以当一个进程唤醒另一个的时候，被唤醒的进程可能会被放到相同的CPU core或者相同的NUMA节点上。</p>
<p>这个特性缺省是打开的，它有时候很有用，但有时候却对性能有伤害作用。设想这样一个应用场景：一个主进程给成百上千个辅进程派发任务，这成百上千个辅进程被唤醒后被安排到与主进程相同的CPU core或者NUMA节点上，就会导致负载严重失衡，CPU忙的忙死、闲的闲死，造成性能下降。<a href="https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA" target="_blank" rel="external">https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA</a></p>
<h2 id="tty"><a href="#tty" class="headerlink" title="tty"></a><a href="https://www.cnblogs.com/liqiuhao/p/9031803.html" target="_blank" rel="external">tty</a></h2><p>tty（teletype–最早的一种终端设备，远程打字机） stty 设置tty的相关参数</p>
<p>tty都在 /dev 下，通过 ps -ax 可以看到进程的tty；通过tty 可以看到本次的终端</p>
<p>/dev/pty（Pseudo Terminal） 伪终端</p>
<p>/dev/tty 控制终端</p>
<p>远古时代tty是物理形态的存在</p>
<p><img src="/images/951413iMgBlog/v2-7aa6997d017d876543671e4113048a62_1440w.jpg" alt="img"></p>
<p>PC时代，物理上的terminal已经没有了（用虚拟的伪终端代替，pseudo tty, 简称pty），相对kernel增加了shell，这是terminal和shell容易混淆，他们的含义</p>
<p><img src="/images/951413iMgBlog/v2-63cdd117f1026c2bbf455920b29c4454_1440w.jpg" alt="img"></p>
<p>实际像如下图的工作协作:</p>
<p><img src="/images/951413iMgBlog/case3.png" alt="Diagram"></p>
<h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a><a href="https://wangdoc.com/ssh/rsync.html" target="_blank" rel="external">rsync</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">将本地yum备份到150上的/data/yum/ 下</div><div class="line">rsync -arv ./yum/ root@11.167.60.150:/data/yum/</div><div class="line"></div><div class="line">走ssh的8022端口把目录备份到本地</div><div class="line">rsync -e &apos;ssh -p 8022&apos; -arv gcsql@10.2.3.4:/home/gcsql/doc/ ./</div></pre></td></tr></table></figure>
<p><code>-a</code>、<code>--archive</code>参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。</p>
<p><code>--delete</code>参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。</p>
<p><code>-i</code>参数表示输出源目录与目标目录之间文件差异的详细情况。</p>
<p><code>--link-dest</code>参数指定增量备份的基准目录。</p>
<p><code>-n</code>参数或<code>--dry-run</code>参数模拟将要执行的操作，而并不真的执行。配合<code>-v</code>参数使用，可以看到哪些内容会被同步过去。</p>
<p><code>--partial</code>参数允许恢复中断的传输。不使用该参数时，<code>rsync</code>会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与<code>--append</code>或<code>--append-verify</code>配合使用。</p>
<p><code>--progress</code>参数表示显示进展。</p>
<p><code>-r</code>参数表示递归，即包含子目录。</p>
<p><code>-v</code>参数表示输出细节。<code>-vv</code>表示输出更详细的信息，<code>-vvv</code>表示输出最详细的信息。</p>
<h2 id="Shebang"><a href="#Shebang" class="headerlink" title="Shebang"></a>Shebang</h2><p>Shebang 的东西 <code>#!/bin/bash</code></p>
<p>对 Shebang 的处理是内核在进行。当内核加载一个文件时，会首先读取文件的前 128 个字节，根据这 128 个字节判断文件的类型，然后调用相应的加载器来加载。</p>
<h3 id="ELF（Executable-and-Linkable-Format）"><a href="#ELF（Executable-and-Linkable-Format）" class="headerlink" title="ELF（Executable and Linkable Format）"></a>ELF（Executable and Linkable Format）</h3><p>对应windows下的exe</p>
<h2 id="修改启动参数"><a href="#修改启动参数" class="headerlink" title="修改启动参数"></a>修改启动参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">$cat change_kernel_parameter.sh </div><div class="line">#cat /sys/devices/system/cpu/vulnerabilities/*</div><div class="line">#grep &apos;&apos; /sys/devices/system/cpu/vulnerabilities/*</div><div class="line">#https://help.aliyun.com/document_detail/102087.html?spm=a2c4g.11186623.6.721.4a732223pEfyNC</div><div class="line"></div><div class="line">#cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">#transparent_hugepage=always</div><div class="line">#noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off</div><div class="line">#追加nopti nospectre_v2到内核启动参数中</div><div class="line">sudo sed -i &apos;s/\(GRUB_CMDLINE_LINUX=&quot;.*\)&quot;/\1 nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off transparent_hugepage=always&quot;/&apos; /etc/default/grub</div><div class="line"></div><div class="line">//从修改的 /etc/default/grub 生成 /boot/grub2/grub.cfg 配置</div><div class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</div><div class="line"></div><div class="line">#limit the journald log to 500M</div><div class="line">sed -i &apos;s/^#SystemMaxUse=$/SystemMaxUse=500M/g&apos; /etc/systemd/journald.conf</div><div class="line">#重启系统</div><div class="line">#sudo reboot</div><div class="line"></div><div class="line">## 选择不同的kernel启动</div><div class="line">#sudo grep &quot;menuentry &quot; /boot/grub2/grub.cfg | grep -n menu</div><div class="line">##grub认的index从0开始数的</div><div class="line">#sudo grub2-reboot 0; sudo reboot</div><div class="line"></div><div class="line">$cat /sys/kernel/mm/transparent_hugepage/enabled</div><div class="line">always [madvise] never</div></pre></td></tr></table></figure>
<h2 id="制作启动盘"><a href="#制作启动盘" class="headerlink" title="制作启动盘"></a>制作启动盘</h2><p>Windows 上用 UltraISO 烧制，Mac 上就比较简单了，直接用 dd 就可以搞</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">$ diskutil list</div><div class="line">/dev/disk6 (external, physical):</div><div class="line">   #:                       TYPE NAME                    SIZE       IDENTIFIER</div><div class="line">   0:                                                   *31.5 GB    disk6</div><div class="line">                        </div><div class="line"># 找到 U 盘的那个设备，umount</div><div class="line">$ diskutil unmountDisk /dev/disk3</div><div class="line"></div><div class="line"># 用 dd 把 ISO 文件写进设备，注意这里是 rdisk3 而不是 disk3，在 BSD 中 r(IDENTIFIER)</div><div class="line"># 代表了 raw device，会快很多</div><div class="line">$ sudo dd if=/path/image.iso of=/dev/rdisk3 bs=1m</div><div class="line"></div><div class="line"># 弹出 U 盘</div><div class="line">$ sudo diskutil eject /dev/disk3</div></pre></td></tr></table></figure>
<p><a href="https://linuxiac.com/how-to-create-bootable-usb-drive-using-dd-command/" target="_blank" rel="external">Linux 下制作步骤</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">umount /dev/sdn1</div><div class="line">sudo mkfs.vfat /dev/sdn1</div><div class="line">dd if=/polarx/uniontechos-server-20-1040d-amd64.iso of=/dev/sdn1 status=progress</div></pre></td></tr></table></figure>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>为保证服务性能应选用 performance 模式，将 CPU 频率固定工作在其支持的最高运行频率上，不进行动态调节，操作命令为 <code>cpupower frequency-set --governor performance</code>。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ul>
<li>dmesg | tail</li>
<li>vmstat 1</li>
<li>mpstat -P ALL 1</li>
<li>pidstat 1</li>
<li>iostat -xz 1</li>
<li>free -m</li>
<li>sar -n DEV 1</li>
<li>sar -n TCP,ETCP 1</li>
</ul>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">//检查sda磁盘中哪个应用程序占用的io比较高</div><div class="line">pidstat -d  1</div><div class="line"></div><div class="line">//分析应用程序中哪一个线程占用的io比较高</div><div class="line">pidstat -dt -p 73739 1  执行两三秒即可,得到74770线程io高</div><div class="line"></div><div class="line">//分析74770这个线程在干什么</div><div class="line">perf trace -t 74770 -o /tmp/tmp_aa.pstrace</div><div class="line">cat /tmp/tmp_aa.pstrace</div><div class="line">  2850.656 ( 1.915 ms): futex(uaddr: 0x653ae9c4, op: WAIT|PRIVATE_FLAG, val: 1)               = 0</div><div class="line">  2852.572 ( 0.001 ms): futex(uaddr: 0x653ae990, op: WAKE|PRIVATE_FLAG, val: 1)               = 0</div><div class="line">  2852.601 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2852.690 ( 0.040 ms): write(fd: 159, buf: 0xd7a30020, count: 65536)                         = 65536</div><div class="line">  2852.796 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2852.798 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58)             = 0</div><div class="line">  2852.939 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38)             = 0</div><div class="line">  2852.950 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2852.977 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2853.029 ( 0.035 ms): write(fd: 64, buf: 0xcd51e020, count: 65536)                          = 65536</div><div class="line">  2853.164 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</div><div class="line">  2853.167 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58)             = 0</div><div class="line">  2853.302 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38)             = 0</div></pre></td></tr></table></figure>
<h3 id="内存——虚拟内存参数"><a href="#内存——虚拟内存参数" class="headerlink" title="内存——虚拟内存参数"></a>内存——虚拟内存参数</h3><ul>
<li><code>dirty_ratio</code> 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统将开始使用 pdflush 操作将脏的 page cache 写入磁盘。默认值为 20％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，降低其值有利于提高内存回收时的效率。</li>
<li><code>dirty_background_ratio</code> 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统开始在后台将脏的 page cache 写入磁盘。默认值为 10％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，设置较低的值有利于提高内存回收时的效率。</li>
</ul>
<h3 id="I-O-调度器"><a href="#I-O-调度器" class="headerlink" title="I/O 调度器"></a>I/O 调度器</h3><p>I/O 调度程序确定 I/O 操作何时在存储设备上运行以及持续多长时间。也称为 I/O 升降机。对于 SSD 设备，宜设置为 noop。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> noop &gt; /sys/block/<span class="variable">$&#123;SSD_DEV_NAME&#125;</span>/queue/scheduler</div></pre></td></tr></table></figure>
<h3 id="磁盘挂载参数"><a href="#磁盘挂载参数" class="headerlink" title="磁盘挂载参数"></a>磁盘挂载参数</h3><p><code>noatime</code> 读取文件时，将禁用对元数据的更新。它还启用了 nodiratime 行为，该行为会在读取目录时禁用对元数据的更新。</p>
<h2 id="Unix-Linux关系"><a href="#Unix-Linux关系" class="headerlink" title="Unix Linux关系"></a>Unix Linux关系</h2><p><img src="/images/951413iMgBlog/image-20211210085124387.png" alt="image-20211210085124387"></p>
<p><img src="/images/951413iMgBlog/G2Xri.png" alt="img"></p>
<h3 id="linux-发行版关系"><a href="#linux-发行版关系" class="headerlink" title="linux 发行版关系"></a><a href="https://blog.51cto.com/wangyafei/1881605" target="_blank" rel="external">linux 发行版关系</a></h3><p><img src="/images/951413iMgBlog/5cc164f5d79a11261.jpg_fo742.jpg" alt="细数各家linux之间的区别_软件应用_什么值得买"></p>
<p>Fedora：基于Red Hat Linux，在Red Hat Linux终止发行后，红帽公司计划以Fedora来取代Red Hat Linux在个人领域的应用，而另外发行的Red Hat Enterprise Linux取代Red Hat Linux在商业应用的领域。Fedora的功能对于用户而言，它是一套功能完备、更新快速的免费操作系统，而对赞助者Red Hat公司而言，它是许多新技术的测试平台，被认为可用的技术最终会加入到Red Hat Enterprise Linux中。Fedora大约每六个月发布新版本。</p>
<p>不同发行版几乎采用了不同包管理器（SLES、Fedora、openSUSE、centos、RHEL使用rmp包管理系统，包文件以RPM为扩展名；Ubuntu系列，Debian系列使用基于DPKG包管理系统，包文件以deb为扩展名。)</p>
<p>69年Unix诞生在贝尔实验室，80年 DARPA（国防部高级计划局）请人在Unix实现全新的TCP、IP协议栈。ARPANET最先搞出以太网</p>
<p>Linux 从91年到95年处于成长期，真正大规模应用是Linux+Apache提供的WEB服务被大家大规模采用</p>
<p>rpm:  centos/fedora/suse</p>
<p>deb:  debian/ubuntu/uos(早期基于ubuntu定制，后来基于debian定制，再到最近开始直接基于kernel定制)</p>
<p>ARPANET：<strong>高等研究計劃署網路</strong>（英語：Advanced Research Projects Agency Network），通称<strong>阿帕网</strong>（英語：ARPANET）是美國<a href="https://zh.m.wikipedia.org/wiki/國防高等研究計劃署" target="_blank" rel="external">國防高等研究計劃署</a>开发的世界上第一个运营的<a href="https://zh.m.wikipedia.org/wiki/封包交換" target="_blank" rel="external">封包交换</a>网络，是全球<a href="https://zh.m.wikipedia.org/wiki/互联网" target="_blank" rel="external">互联网</a>的鼻祖。</p>
<p>TCP/IP：1974年，卡恩和瑟夫带着研究成果，在IEEE期刊上，发表了一篇题为《关于分组交换的网络通信协议》的论文，正式提出TCP/IP，用以实现计算机网络之间的互联。</p>
<p>在1983年，美国国防部高级研究计划局决定淘汰NCP协议（ARPANET最早使用的协议），TCP/IP取而代之。</p>
<h3 id="Deepin-UOS"><a href="#Deepin-UOS" class="headerlink" title="Deepin UOS"></a>Deepin UOS</h3><p><strong><em>\</em>Deepin 与统信 UOS 类似于红帽的 Fedora 与 RHEL 的上下游关系，Deepin 依然保持着原来的社区运营模式，而统信 UOS 则是基于社区版 Deepin 构建的商业发行版，为 Deepin 挖掘更多的商业机会和更大的商业价值，进而反哺社区，形成良性循环**</strong>。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.cnblogs.com/kevingrace/p/8671964.html" target="_blank" rel="external">https://www.cnblogs.com/kevingrace/p/8671964.html</a></p>
<p><a href="https://www.jianshu.com/p/ac3e7009a764" target="_blank" rel="external">https://www.jianshu.com/p/ac3e7009a764</a></p>
<p>B 站哈工大操作系统视频地址：<a href="https://www.bilibili.com/video/BV1d4411v7u7?from=search&amp;seid=2361361014547524697" target="_blank" rel="external">https://www.bilibili.com/video/BV1d4411v7u7?from=search&amp;seid=2361361014547524697</a></p>
<p>B 站清华大学操作系统视频地址：<a href="https://www.bilibili.com/video/BV1js411b7vg?from=search&amp;seid=2361361014547524697" target="_blank" rel="external">https://www.bilibili.com/video/BV1js411b7vg?from=search&amp;seid=2361361014547524697</a></p>
<p><a href="https://linux.cn/article-10465-1.html" target="_blank" rel="external">Linux 工具：点的含义</a> <a href="https://www.linux.com/training-tutorials/linux-tools-meaning-dot/" target="_blank" rel="external">英文版</a></p>
<p><a href="http://coolnull.com/4432.html" target="_blank" rel="external">linux cp实现强制覆盖</a></p>
<p><a href="https://wangdoc.com/bash/startup.html" target="_blank" rel="external">https://wangdoc.com/bash/startup.html</a></p>
<p><a href="https://cjting.me/2020/12/10/tiny-x64-helloworld/" target="_blank" rel="external">编写一个最小的 64 位 Hello World</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/01/ipmitool修改BIOS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/01/ipmitool修改BIOS/" itemprop="url">ipmitool 和 BIOS</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-01T12:30:03+08:00">
                2020-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ipmitool-和-BIOS"><a href="#ipmitool-和-BIOS" class="headerlink" title="ipmitool 和 BIOS"></a>ipmitool 和 BIOS</h1><h2 id="什么是-IPMI"><a href="#什么是-IPMI" class="headerlink" title="什么是 IPMI"></a>什么是 IPMI</h2><p>IPMI（智能平台管理接口），Intelligent Platform Management Interface 的缩写。原本是一种<a href="https://baike.baidu.com/item/Intel" target="_blank" rel="external">Intel</a>架构的企业系统的周边设备所采用的一种工业标准。IPMI亦是一个开放的免费标准，用户无需支付额外的费用即可使用此标准。</p>
<p>IPMI 能够横跨不同的操作系统、固件和硬件平台，可以智能的监视、控制和自动回报大量服务器的运作状况，以降低服务器系统成本。</p>
<p>1998年Intel、DELL、HP及NEC共同提出IPMI规格，可以透过网路远端控制温度、电压。</p>
<p>2001年IPMI从1.0版改版至1.5版，新增 PCI Management Bus等功能。</p>
<p>2004年Intel发表了IPMI 2.0的规格，能够向下相容IPMI 1.0及1.5的规格。新增了Console Redirection，并可以通过Port、Modem以及Lan远端管理伺服器，并加强了安全、VLAN 和刀锋伺服器的支援性。</p>
<p>Intel/amd/hygon 基本都支持 ipmitool，看起来ARM 支持的接口也许不一样</p>
<p>BMC（Baseboard Management Controller）即我们常说的带外系统，是在机器上电时即完成自身初始化，开始运行。其系统可在standby电模式下工作。所以，通过带外监控服务器硬件故障，不受OS存活状态影响，可实现7*24小时无间断监控，甚至我们可以通过带外方式，精确感知带内存活，实现OS存活监控。</p>
<p>BMC在物理形态上，由一主嵌入式芯片+系列总线+末端芯片组成的一个硬件监控&amp;控制系统，嵌入式芯片中运行嵌入式Linux操作系统，负责整个BMC系统的资源协调及用户交互，核心进程是IPMImain进程，实现了全部IPMI2.0协议的消息传递&amp;处理工作。</p>
<h2 id="ipmitool-用法"><a href="#ipmitool-用法" class="headerlink" title="ipmitool 用法"></a>ipmitool 用法</h2><p>基本步骤：</p>
<ol>
<li>查看当前值：ipmitool raw 0x3e 0x5f 0x00 0x11 （非必要，列出目前BIOS中的值）</li>
<li>打开配置开关(让BIOS进入可配置，默认不可配置)：ipmitool raw 0x3e 0x5c 0x00 0x01 0x81</li>
<li>修改某个值，比如将numa 设置为on：ipmitool raw 0x3e 0x5c 0x05 0x01 0x81</li>
<li>查看修改后的值：ipmitool raw 0x3e 0x5d 0x05 0x01 (必须要)</li>
<li>最后reboot机器新的值就会apply到BIOS中</li>
</ol>
<p><a href="https://blog.csdn.net/zygblock/article/details/53367664" target="_blank" rel="external">ipmitool使用</a>基本语法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">         固定-不变 0x5c修改   要修改的项    长度(一搬都是01)    新的值(0x81 表示on、0x80表示off)</div><div class="line">ipmitool raw 0x3e 0x5c      index       0x01                value</div></pre></td></tr></table></figure>
<p>第1/2个参数raw 0x3e 固定不变</p>
<p>第三个参数表示操作可以是：</p>
<ul>
<li>0x5c 修改</li>
<li>0x5f 查看BIOS中的当前值（海光是这样，intel不是）</li>
<li>0x5d 查询即将写入的值（修改后没有写入 0x5f 看到的是老值）  </li>
</ul>
<p>第四个参数Index表示需要修改的配置项（具体见后表）</p>
<p>第五个参数 0x01 表示值的长度，一般固定不需要改</p>
<p>value 表示值，0x81表示enable; 0x80表示disable</p>
<h3 id="ipmitool带外设置步骤"><a href="#ipmitool带外设置步骤" class="headerlink" title="ipmitool带外设置步骤"></a><a href="https://promisechen.github.io/kbase/ipmi.html" target="_blank" rel="external">ipmitool</a>带外设置步骤</h3><p>1）设置valid flag：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 raw 0x3e 0x5c 0x00 0x01 0x81</p>
<p>2） 设置对应的选项：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 raw 0x3e 0x5c index 0x01 Data  —- index 和Data参考下述表格；</p>
<p>3）重启CN：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 power reset</p>
<p>4）读取当前值：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10  raw 0x3e 0x5f index 0x01 </p>
<p> 如moc机型，读取CN的 Numa值：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 raw 0x3e 0x5f 0x05 0x01 </p>
<h3 id="确认是否设置成功"><a href="#确认是否设置成功" class="headerlink" title="确认是否设置成功"></a>确认是否设置成功</h3><p>查询要写入的新值：ipmitool 0x3e 0x5d 0x00 0x11</p>
<p> 返回值，如：11 <strong>81</strong> 81 00 00 00 00 00 00 00 00 00 00 00 00 00  00 00</p>
<p>​      第一个byte 表示查询数量，表示查询0x11个设置项；</p>
<pre><code>第二个byte 表示index=0的值，即Configuration，必须保证是0x81，才能进行重启，否则设置不生效；
</code></pre><p>​      第三个byte 表示index=1的值，即Turbo，表示要设置为0x81；</p>
<p>​      剩余byte依次类推……….</p>
<p>​      未设置新值的index对应值是00，要设置的index其对应值为Data（步骤3的设置值）；</p>
<h2 id="海光服务器修改案例"><a href="#海光服务器修改案例" class="headerlink" title="海光服务器修改案例"></a>海光服务器修改案例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">//海光+alios下 第二列为：0x5c 修改、0x5f BIOS中的查询、0x5d 查询即将写入的值 </div><div class="line">//0x5f 查询BIOS中的值</div><div class="line"></div><div class="line">#ipmitool raw 0x3e 0x5f 0x00 0x11</div><div class="line"> 11 81 81 81 81 80 81 80 81 81 80 81 81 00 00 81</div><div class="line"> 80 81</div><div class="line"> </div><div class="line">//还没有写入任何新值</div><div class="line">#ipmitool raw 0x3e 0x5d 0x00 0x11</div><div class="line"> 11 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</div><div class="line"> 00 00 </div><div class="line"></div><div class="line">//enable numa</div><div class="line">#ipmitool raw 0x3e 0x5c 0x00 0x01 0x81</div><div class="line">#ipmitool raw 0x3e 0x5c 0x05 0x01 0x81</div><div class="line"></div><div class="line">//enable boost</div><div class="line">#ipmitool raw 0x3e 0x5c 0x01 0x01 0x81</div><div class="line"></div><div class="line">#ipmitool raw 0x3e 0x5d 0x01 0x01</div><div class="line"> 11 81 </div><div class="line">//关闭 SMT</div><div class="line">#ipmitool raw 0x3e 0x5c 0x02 0x01 0x80</div><div class="line"></div><div class="line">#ipmitool raw 0x3e 0x5d 0x02 0x01</div><div class="line"> 11 81</div></pre></td></tr></table></figure>
<h2 id="BIOS中选项的对应关系"><a href="#BIOS中选项的对应关系" class="headerlink" title="BIOS中选项的对应关系"></a>BIOS中选项的对应关系</h2><h3 id="Intel服务器"><a href="#Intel服务器" class="headerlink" title="Intel服务器"></a>Intel服务器</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Index</th>
<th>Data Length（Bytes）</th>
<th>Data （不在列表中则为无效值）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Configuration</td>
<td>0x00</td>
<td>1</td>
<td>0x81 – valid Flag 0x82 – Restore Default Value (恢复为PRD定义的默认值)0x00  BIOS在读取设定后，会发送index=0x00，data=0x00的命令给BMC，BMC应清零所有参数值。</td>
</tr>
<tr>
<td>Turbo</td>
<td>0x01</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>HT</td>
<td>0x02</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>VT</td>
<td>0x03</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>EIST</td>
<td>0x04</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Numa</td>
<td>0X05</td>
<td>1</td>
<td>0x80 – disable 0x81 - enable</td>
</tr>
<tr>
<td>Vendor Change</td>
<td>0x06</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>VT-d</td>
<td>0x07</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>SRIOV</td>
<td>0x08</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Active Video</td>
<td>0x09</td>
<td>1</td>
<td>0x80 – Onboard0x81 – PCIe</td>
</tr>
<tr>
<td>Local HDD Boot</td>
<td>0x0A</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Hotkey support</td>
<td>0x0B</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Intel Speed Select</td>
<td>0x0C</td>
<td>1</td>
<td>0x80-Disable0x81-Config 10x82-Config 2</td>
</tr>
<tr>
<td>IMS</td>
<td>0x0D</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>TPM</td>
<td>0x0E</td>
<td>1</td>
<td>0x80-Disable0x81-Enabled0x83-Enable&amp;Clear TPM</td>
</tr>
<tr>
<td>Power off remove</td>
<td>0x0F</td>
<td>1</td>
<td>0x80 – disable – 响应命令0x81 – enable –不响应命令</td>
</tr>
<tr>
<td>BIOS BOOT MODE</td>
<td>0x10</td>
<td>1</td>
<td>0x80 – Legacy0x81 – UEFI</td>
</tr>
<tr>
<td>Active Cores</td>
<td>0x11</td>
<td>1</td>
<td>0x80 - Default Core Number0x81 - Active 1 Core0x82 - Active 2 Cores0x83 - Active 3 Cores…0xFE - Active 126 Cores</td>
</tr>
<tr>
<td>C   State</td>
<td>0x12</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>HWPM</td>
<td>0x13</td>
<td>1</td>
<td>0x80-Disable0x81-Native   mode0x82-OOB   Mode0x83-Native   mode Without Legacy support</td>
</tr>
<tr>
<td>Intel   SgxSW Guard Extensions (SGX)</td>
<td>0x14</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>SGX PRMRR Size</td>
<td>0x15</td>
<td>1</td>
<td>0X80-[00]No valid PRMRR   size    0X81-[40000000]1G0X82-[80000000]2G0X83-[100000000]4G0X84-[200000000]8G0X85-[400000000]16G0X86-[800000000]32G0X87-[1000000000]64G0X88-[2000000000]128G0X89-[4000000000]256G0X8A-[8000000000]512G</td>
</tr>
<tr>
<td>SGX Factory Reset</td>
<td>0x16</td>
<td></td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td></td>
<td>0x17</td>
<td></td>
<td>预留</td>
</tr>
<tr>
<td>CPU0_IOU0 (IIO PCIe Br1)</td>
<td>0x18</td>
<td>1</td>
<td>0x80 – x4x4x4x4   0x81 – x4x4x8   0x82 – x8x4x4   0x83 – x8x8   0x84 – x16   0x85 - Auto</td>
</tr>
<tr>
<td>CPU0_IOU1 (IIO PCIe Br2)</td>
<td>0x19</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU0_IOU2 (IIO PCIe Br3)</td>
<td>0x1a</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU0_IOU3 (IIO PCIe Br4)</td>
<td>0x1b</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU0_IOU4 (IIO PCIe Br5)</td>
<td>0x1c</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU0 (IIO PCIe Br1)</td>
<td>0x1d</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU1 (IIO PCIe Br2)</td>
<td>0x1e</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU2 (IIO PCIe Br3)</td>
<td>0x1f</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU3 (IIO PCIe Br4)</td>
<td>0x20</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU4 (IIO PCIe Br5)</td>
<td>0x21</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU0 (IIO PCIe Br1)</td>
<td>0x22</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU1 (IIO PCIe Br2)</td>
<td>0x23</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU2 (IIO PCIe Br3)</td>
<td>0x24</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU3 (IIO PCIe Br4)</td>
<td>0x25</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU4 (IIO PCIe Br5)</td>
<td>0x26</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU0 (IIO PCIe Br1)</td>
<td>0x27</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU1 (IIO PCIe Br2)</td>
<td>0x28</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU2 (IIO PCIe Br3)</td>
<td>0x29</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU3 (IIO PCIe Br4)</td>
<td>0x2a</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU4 (IIO PCIe Br5)</td>
<td>0x2b</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>SGXLEPUBKEYHASHx Write Enable</td>
<td>0x2C</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>SubNuma</td>
<td>0x2D</td>
<td>1</td>
<td>0x80-Disabled0x81-SN2</td>
</tr>
<tr>
<td>VirtualNuma</td>
<td>0x2E</td>
<td>1</td>
<td>0x80-Disabled0x81-Enabled</td>
</tr>
<tr>
<td>TPM Priority</td>
<td>0x2F</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>TDX</td>
<td>0x30</td>
<td>1</td>
<td>0x80 - Disabled0x81 - Enabled</td>
</tr>
<tr>
<td>Select Owner EPOCH input type</td>
<td>0x31</td>
<td>1</td>
<td>0x81-Change to New Random Owner EPOCHs0x82-Manual User Defined Owner EPOCHs</td>
</tr>
<tr>
<td>Software Guard Extensions Epoch 0</td>
<td>0x32</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Software Guard Extensions Epoch 1</td>
<td>0x33</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 id="AMD服务器"><a href="#AMD服务器" class="headerlink" title="AMD服务器"></a>AMD服务器</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Index</th>
<th>Data Length（Bytes）</th>
<th>Data （不在列表中则为无效值）</th>
<th>支持项目</th>
</tr>
</thead>
<tbody>
<tr>
<td>Configuration</td>
<td>0x00</td>
<td>1</td>
<td>0x81 – valid Flag 0x82 – Restore Default Value (恢复为PRD定义的默认值)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Core Performance Boost</td>
<td>0x01</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>SMT Mode</td>
<td>0x02</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>SVM Mode</td>
<td>0x03</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>EIST</td>
<td>0x04</td>
<td>1</td>
<td>0x80 (AMD默认支持智能调频但无此选项)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>NUMA nodes per socket</td>
<td>0X05</td>
<td>1</td>
<td>0x80 – NPS0 0x81 – NPS1 0x82 – NPS2 0x83 – NPS4 （开）0x87 – Auto(Auto为NPS1)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Vendor Change</td>
<td>0x06</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>IOMMU</td>
<td>0x07</td>
<td>1</td>
<td>0x80 – disable0x81 – enable0x8F – Auto</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>SRIOV</td>
<td>0x08</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Active Video</td>
<td>0x09</td>
<td>1</td>
<td>0x80 – Onboard0x81 – PCIe</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Local HDD Boot</td>
<td>0x0A</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Hotkey support</td>
<td>0x0B</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Intel Speed Select</td>
<td>0x0C</td>
<td>1</td>
<td>0x80 (AMD无此选项)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>IMS</td>
<td>0x0D</td>
<td>1</td>
<td>0x80 (AMD暂未做IMS功能)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>TPM</td>
<td>0x0E</td>
<td>1</td>
<td>0x80 – disable0x81 – enable0x83 – enable &amp; TPM   clear</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Power off remove</td>
<td>0x0F</td>
<td>1</td>
<td>0x80 (AMD暂未做此功能)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>BIOS BOOT MODE</td>
<td>0x10</td>
<td>1</td>
<td>0x80 – Legacy0x81 – UEFI</td>
<td>RomeMilan</td>
</tr>
</tbody>
</table>
<h3 id="海光服务器"><a href="#海光服务器" class="headerlink" title="海光服务器"></a>海光服务器</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Index</th>
<th>Data Length（Bytes）</th>
<th>Data （不在列表中则为无效值）</th>
<th>支持项目</th>
</tr>
</thead>
<tbody>
<tr>
<td>Configuration</td>
<td>0x00</td>
<td>1</td>
<td>0x81 – valid Flag 0x82 – Restore Default Value (恢复为PRD定义的默认值)</td>
<td>海光2</td>
</tr>
<tr>
<td>Core Performance Boost</td>
<td>0x01</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>SMT</td>
<td>0x02</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>SVM</td>
<td>0x03</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>P-State Control</td>
<td>0x04</td>
<td>1</td>
<td>0x80 – Performance0x81 – Normal</td>
<td>海光2</td>
</tr>
<tr>
<td>Memory Interleaving</td>
<td>0X05</td>
<td>1</td>
<td>0x80 – Socket (关numa) 0x81 - channel（8 node）</td>
<td>海光2</td>
</tr>
<tr>
<td>Vendor Change</td>
<td>0x06</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>IOMMU</td>
<td>0x07</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>SRIOV</td>
<td>0x08</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>Onboard VGA</td>
<td>0x09</td>
<td>1</td>
<td>0x80 – Onboard0x81 – PCIe</td>
<td>海光2</td>
</tr>
<tr>
<td>Local HDD Boot</td>
<td>0x0A</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>Hotkey support</td>
<td>0x0B</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>Hygon平台没有此选项</td>
<td>0x0C</td>
<td>1</td>
<td>0x80-Disable0x81-Config 10x82-Config 2</td>
<td>不支持</td>
</tr>
<tr>
<td>Hygon平台没有此选项</td>
<td>0x0D</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
<td>不支持</td>
</tr>
<tr>
<td>TPM</td>
<td>0x0E</td>
<td>1</td>
<td>0x80-Disable0x81-Enabled0x83-Enable&amp;Clear TPM</td>
<td>海光2</td>
</tr>
<tr>
<td>Power off remove</td>
<td>0x0F</td>
<td>1</td>
<td>0x80 – disable – 响应命令0x81 – enable –不响应命令</td>
<td>海光2</td>
</tr>
<tr>
<td>Boot option Filter</td>
<td>0x10</td>
<td>1</td>
<td>0x80 – Legacy0x81 – UEFI</td>
<td>海光2</td>
</tr>
</tbody>
</table>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/01/2010到2020这10年的碎碎念念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/01/2010到2020这10年的碎碎念念/" itemprop="url">2010到2020这10年的碎碎念念</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-01T00:30:03+08:00">
                2020-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/others/" itemprop="url" rel="index">
                    <span itemprop="name">others</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="2010到2020这10年的碎碎念念"><a href="#2010到2020这10年的碎碎念念" class="headerlink" title="2010到2020这10年的碎碎念念"></a>2010到2020这10年的碎碎念念</h1><h2 id="来自网络的一些数据"><a href="#来自网络的一些数据" class="headerlink" title="来自网络的一些数据"></a>来自网络的一些数据</h2><p>这十年，中国的人均GDP从大约3300美金干到了9800美金。这意味着：更多的中国人脱贫，更多的中国人变成了中产。这是这一轮消费升级的核心原动力，没有之一。</p>
<p>这十年，中国的进出口的总额从2009年占GDP的44.86%，降至34.35%。</p>
<p>互联网从美国copy开始变成创新、走在前列，因为有庞大的存量市场</p>
<p>2010年，一个数据发生了逆势波动。那就是中国的适龄小学人口增速。在此之前从1997年后，基本呈负增长。这是因为中国80后家长开始登上历史舞台。这带动了诸多产业的蓬勃发展，比如互联网教育，当然还有学区房。</p>
<p>10年吉利收购沃尔沃，18年吉利收购戴姆勒10%的股份。</p>
<p>微信崛起、头条崛起、百度走下神坛。美团、pdd崛起</p>
<p>12年2月6号的王护士长外逃美国大使馆也让大家兴奋了，11年的郭美美红十字会事件快要被忘记了，但是也让大家对慈善事件更加警惕，倒是谅解了汶川地震的王石10块捐款事件，不过老王很快因为娶了年轻的影星田朴珺一下子人设坍塌，大家更热衷老王的负面言论了。</p>
<p>温州动车事件让高铁降速了</p>
<p>我爸是李刚、药家鑫、李天一、邓玉娇（09年），陈冠希艳照门、三鹿奶粉、汶川地震、奥运会（08年）</p>
<p>2018年：中美贸易站、问题疫苗、个税改革、中兴被美制裁，北京驱赶低端人口，鸿茅药酒，p2p暴雷，昆山反杀案，相互宝</p>
<p>2015年：雾霾、柴静纪录片《穹顶之下》，屠呦呦诺贝尔奖，放开二胎</p>
<p>2014年：东莞扫黄、马航370事件；周师傅被查、占中</p>
<p>2013年：劳教正式被废除，想起2003年的孙志刚事件废除收容制度</p>
<p>2012年：方韩之争，韩寒走下神坛</p>
<p>2011年：日本海啸地震，中国抢盐事件；郭美美，温州动车</p>
<p>2010年：google退出中国，上海世博会开幕，富士康N连跳楼事件；我爸是李刚，腾讯大战360</p>
<h2 id="自我记忆"><a href="#自我记忆" class="headerlink" title="自我记忆"></a>自我记忆</h2><p>刚看到有人在说乐清钱云会事件，一晃10年了，10年前微博开始流行改变了好多新闻、热点事件的引爆方式。</p>
<p>这十年BBS、门户慢慢在消亡，10年前大家都知道三大门户网站和天涯，现在的新网民应该知道的不多了。</p>
<p>影响最大的还是移动网络的崛起，这也取决于4G和山寨机以及后来的小米手机，真正给中国的移动互联网带来巨大的红利，注入的巨大的增长。<br>我自己对移动互联网的判断是极端错误的，即使09年我就开始用上了iphone手机，那又怎么样，看问题还是用静态的视觉观点。手机没有键盘、手机屏幕狭小，这些确实是限制，到2014年我还想不明白为什么要在手机上购物，比较、看物品图片太不方便了，结果便利性秒杀了这些不方便；只有手机的群体秒杀了办公室里的白领，最后大家都很高兴地用手机购物了，甚至PC端bug更多，更甚至有些网站不提供PC端。</p>
<p>移动网络的崛起和微信的成功也相辅相成的，在移动网络时代每个人都有自己的手机，所以账号系统的打通不再是问题，尤其是都被微信这个移动航母在吞噬，其它公司都活在微信的阴影里。</p>
<p>当然移动支付的崛起就理所当然了。</p>
<p>即使今天网上购物还是PC上要方便，那又怎么样，很多时候网上购物都是不在电脑前的零碎时间。</p>
<p>10多年前第一次看到智能手机是室友购买的多普达，20年前也是这个室友半夜里很兴奋地播报台湾大选，让我知道了台湾大选这个事情。</p>
<p>基本的价值观、世界观，没怎么改变，不应该是年龄大了僵化了，应该是掌握信息的手段和能力增强了，翻墙获取信息也很容易，基本的逻辑还在也没那么容易跑偏了。可能就是别人看到的年纪大了脑子僵化了吧，自我感觉不一定对。</p>
<p>最近10年经济发展的非常好，政府对言论的控制越来越精准，舆论引导也非常”成功”,所以网络上看到这5年和5年前基本差别很大，5年前公知是个褒义词，5年后居然成了贬义词。</p>
<p>房价自然是这10年最火的话题，07年大家开始感觉到房价上涨快、房价高，08年金融危机本来是最好的机会，结果4万亿刺激下09年年底房价开始翻倍，到10年面对翻倍了的房价政府、媒体、老百姓都在喊高，实际也只是横盘，13-14年小拉一波，16年涨价去库存再大拉一波。基本让很多人绝望了</p>
<p>这十年做的最错的事情除了没有早点买房外就是想搞点投资收入投了制造外加炒股，踩点能力太差了，虽然前5年像任志强一样一直看多房价的不多，这个5年都被现实教育了，房价也基本到头了。</p>
<p>工作上应该更早地、坚定地进入互联网、移动互联网，这10年互联网对人才的需求实在太大了，虽然最终能伴随公司成长的太少，毕竟活下来长大的公司不多。</p>
<p>Google退出中国、看着小杨同学和一些同事移民、360大战QQ、诺贝尔和平奖、华为251事件都算是自己在一些公众事情上投入比较多的。非常不舍google的离开，这些年也基本还是只用google，既是无奈中用下百度也还是觉得搜不到什么有效信息；好奇移民的想法和他们出去后的各种生活；360跟QQ大战的时候觉得腾讯的垄断太牛叉了，同时认为可能360有这种资源的话会更作恶和垄断的更厉害，至少腾讯还是在乎外面的看法和要面子的；LXB到现在也是敏感词，直到病死在软禁中，这些年敏感词越来越多，言论的控制更严厉了；华为251也是个奇葩事件了，暴露了资本家的粗野和枉法。</p>
<p>自己工作上跳槽一次，继续做一个北漂。公司对自己的方法论改变确实比较大，近距离看到了一些成功因素方面的逻辑（更有效的激励和企业文化）。</p>
<p>经历了从外企到私企，从小公司到大公司的不同，外企英语是天花板，也看到了华为所谓的狼性、在金钱激励下的狼性，和对企业文化的维护，不能否认90%以上的人工作是为了钱</p>
<p>这几年也开始习惯写技术文章来总结了，这得益于Markdown+截图表述的便利，也深刻感受到深抠，然后总结分享的方法真的很好（高斯学习方法），也体会到了抓手、触类旁通的运用。10年前在搜狐blog写过一两年的博客放弃的很快，很难一直有持续的高水平总结和输出。</p>
<p>10年前还在比较MSN和QQ谁更好（我是坚定站在QQ这边的），10年后MSN再也看不见了，QQ也有了更好的替代工具微信。用处不大的地方倒是站对了，对自己最有用的关键地方都站错了。</p>
<p>10年前差点要去豆瓣，10年后豆瓣还活着，依然倔强地保持自己的品味，这太难得了。相反十年前好用得不得了的RSS订阅，从抓虾转到google reader再到feedly好东西就是活得这么艰难。反过来公众号起来了、贴吧式微了，公众号运作新闻类是没问题的（看完就过），但是对技术类深度一点的就很不合适了，你看看一篇文章24小时内的阅读量占据了98%以上，再到后面就存亡了！但是公众号有流量，流量可以让大家跪在地上。</p>
<p>10年前啃老是被看不起的，10年后早结婚、多啃老也基本成了这10年更对的事情，结婚得买房，啃老买得更早，不对的事情变对了（结婚早没错）。</p>
<p>很成功地组织了一次同学20周年的聚会，也看到了远则亲、近则仇的现实情况，自己组织统筹能力还可以。</p>
<p>情绪控制能力太差、容易失眠。这十年爱上了羽毛球和滑雪，虽然最近几年滑雪少了。</p>
<p>体会到自小贫穷带来的一些抠门的坏习惯。</p>
<p>2015年的股票大跌让自己很痛苦，这个过程反馈出来的不愿意撒手、在股市上的鸵鸟方式，股市上总是踩不到正确的点。割肉太难，割掉的总是错误的。</p>
<p>15、16年我认为云计算不怎么样，觉得无非就是新瓶装旧酒，现在云计算不再有人质疑了，即使现在都还是亏钱。</p>
<p>当然我也质疑过外卖就是一跑腿的，确实撑不起那么大的盘子，虽然没有像团购一样消亡，基本跟共享单车一样了，主要因为我是共享单车的重度用户，而我极端不喜欢外卖，所以要站出来看问题、屁股坐在哪边会严重影响看法，也就是不够客观。</p>
<p>网约车和移动支付一起在硝烟中混战</p>
<p>电动车开始起来，主要受政府弯道超车的刺激，目前看取决于自有充电位（适合三四线城市），可是三四线城市用户舍不得花这个溢价，汽油车都还没爽够呢。</p>
<p>对世界杯不再那么关注，对AlphaGo的新闻倒是很在意了。魏则西事件牢牢地把百度钉死在耻辱柱上。</p>
<p>随着12306的发展和高铁的起来，终于过年回家的火车票不用再靠半夜排队了。</p>
<p>2019年年末行政强制安装ETC，让我想起20年前物理老师在课堂上跟我们描述的将来小汽车走高速公路再也不用停下来收费了，会自动感应，开过去就自动扣钱了。我一直对这个未来场景念念不忘，最近10年我经常问别人为什么不办ETC，这个年底看到的是行政命令下的各种抱怨。</p>
<h3 id="看到："><a href="#看到：" class="headerlink" title="看到："></a>看到：</h3><ul>
<li><p>老人、家人更不愿意听身边亲近人员的建议；</p>
</li>
<li><p>老人思维为什么固化、怎么样在自己老后不是那样固化；</p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/12/24/Linux内核版本升级，性能到底提升多少？拿数据说话/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/24/Linux内核版本升级，性能到底提升多少？拿数据说话/" itemprop="url">Linux内核版本升级，性能到底提升多少？拿数据说话</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-24T17:30:03+08:00">
                2019-12-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux内核版本升级，性能到底提升多少？"><a href="#Linux内核版本升级，性能到底提升多少？" class="headerlink" title="Linux内核版本升级，性能到底提升多少？"></a>Linux内核版本升级，性能到底提升多少？</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>X 产品在公有云售卖一直使用的2.6.32的内核，有点老并且有些内核配套工具不能用，于是想升级一下内核版本。预期新内核的性能不能比2.6.32差</p>
<p>以下不作特殊说明的话都是在相同核数的Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz下得到的数据，最后还会比较相同内核下不同机型/CPU型号的性能差异。</p>
<p>场景都是用sysbench 100个并发跑点查。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><strong>先说大家关心的数据，最终4.19内核性能比2.6.32好将近30%，建议大家升级新内核，不需要做任何改动，尤其是Java应用（不同场景会有差异）</strong></p>
<p>本次比较的场景是Java应用的Proxy类服务，主要瓶颈是网络消耗，类似于MaxScale。后面有一个简单的MySQL Server场景下2.6.32和4.19的比较，性能也有33%的提升。</p>
<h2 id="2-6-32性能数据"><a href="#2-6-32性能数据" class="headerlink" title="2.6.32性能数据"></a>2.6.32性能数据</h2><p>升级前先看看目前的性能数据好对比（以下各个场景都是CPU基本跑到85%）</p>
<p><img src="/images/oss/b57c5ee5fe50ceb81cbad158f7b7aeeb.png" alt="image.png"></p>
<h2 id="一波N折的4-19"><a href="#一波N折的4-19" class="headerlink" title="一波N折的4.19"></a>一波N折的4.19</h2><p>阿里云上默认买到的ALinux2 OS（4.19），同样配置跑起来后，tps只有16000，比2.6.32的22000差了不少，心里只能暗暗骂几句坑爹的货，看了下各项指标，看不出来什么问题，就像是CPU能力不行一样。如果这个时候直接找内核同学，估计他们心里会说 X 是个什么东西？是不是你们测试有问题，是不是你们配置的问题，不要来坑我，内核性能我们每次发布都在实验室里跑过了，肯定是你们的应用问题。</p>
<p>所以要找到一个公认的场景下的性能差异。幸好通过qperf发现了一些性能差异。</p>
<h3 id="通过qperf来比较差异"><a href="#通过qperf来比较差异" class="headerlink" title="通过qperf来比较差异"></a>通过qperf来比较差异</h3><p>大包的情况下性能基本差不多，小包上差别还是很明显</p>
<pre><code>qperf -t 40 -oo msg_size:1  4.19 tcp_bw tcp_lat
tcp_bw:
    bw  =  2.13 MB/sec
tcp_lat:
    latency  =  224 us
tcp_bw:
    bw  =  2.15 MB/sec
tcp_lat:
    latency  =  226 us

qperf -t 40 -oo msg_size:1  2.6.32 tcp_bw tcp_lat
tcp_bw:
    bw  =  82 MB/sec
tcp_lat:
    latency  =  188 us
tcp_bw:
    bw  =  90.4 MB/sec
tcp_lat:
    latency  =  229 us
</code></pre><p>这下不用担心内核同学怼回来了，拿着这个数据直接找他们，可以稳定重现。</p>
<p>经过内核同学排查后，发现默认镜像做了一些安全加固，简而言之就是CPU拿出一部分资源做了其它事情，比如旁路攻击的补丁之类的，需要关掉（因为 X 的OS只给我们自己用，上面部署的代码都是X 产品自己的代码，没有客户代码，客户也不能够ssh连上X 产品节点）</p>
<pre><code>去掉 melt、spec 能到20000， 去掉sonypatch能到21000 
</code></pre><p>关闭的办法在 /etc/default/grub 里 GRUB_CMDLINE_LINUX 配置中增加这些参数：</p>
<pre><code>nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off
</code></pre><p>关掉之后的状态看起来是这样的：</p>
<pre><code>$sudo cat /sys/devices/system/cpu/vulnerabilities/*
Mitigation: PTE Inversion
Vulnerable; SMT Host state unknown
Vulnerable
Vulnerable
Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerable, STIBP: disabled
</code></pre><p>这块参考<a href="https://help.aliyun.com/knowledge_detail/154567.html?spm=a2c4g.11186623.2.12.887e38843VLHkv" target="_blank" rel="external">阿里云文档</a> 和<a href="https://help.aliyun.com/document_detail/102087.html?spm=a2c4g.11186623.6.721.4a732223pEfyNC" target="_blank" rel="external">这个</a></p>
<p>开启漏洞补丁（性能差）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># uname -r</div><div class="line">4.19.91-24.8.an8.x86_64</div><div class="line"></div><div class="line"># cat /proc/cmdline</div><div class="line">BOOT_IMAGE=(hd0,gpt2)/vmlinuz-4.19.91-24.8.an8.x86_64 root=UUID=ac9faf02-89c6-44d8-80b2-0f8ea1084fc3 ro console=tty0 crashkernel=auto console=ttyS0,115200 crashkernel=0M-2G:0M,2G-8G:192M,8G-:256M</div><div class="line">[root@Anolis82 ~]# sudo cat /sys/devices/system/cpu/vulnerabilities/*</div><div class="line">KVM: Mitigation: Split huge pages</div><div class="line">Mitigation: PTE Inversion; VMX: conditional cache flushes, SMT vulnerable</div><div class="line">Mitigation: Clear CPU buffers; SMT vulnerable</div><div class="line">Mitigation: PTI</div><div class="line">Mitigation: Speculative Store Bypass disabled via prctl and seccomp</div><div class="line">Mitigation: usercopy/swapgs barriers and __user pointer sanitization</div><div class="line">Mitigation: Full generic retpoline, IBPB: conditional, IBRS_FW, STIBP: conditional, RSB filling</div><div class="line">Not affected</div><div class="line">Mitigation: Clear CPU buffers; SMT vulnerable</div></pre></td></tr></table></figure>
<p>关闭（性能好）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@Anolis82 ~]# sudo cat /sys/devices/system/cpu/vulnerabilities/*</div><div class="line">KVM: Vulnerable</div><div class="line">Mitigation: PTE Inversion; VMX: vulnerable</div><div class="line">Vulnerable; SMT vulnerable</div><div class="line">Vulnerable</div><div class="line">Vulnerable</div><div class="line">Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers</div><div class="line">Vulnerable, IBPB: disabled, STIBP: disabled</div><div class="line">Not affected</div><div class="line">Vulnerable</div><div class="line">[root@Anolis82 ~]# cat /proc/cmdline</div><div class="line">BOOT_IMAGE=(hd0,gpt2)/vmlinuz-4.19.91-24.8.an8.x86_64 root=UUID=ac9faf02-89c6-44d8-80b2-0f8ea1084fc3 ro console=tty0 crashkernel=auto console=ttyS0,115200 nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off crashkernel=0M-2G:0M,2G-8G:192M,8G-:256M</div></pre></td></tr></table></figure>
<h3 id="4-9版本的内核性能"><a href="#4-9版本的内核性能" class="headerlink" title="4.9版本的内核性能"></a>4.9版本的内核性能</h3><p>但是性能还是不符合预期，总是比2.6.32差点。在中间经过几个星期排查不能解决问题，陷入僵局的过程中，尝试了一下4.9内核，果然有惊喜。</p>
<p>下图中对4.9的内核版本验证发现，tps能到24000，明显比2.6.32要好，所以传说中的新内核版本性能要好看来是真的，这下坚定了升级的念头，同时也看到了兜底的方案–最差就升级到4.9</p>
<p><img src="/images/oss/2f035e145f1bc41eb4a8b8bda8ed4ea2.png" alt="image.png"></p>
<p><strong>多队列是指网卡多队列功能，也是这次升级的一个动力。看起来在没达到单核瓶颈前，网卡多队列性能反而差点，这也符合预期</strong></p>
<h3 id="继续分析为什么4-19比4-9差了这么多"><a href="#继续分析为什么4-19比4-9差了这么多" class="headerlink" title="继续分析为什么4.19比4.9差了这么多"></a>继续分析为什么4.19比4.9差了这么多</h3><p>4.9和4.19这两个内核版本隔的近，比较好对比分析内核参数差异，4.19跟2.6.32差太多，比较起来很困难。</p>
<p>最终仔细对比了两者配置的差异，发现ALinux的4.19中 transparent_hugepage 是 madvise ,这对Java应用来说可不是太友好：</p>
<pre><code>$cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never
</code></pre><p>将其改到 always 后4.19的tps终于稳定在了28300</p>
<p><img src="/images/oss/081c08801adb36cdfd8ff62be54fce94.png" alt="image.png"></p>
<p>这个过程中花了两个月的一些其他折腾就不多说了，主要是内核补丁和transparent_hugepage导致了性能差异。</p>
<p>transparent_hugepage，在redis、mongodb、memcache等场景（很多小内存分配）是推荐关闭的，所以要根据不同的业务场景来选择开关。</p>
<p><strong>透明大页打开后在内存紧张的时候会触发sys飙高对业务会导致不可预期的抖动，同时存在已知内存泄漏的问题，我们建议是关掉的，如果需要使用，建议使用madvise方式或者hugetlbpage</strong></p>
<h2 id="一些内核版本、机型和CPU的总结"><a href="#一些内核版本、机型和CPU的总结" class="headerlink" title="一些内核版本、机型和CPU的总结"></a>一些内核版本、机型和CPU的总结</h2><p>到此终于看到不需要应用做什么改变，整体性能将近有30%的提升。 在这个测试过程中发现不同CPU对性能影响很明显，相同机型也有不同的CPU型号（性能差异在20%以上–这个太坑了）</p>
<p>性能方面 4.19&gt;4.9&gt;2.6.32</p>
<p>没有做3.10内核版本的比较</p>
<p>以下仅作为大家选择ECS的时候做参考。</p>
<h3 id="不同机型-CPU对性能的影响"><a href="#不同机型-CPU对性能的影响" class="headerlink" title="不同机型/CPU对性能的影响"></a>不同机型/CPU对性能的影响</h3><p>还是先说结论：</p>
<ul>
<li>CPU:内存为1:2机型的性能排序：c6-&gt;c5-&gt;sn1ne-&gt;hfc5-&gt;s1</li>
<li>CPU:内存为1:4机型的性能排序：g6-&gt;g5-&gt;sn2ne-&gt;hfg5-&gt;sn2</li>
</ul>
<p>性能差异主要来源于CPU型号的不同</p>
<pre><code>c6/g6:                  Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz
c5/g5/sn1ne/sn2ne:      Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
</code></pre><p>8269比8163大概好5-10%，价格便宜一点点，8163比E5-2682好20%以上，价格便宜10%（该买什么机型你懂了吧，价格是指整个ECS，而不是单指CPU）</p>
<p>要特别注意sn1ne/sn2ne 是8163和E5-2682 两种CPU型号随机的，如果买到的是E5-2682就自认倒霉吧</p>
<p>C5的CPU都是8163，相比sn1ne价格便宜10%，网卡性能也一样。但是8核以上的sn1ne机型就把网络性能拉开了（价格还是维持c5便宜10%），从点查场景的测试来看网络不会成为瓶颈，到16核机型网卡多队列才会需要打开。</p>
<p>顺便给一下部分机型的包月价格比较：</p>
<p><img src="/images/oss/7c8b107fb12e285c8eab2c2d136bbd4e.png" alt="image.png"></p>
<p>官方给出的CPU数据：</p>
<p><img src="/images/oss/5f57f4228621378d14ffdd124fe54626.png" alt="image.png"></p>
<h2 id="4-19内核在MySQL-Server场景下的性能比较"><a href="#4-19内核在MySQL-Server场景下的性能比较" class="headerlink" title="4.19内核在MySQL Server场景下的性能比较"></a>4.19内核在MySQL Server场景下的性能比较</h2><p>这只是sysbench点查场景粗略比较，因为本次的目标是对X 产品性能的改进</p>
<p><img src="/images/oss/4f276e93cb914b3cdd312423be63c376.png" alt="image.png"></p>
<p>（以上表格数据主要由 内核团队和我一起测试得到）</p>
<p><strong>重点注意2.6.32不但tps差30%，并发能力也差的比较多，如果同样用100个并发压2.6.32上的MySQL，TPS在30000左右。只有在减少并发到20个的时候压测才能达到图中最好的tps峰值：45000. </strong></p>
<h2 id="新内核除了性能提升外带来的便利性"><a href="#新内核除了性能提升外带来的便利性" class="headerlink" title="新内核除了性能提升外带来的便利性"></a>新内核除了性能提升外带来的便利性</h2><p>升级内核带来的性能提升只是在极端场景下才会需要，大部分时候我们希望节省开发人员的时间，提升工作效率。于是X 产品在新内核的基础上定制如下一些便利的工具。</p>
<h3 id="麻烦的网络重传率"><a href="#麻烦的网络重传率" class="headerlink" title="麻烦的网络重传率"></a>麻烦的网络重传率</h3><p>通过tsar或者其它方式发现网络重传率有点高，有可能是别的管理端口重传率高，有可能是往外连其它服务端口重传率高等，尤其是在整体流量小的情况下一点点管理端口的重传包拉升了整个机器的重传率，严重干扰了问题排查，所以需要进一步确认重传发生在哪个进程的哪个端口上，是否真正影响了我们的业务。</p>
<p>在2.6.32内核下的排查过程是：抓包，然后写脚本分析（或者下载到本地通过wireshark分析），整个过程比较麻烦，需要的时间也比较长。那么在新镜像中我们可以利用内核自带的bcc来快速得到这些信息</p>
<pre><code>sudo /usr/share/bcc/tools/tcpretrans -l
</code></pre><p><img src="/images/oss/c68cc22b2e6eb7dd51d8613c5e79e88c.png" alt="image.png"></p>
<p>从截图可以看到重传时间、pid、tcp四元组、状态，针对重传发生的端口和阶段（SYN_SENT握手、ESTABLISHED）可以快速推断导致重传的不同原因。</p>
<p>再也不需要像以前一样抓包、下载、写脚本分析了。</p>
<h3 id="通过perf-top直接看Java函数的CPU消耗"><a href="#通过perf-top直接看Java函数的CPU消耗" class="headerlink" title="通过perf top直接看Java函数的CPU消耗"></a>通过perf top直接看Java函数的CPU消耗</h3><p>这个大家都比较了解，不多说，主要是top的时候能够把java函数给关联上，直接看截图：</p>
<pre><code>sh ~/tools/perf-map-agent/bin/create-java-perf-map.sh pid
sudo perf top
</code></pre><p><img src="/images/oss/1568775788220-32745082-5155-4ecd-832a-e814a682c0df.gif" alt=""></p>
<h3 id="快速定位Java中的锁等待"><a href="#快速定位Java中的锁等待" class="headerlink" title="快速定位Java中的锁等待"></a>快速定位Java中的锁等待</h3><p>如果CPU跑不起来，可能会存在锁瓶颈，需要快速找到它们</p>
<p>如下测试中上面的11万tps是解决掉锁后得到的，下面的4万tps是没解决锁等待前的tps：</p>
<pre><code>#[ 210s] threads: 400, tps: 0.00, reads/s: 115845.43, writes/s: 0.00, response time: 7.57ms (95%)
#[ 220s] threads: 400, tps: 0.00, reads/s: 116453.12, writes/s: 0.00, response time: 7.28ms (95%)
#[ 230s] threads: 400, tps: 0.00, reads/s: 116400.31, writes/s: 0.00, response time: 7.33ms (95%)
#[ 240s] threads: 400, tps: 0.00, reads/s: 116025.35, writes/s: 0.00, response time: 7.48ms (95%)

#[ 250s] threads: 400, tps: 0.00, reads/s: 45260.97, writes/s: 0.00, response time: 29.57ms (95%)
#[ 260s] threads: 400, tps: 0.00, reads/s: 41598.41, writes/s: 0.00, response time: 29.07ms (95%)
#[ 270s] threads: 400, tps: 0.00, reads/s: 41939.98, writes/s: 0.00, response time: 28.96ms (95%)
#[ 280s] threads: 400, tps: 0.00, reads/s: 40875.48, writes/s: 0.00, response time: 29.16ms (95%)
#[ 290s] threads: 400, tps: 0.00, reads/s: 41053.73, writes/s: 0.00, response time: 29.07ms (95%)
</code></pre><p>下面这行命令得到如下等锁的top 10堆栈（<a href="https://github.com/jvm-profiling-tools/async-profiler" target="_blank" rel="external">async-profiler</a>）：</p>
<pre><code>$~/tools/async-profiler/profiler.sh -e lock -d 5 1560

--- 1687260767618 ns (100.00%), 91083 samples
 [ 0] ch.qos.logback.classic.sift.SiftingAppender
 [ 1] ch.qos.logback.core.AppenderBase.doAppend
 [ 2] ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders
 [ 3] ch.qos.logback.classic.Logger.appendLoopOnAppenders
 [ 4] ch.qos.logback.classic.Logger.callAppenders
 [ 5] ch.qos.logback.classic.Logger.buildLoggingEventAndAppend
 [ 6] ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus
 [ 7] ch.qos.logback.classic.Logger.info
 [ 8] com.*****.logger.slf4j.Slf4jLogger.info
 [ 9] com.*****.utils.logger.support.FailsafeLogger.info
 [10] com.*****.util.LogUtils.recordSql



&quot;ServerExecutor-3-thread-480&quot; #753 daemon prio=5 os_prio=0 tid=0x00007f8265842000 nid=0x26f1 waiting for monitor entry [0x00007f82270bf000]
  java.lang.Thread.State: BLOCKED (on object monitor)
    at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:64)
    - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
    at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:48)
    at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:282)
    at ch.qos.logback.classic.Logger.callAppenders(Logger.java:269)
    at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:470)
    at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:424)
    at ch.qos.logback.classic.Logger.info(Logger.java:628)
    at com.****.utils.logger.slf4j.Slf4jLogger.info(Slf4jLogger.java:42)
    at com.****.utils.logger.support.FailsafeLogger.info(FailsafeLogger.java:102)
    at com.****.util.LogUtils.recordSql(LogUtils.java:115)

          ns  percent  samples  top
  ----------  -------  -------  ---
160442633302   99.99%    38366  ch.qos.logback.classic.sift.SiftingAppender
    12480081    0.01%       19  java.util.Properties
     3059572    0.00%        9  com.***.$$$.common.IdGenerator
      244394    0.00%        1  java.lang.Object
</code></pre><p>堆栈中也可以看到大量的：</p>
<pre><code>- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - locked &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
</code></pre><p>当然还有很多其他爽得要死的命令，比如一键生成火焰图等，不再一一列举，可以从业务层面的需要从这次镜像升级的便利中将他们固化到镜像中，以后排查问题不再需要繁琐的安装、配置、调试过程了。</p>
<h2 id="跟内核无关的应用层的优化"><a href="#跟内核无关的应用层的优化" class="headerlink" title="跟内核无关的应用层的优化"></a>跟内核无关的应用层的优化</h2><p>到此我们基本不用任何改动得到了30%的性能提升，但是对整个应用来说，通过以上工具让我们看到了一些明显的问题，还可以从应用层面继续提升性能。</p>
<p>如上描述通过锁排序定位到logback确实会出现锁瓶颈，同时在一些客户场景中，因为网盘的抖动也带来了灾难性的影响，所以日志需要异步处理，经过异步化后tps 达到了32000，关键的是rt 95线下降明显，这个rt下降对X 产品这种Proxy类型的应用是非常重要的（经常被客户指责多了一层转发，rt增加了）。</p>
<p>日志异步化和使用协程后的性能数据：</p>
<p><img src="/images/oss/bec4e8105091bc4b8a263aef245c0ce9.png" alt="image.png"></p>
<h3 id="Wisp2-协程带来的红利"><a href="#Wisp2-协程带来的红利" class="headerlink" title="Wisp2 协程带来的红利"></a>Wisp2 协程带来的红利</h3><p>在整个测试过程中都很顺利，只是<strong>发现Wisp2在阻塞不明显的场景下，抖的厉害</strong>。简单来说就是压力比较大的话Wisp2表现很稳定，一旦压力一般（这是大部分应用场景），Wisp2表现像是一会是协程状态，一会是没开携程状态，系统的CS也变化很大。</p>
<p>比如同一测试过程中tps抖动明显，从15000到50000：</p>
<p><img src="/images/oss/1550cc74116a56220d25e1434a675d14.png" alt="image.png"></p>
<p>100个并发的时候cs很小，40个并发的时候cs反而要大很多：</p>
<p><img src="/images/oss/3f79909f89889459d1f0dfe4fa0a2f53.png" alt="image.png"></p>
<p>最终在 @梁希 同学的攻关下发布了新的jdk版本，问题基本都解决了。不但tps提升明显，rt也有很大的下降。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>感谢 @夷则 团队对这次内核版本升级的支持，感谢 @雏雁 @飞绪 @李靖轩(无牙) @齐江(窅默) @梁希 等大佬的支持。</p>
<p>最终应用不需要任何改动可以得到 30%的性能提升，经过开启协程等优化后应用有将近80%的性能提升，同时平均rt下降了到原来的60%，rt 95线下降到原来的40%。</p>
<p>快点升级你们的内核，用上协程吧。同时考虑下在你们的应用中用上X 产品。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://help.aliyun.com/document_detail/25378.html" target="_blank" rel="external">https://help.aliyun.com/document_detail/25378.html</a></p>
<p><a href="https://help.aliyun.com/document_detail/55263.html" target="_blank" rel="external">https://help.aliyun.com/document_detail/55263.html</a></p>
<p><a href="https://help.aliyun.com/document_detail/52559.html" target="_blank" rel="external">https://help.aliyun.com/document_detail/52559.html</a> (网卡)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/16/Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的/" itemprop="url">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-16T12:30:03+08:00">
                2019-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Intel-PAUSE指令变化如何影响MySQL的性能"><a href="#Intel-PAUSE指令变化如何影响MySQL的性能" class="headerlink" title="Intel PAUSE指令变化如何影响MySQL的性能"></a>Intel PAUSE指令变化如何影响MySQL的性能</h1><h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>x86、arm指令都很多，无论是应用程序员还是数据库内核研发大多时候都不需要对这些指令深入理解，但是 Pause 指令和数据库操作太紧密了，本文通过一次非常有趣的性能优化来引入对 Pause 指令的理解，期望可以事半功倍地搞清楚 CPU指令集是如何影响你的程序的。</p>
<p>文章分成两大部分，第一部分是 MySQL 集群的一次全表扫描性能优化过程； 第二部分是问题解决后的原理分析以及Pause指令的来龙去脉和优缺点以及应用场景分析。</p>
<h2 id="业务结构"><a href="#业务结构" class="headerlink" title="业务结构"></a>业务结构</h2><p>为理解方便做了部分简化：</p>
<p>client -&gt; Tomcat -&gt; LVS -&gt; MySQL（32 个 MySQLD实例集群，每个实例8Core）</p>
<h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>通过 client 压 Tomcat 和 MySQL 集群（对数据做分库分表），MySQL 集群是32个实例，每个业务 SQL 都需要经过 Tomcat 拆分成 256 个 SQL 发送给 32 个MySQL（每个MySQL上有8个分库），这 256 条下发给 MySQL 的 SQL 不是完全串行，但也不是完全并行，有一定的并行性。</p>
<p>业务 SQL 如下是一个简单的select sum求和，这个 SQL在每个MySQL上都很快（有索引）</p>
<pre><code>SELECT SUM(emp_arr_amt) FROM table_c WHERE INSUTYPE=&apos;310&apos; AND Revs_Flag=&apos;Z&apos; AND accrym=&apos;201910&apos; AND emp_no=&apos;1050457&apos;;
</code></pre><h2 id="监控指标说明"><a href="#监控指标说明" class="headerlink" title="监控指标说明"></a>监控指标说明</h2><ul>
<li>后述或者截图中的逻辑RT/QPS是指 client 上看到的Tomcat的 RT 和 QPS； </li>
<li>RT ：response time 请求响应时间，判断性能瓶颈的唯一指标;</li>
<li>物理RT/QPS是指Tomcat看到的MySQL  RT 和QPS（这里的 RT 是指到达Tomcat节点网卡的 RT ，所以还包含了网络消耗）</li>
</ul>
<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>通过client压一个Tomcat节点+32个MySQL，QPS大概是430，Tomcat节点CPU跑满，MySQL  RT 是0.5ms，增加一个Tomcat节点，QPS大概是700，Tomcat CPU接近跑满，MySQL  RT 是0.6ms，到这里性能基本随着扩容线性增加，是符合预期的。</p>
<p>继续增加Tomcat节点来横向扩容性能，通过client压三个Tomcat节点+32个MySQL，QPS还是700，Tomcat节点CPU跑不满，MySQL  RT 是0.8ms，这就严重不符合预期了。</p>
<p>性能压测原则：</p>
<blockquote>
<p>加并发QPS不再上升说明到了某个瓶颈，哪个环节RT增加最多瓶颈就在哪里</p>
</blockquote>
<p><img src="/images/951413iMgBlog/image-20221026145848312.png" alt="image-20221026145848312"></p>
<p><strong>到这里一切都还是符合我们的经验的，看起来就是 MySQL 有瓶颈（RT 增加明显）。</strong></p>
<h2 id="排查-MySQL"><a href="#排查-MySQL" class="headerlink" title="排查 MySQL"></a>排查 MySQL</h2><p>现场DBA通过监控看到MySQL CPU不到20%，没有慢查询，并且尝试用client越过所有中间环节直接压其中一个MySQL，可以将 MySQL CPU 跑满，这时的QPS大概是38000（对应上面的场景client QPS为700的时候，单个MySQL上的QPS才跑到6000) 所以排除了MySQL的嫌疑(这个推理不够严谨为后面排查埋下了大坑)。</p>
<p>那么接下来的嫌疑在网络、LVS 等中间环节上。</p>
<h2 id="LVS和网络的嫌疑"><a href="#LVS和网络的嫌疑" class="headerlink" title="LVS和网络的嫌疑"></a>LVS和网络的嫌疑</h2><p>首先通过大查询排除了带宽的问题，因为这里都是小包，pps到了72万，很自然想到了网关、LVS的限流之类的</p>
<p>pps监控，这台物理机有4个MySQL实例上，pps 9万左右，9*32/4=72万<br><img src="/images/oss/b84245c17e213de528f2ad8090d504f6.png" alt="image.png"></p>
<p>…………（省略巨长的分析、拉人、扯皮过程）</p>
<p>最终所有网络因素都被排除，核心证据是：做压测的时候反复从 Tomcat 上 ping 后面的MySQL，RT 跟没有压力的时候一样，也说明了网络没有问题(请思考这个 ping 的作用)。</p>
<h2 id="问题的确认"><a href="#问题的确认" class="headerlink" title="问题的确认"></a>问题的确认</h2><p>尝试在Tomcat上打开日志，并将慢 SQL 阈值设置为100ms，这个时候确实能从日志中看到大量MySQL上的慢查询，因为这个SQL需要在Tomcat上做拆分成256个SQL，同时下发，一旦有一个SQL返回慢，整个请求就因为这个短板被拖累了。平均 RT  0.8ms，但是经常有超过100ms的话对整体影响还是很大的。</p>
<p>将Tomcat记录下来的慢查询（Tomcat增加了一个唯一id下发给MySQL）到MySQL日志中查找，果然发现MySQL上确实慢了，所以到这里基本确认是MySQL的问题，终于不用再纠结是否是网络问题了。</p>
<p>同时在Tomcat进行抓包，对网卡上的 RT 进行统计分析：</p>
<p><img src="/images/oss/ffd66d9a6098979b555dfb00d3494255.png" alt="image.png"></p>
<p>上是Tomcat上抓到的每个sql的物理RT 平均值，上面是QPS 430的时候， RT  0.6ms，下面是3个server，QPS为700，但是 RT 上升到了0.9ms，基本跟Tomcat监控记录到的物理RT一致。如果MySQL上也有类似抓包计算 RT 时间的话可以快速排除网络问题。</p>
<p>网络抓包得到的 RT 数据更容易被所有人接受。尝试过在MySQL上抓包，但是因为LVS模块的原因，进出端口、ip都被修改过，所以没法分析一个流的响应时间。</p>
<h2 id="重心再次转向MySQL"><a href="#重心再次转向MySQL" class="headerlink" title="重心再次转向MySQL"></a>重心再次转向MySQL</h2><p>这个时候因为问题点基本确认，再去查看MySQL是否有问题的重心都不一样了，不再只是看看CPU和慢查询，这个问题明显更复杂一些。</p>
<blockquote>
<p>教训：CPU只是影响性能的一个因素，RT 才是结果，要追着 RT 跑，而不是只看 CPU</p>
</blockquote>
<p>通过监控发现MySQL CPU虽然一直不高，但是经常看到running thread飙到100多，很快又降下去了，看起来像是突发性的并发查询请求太多导致了排队等待，每个MySQL实例是8Core的CPU，尝试将MySQL实例扩容到16Core（只是为了验证这个问题），QPS确实可以上升到1000（没有到达理想的1400）。</p>
<p>这是Tomcat上监控到的MySQL状态：<br><img src="/images/oss/e73c1371a02106a52f8a13f89a9dd9ad.png" alt="image.png"></p>
<p>同时在MySQL机器上通过vmstat也可以看到这种飙升：<br><img src="/images/oss/4dbd9dff9deacec0e9911e3a7d025578.png" alt="image.png"></p>
<p>以上分析可以清晰看到虽然 MySQL 整体压力不大，但是似乎会偶尔来一波卡顿、running 任务飙升。</p>
<p>像这种短暂突发性的并发流量似乎监控都很难看到（基本都被平均掉了），只有一些实时性监控偶尔会采集到这种短暂突发性飙升，这也导致了一开始忽视了MySQL。</p>
<p>所以接下来的核心问题就是MySQL为什么会有这种飙升、这种飙升的影响到底是什么？</p>
<h2 id="perf-top"><a href="#perf-top" class="headerlink" title="perf top"></a>perf top</h2><p>直接用 perf 看下 MySQLD 进程，发现 ut_delay 高得不符合逻辑：</p>
<p><img src="/images/oss/cd145c494c074e01e9d2d1d5583a87a0.png" alt="image.png"></p>
<p>展开看一下，基本是在优化器中做索引命中行数的选择：</p>
<p><img src="/images/oss/46d5f5ee5c58d7090a71164e645ccf79.png" alt="image.png" style="zoom: 67%;"></p>
<p>跟直接在 MySQL 命令行中通过 show processlist看到的基本一致：</p>
<p><img src="/images/oss/89cccebe41a8b8461ea75586b61b929f.png" alt="image.png" style="zoom:50%;"></p>
<p>这是 MySQL 的优化器在对索引进行统计，统计的时候要加锁，thread running 抖动的时候通过 show processlist 看到很多 thread处于 statistics 状态。也就是高并发下加锁影响了 CPU 压不上去同时 RT 剧烈增加。</p>
<p>这里ut_delay 消耗了 28% 的 CPU 肯定太不正常了，于是将 innodb_spin_wait_delay 从 30 改成 6 后性能立即上去了，继续增加 Tomcat 节点，QPS也可以线性增加。</p>
<blockquote>
<p>耗CPU最高的调用函数栈是…<code>mutex_spin_wait</code>-&gt;<code>ut_delay</code>，属于锁等待的逻辑。InnoDB在这里用的是自旋锁，锁等待是通过调用 ut_delay 让 CPU做空循环在等锁的时候不释放CPU从而避免上下文切换，会消耗比较高的CPU。</p>
</blockquote>
<h2 id="最终的性能"><a href="#最终的性能" class="headerlink" title="最终的性能"></a>最终的性能</h2><p>调整参数 innodb_spin_wait_delay=6 后在4个Tomcat节点下，并发40时，QPS跑到了1700，物理RT：0.7，逻辑RT：19.6，cpu：90%，这个时候只需要继续扩容 Tomcat 节点的数量就可以增加QPS<br><img src="/images/oss/48c976f989747266f9892403794996c0.png" alt="image.png"></p>
<p>再跟调整前比较一下，innodb_spin_wait_delay=30，并发40时，QPS 500+，物理RT：2.6ms 逻辑RT：72.1ms cpu：37%<br><img src="/images/oss/fdb459972926cff371f5f5ab703790bb.png" alt="image.png"></p>
<p>再看看调整前压测的时候的vmstat和tsar –cpu，可以看到process running抖动明显<br><img src="/images/oss/4dbd9dff9deacec0e9911e3a7d025578.png" alt="image.png"></p>
<p>对比修改delay后的process running就很稳定了，即使QPS大了3倍<br><img src="/images/oss/ed46d35161ea28352acd4289a3e9ddad.png" alt="image.png"></p>
<h2 id="事后思考和分析"><a href="#事后思考和分析" class="headerlink" title="事后思考和分析"></a>事后思考和分析</h2><p>到这里问题得到了完美解决，但是不禁要问为什么？ut_delay 是怎么工作的？ 和 innodb_spin_wait_delay 以及自旋锁的关系？</p>
<h2 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h2><p>既然调整 innodb_spin_wait_delay 就能解决这个问题，那就要先分析一下 innodb_spin_wait_delay 的作用</p>
<h3 id="关于-innodb-spin-wait-delay"><a href="#关于-innodb-spin-wait-delay" class="headerlink" title="关于 innodb_spin_wait_delay"></a>关于 innodb_spin_wait_delay</h3><p>innodb通过大量的自旋锁(比如 <code>InnoDB</code> <a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_mutex" target="_blank" rel="external">mutexes</a> and <a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_rw_lock" target="_blank" rel="external">rw-locks</a>)来用高CPU消耗避免上下文切换，这是自旋锁的正确使用方式，在多核场景下，它们一起自旋抢同一个锁，容易造成<a href="https://stackoverflow.com/questions/30684974/are-cache-line-ping-pong-and-false-sharing-the-same" target="_blank" rel="external">cache ping-pong</a>，进而多个CPU核之间会互相使对方缓存部分无效。所以这里<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-performance-spin_lock_polling.html" target="_blank" rel="external">innodb通过增加 innodb_spin_wait_delay 和 Pause 配合来缓解cache ping-pong</a>，也就是本来通过CPU 高速自旋抢锁，换成了抢锁失败后 delay一下（Pause）但是不释放CPU，delay 时间到后继续抢锁，也就是把连续的自旋抢锁转换成了更稀疏的点状的抢锁（间隔的 delay是个随机数），这样不但避免了上下文切换也大大减少了cache ping-pong。</p>
<h3 id="自旋锁如何减少了cache-ping-pong"><a href="#自旋锁如何减少了cache-ping-pong" class="headerlink" title="自旋锁如何减少了cache ping-pong"></a>自旋锁如何减少了cache ping-pong</h3><p>多线程竞争锁的时候，加锁失败的线程会“忙等待”，直到它拿到锁。什么叫“忙等待”呢？它并不意味着一直执行 CAS 函数，而是会与 CPU 紧密配合 ，它通过 CPU 提供的 <code>PAUSE</code> 指令，减少循环等待时的cache ping-pong和耗电量；对于单核 CPU，忙等待并没有意义，此时它会主动把线程休眠。</p>
<h3 id="X86-PAUSE-指令"><a href="#X86-PAUSE-指令" class="headerlink" title="X86 PAUSE 指令"></a>X86 PAUSE 指令</h3><p>X86设计了Pause指令，也就是调用 Pause 指令的代码会抢着 CPU 不释放，但是CPU 会打个盹，比如 10个时钟周期，相对一次上下文切换是大几千个时钟周期。</p>
<p>这样应用一旦自旋抢锁失败可以先 Pause 一下，只是这个Pause 时间对于 MySQL 来说还不够久，所以需要增加参数 innodb_spin_wait_delay 来将休息时间放大一些。</p>
<p>在我们的这个场景下对每个 SQL的 RT 抖动非常敏感（放大256倍），所以过高的 delay 会导致部分SQL  RT  变高。</p>
<p>函数 ut_delay(ut_rnd_interval(0, srv_spin_wait_delay)) 用来执行这个delay：</p>
<pre><code>/***************************MySQL代码****************************//**
Runs an idle loop on CPU. The argument gives the desired delay
in microseconds on 100 MHz Pentium + Visual C++.
@return dummy value */
UNIV_INTERN
ulint
ut_delay(ulint delay)  //delay 是[0,innodb_spin_wait_delay)之间的一个随机数
{
        ulint   i, j;

        UT_LOW_PRIORITY_CPU();

        j = 0;

        for (i = 0; i &lt; delay * 50; i++) {  //delay 放大50倍
                j += i;
                UT_RELAX_CPU();             //调用 CPU Pause
        }

        UT_RESUME_PRIORITY_CPU();

        return(j);
}
</code></pre><p>innodb_spin_wait_delay的默认值为6. spin 等待延迟是一个动态全局参数，您可以在MySQL选项文件（my.cnf或my.ini）中指定该参数，或者在运行时使用SET GLOBAL 来修改。在我们的MySQL配置中默认改成了30，导致了这个问题。</p>
<h3 id="CPU-为什么要有Pause"><a href="#CPU-为什么要有Pause" class="headerlink" title="CPU 为什么要有Pause"></a>CPU 为什么要有Pause</h3><p>首先可以看到 Pause 指令的作用：</p>
<ul>
<li>避免上下文切换，应用层想要休息可能会用yield、sleep，这两操作对于CPU来说太重了(伴随上下文切换)</li>
<li>能给超线程腾出计算能力（HT共享核，但是有单独的寄存器等存储单元，CPU Pause的时候，对应的HT可以占用计算资源），比如同一个core上先跑多个Pause，同时再跑 nop 指令，这时 nop指令的 IPC基本不受Pause的影响</li>
<li>节能（CPU可以休息、但是不让出来），CPU Pause 的时候你从 top 能看到 CPU 100%，但是不耗能。</li>
</ul>
<p>所以有了 Pause 指令后能够提高超线程的利用率,节能，减少上下文切换提高自旋锁的效率。</p>
<blockquote>
<p><a href="https://www.reddit.com/r/intel/comments/hogk2n/research_on_the_impact_of_intel_Pause_instruction/" target="_blank" rel="external">The PAUSE instruction is first introduced</a> for Intel Pentium 4 processor to improve the performance of “spin-wait loop”. The PAUSE instruction is typically used with software threads executing on two logical processors located in the same processor core, waiting for a lock to be released. Such short wait loops tend to last between tens and a few hundreds of cycles. When the wait loop is expected to last for thousands of cycles or more, it is preferable to yield to the operating system by calling one of the OS synchronization API functions, such as WaitForSingleObject on Windows OS.</p>
<p>An Intel® processor suffers a severe performance penalty when exiting the loop because it detects a possible memory order violation. The PAUSE instruction provides a hint to the processor that the code sequence is a spin-wait loop. The processor uses this hint to avoid the memory order violation in most situations. The PAUSE instruction can improve the performance of the processors supporting Intel Hyper-Threading Technology when executing “spin-wait loops”. With Pause instruction, processors are able to avoid the memory order violation and pipeline flush, and reduce power consumption through pipeline stall.</p>
</blockquote>
<p><strong>从intel sdm手册以及实际测试验证来看，Pause 指令在执行过程中，基本不占用流水线执行资源。</strong></p>
<h3 id="Skylake-架构的8163-和-Broadwell架构-E5-2682-CPU型号的不同"><a href="#Skylake-架构的8163-和-Broadwell架构-E5-2682-CPU型号的不同" class="headerlink" title="Skylake 架构的8163 和 Broadwell架构 E5-2682 CPU型号的不同"></a>Skylake 架构的8163 和 Broadwell架构 E5-2682 CPU型号的不同</h3><p>为什么用得好好的 innodb_spin_wait_delay 参数这次就不行了呢？</p>
<p>这是因为以前业务一直使用的是 E5-2682 CPU，这次用的是新一代架构的 Skylake 8163，那这两款CPU在这里的核心差别是？</p>
<p>在Intel 64-ia-32-architectures-optimization-manual手册中提到：</p>
<blockquote>
<p>The latency of the PAUSE instruction in prior generation microarchitectures is about 10 cycles, whereas in Skylake microarchitecture it has been extended to as many as 140 cycles.</p>
<p><a href="https://xem.github.io/minix86/manual/intel-x86-and-64-manual-vol3/o_fe12b1e2a880e0ce-302.html" target="_blank" rel="external">The PAUSE instruction can improves the performance</a> of processors supporting Intel Hyper-Threading Technology when executing “spin-wait loops” and other routines where one thread is accessing a shared lock or semaphore in a tight polling loop. When executing a spin-wait loop, the processor can suffer a severe performance penalty when exiting the loop because it detects a possible memory order violation and flushes the core processor’s pipeline. The PAUSE instruction provides a hint to the processor that the code sequence is a spin-wait loop. The processor uses this hint to avoid the memory order violation and prevent the pipeline flush. In addition, the PAUSE instruction de-<br>pipelines the spin-wait loop to prevent it from consuming execution resources excessively and consume power needlessly. (See<a href="https://xem.github.io/minix86/manual/intel-x86-and-64-manual-vol3/o_fe12b1e2a880e0ce-305.html" target="_blank" rel="external"> Section 8.10.6.1, “Use the PAUSE Instruction in Spin-Wait Loops,” for more </a>information about using the PAUSE instruction with IA-32 processors supporting Intel Hyper-Threading Technology.)</p>
</blockquote>
<p>也就是<strong>Skylake架构的CPU的PAUSE指令从之前的10 cycles 改成了 140 cycles。</strong>这可是14倍的变化呀。</p>
<p>MySQL 使用 innodb_spin_wait_delay 控制 spin lock等待时间，等待时间时间从0*50个Pause到innodb_spin_wait_delay*50个Pause。<br>以前 innodb_spin_wait_delay 默认配置30，对于E5-2682 CPU，等待的最长时间为：<br>30 * 50 * 10=15000 cycles，对于2.5GHz的CPU，等待时间为6us。<br>对应计算 Skylake CPU的等待时间：30 *50 *140=210000 cycles，CPU主频也是2.5GHz，等待时间84us。</p>
<p>E5-2682 CPU型号在不同的delay参数和不同并发压力下的写入性能数据：</p>
<p><img src="/images/951413iMgBlog/image-20221026153750159.png" alt="image-20221026153750159"></p>
<p>Skylake 8163 CPU型号在不同的delay参数和不同并发压力下的写入性能数据：</p>
<p><img src="/images/951413iMgBlog/image-20221026153813774.png" alt="image-20221026153813774"></p>
<p>==因为8163的cycles从10改到了140，所以可以看到delay参数对性能的影响更加陡峻。==</p>
<h2 id="总结分析"><a href="#总结分析" class="headerlink" title="总结分析"></a>总结分析</h2><p>Intel CPU 架构不同使得 Pause 指令的CPU Cycles不同导致了 MySQL innodb_spin_wait_delay 在 spin lock 失败的时候（此时需要 Pause<em> innodb_spin_wait_delay</em>N）delay更久，使得调用方看到了MySQL更大的 RT ，进而导致 Tomcat Server上业务并发跑不起来，所以最终压力上不去。</p>
<p>在长链路的排查中，细化定位是哪个节点出了问题是最难的，要盯住 RT 而不是 CPU。</p>
<p>欲速则不达，做压测的时候还是要老老实实地从一个并发开始观察QPS、 RT ，然后一直增加压力到压不上去了，再看QPS、 RT 变化，然后确认瓶颈点。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://cloud.tencent.com/developer/article/1005284" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1005284</a></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-performance-spin_lock_polling.html" target="_blank" rel="external">mysql doc</a></p>
<p><a href="http://oliveryang.net/2018/01/cache-false-sharing-debug" target="_blank" rel="external">Cache Line 伪共享发现与优化</a></p>
<p><a href="https://en.wikichip.org/w/images/e/eb/intel-ref-248966-037.pdf" target="_blank" rel="external">intel spec</a></p>
<p><a href="https://mp.weixin.qq.com/s/dlKC13i9Z8wjDDiU2tig6Q" target="_blank" rel="external">Intel PAUSE指令变化影响到MySQL的性能，该如何解决？</a></p>
<p><a href="https://topic.atatech.org/articles/173194" target="_blank" rel="external">ARM软硬件协同设计：锁优化</a>, arm不同于x86，用的是yield来代替Pause</p>
<p><a href="http://cr.openjdk.java.net/~dchuyko/8186670/yield/spinwait.html" target="_blank" rel="external">http://cr.openjdk.java.net/~dchuyko/8186670/yield/spinwait.html</a></p>
<p><a href="https://aloiskraus.wordpress.com/2018/06/16/why-skylakex-cpus-are-sometimes-50-slower-how-intel-has-broken-existing-code/" target="_blank" rel="external">https://aloiskraus.wordpress.com/2018/06/16/why-skylakex-cpus-are-sometimes-50-slower-how-intel-has-broken-existing-code/</a> Windows+.NET 平台下的分析过程及修复方案</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/12/16/Intel PAUSE指令变化如何影响MySQL的性能/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/16/Intel PAUSE指令变化如何影响MySQL的性能/" itemprop="url">Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-16T12:30:03+08:00">
                2019-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Intel-PAUSE指令变化如何影响MySQL的性能"><a href="#Intel-PAUSE指令变化如何影响MySQL的性能" class="headerlink" title="Intel PAUSE指令变化如何影响MySQL的性能"></a>Intel PAUSE指令变化如何影响MySQL的性能</h1><h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>x86、arm指令都很多，无论是应用程序员还是数据库内核研发大多时候都不需要对这些指令深入理解，但是 Pause 指令和数据库操作太紧密了，本文通过一次非常有趣的性能优化来引入对 Pause 指令的理解，期望可以事半功倍地搞清楚 CPU指令集是如何影响你的程序的。</p>
<p>文章分成两大部分，第一部分是 MySQL 集群的一次全表扫描性能优化过程； 第二部分是问题解决后的原理分析以及Pause指令的来龙去脉和优缺点以及应用场景分析。</p>
<h2 id="业务结构"><a href="#业务结构" class="headerlink" title="业务结构"></a>业务结构</h2><p>为理解方便做了部分简化：</p>
<p>client -&gt; Tomcat -&gt; LVS -&gt; MySQL（32 个 MySQLD实例集群，每个实例8Core）</p>
<h2 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h2><p>通过 client 压 Tomcat 和 MySQL 集群（对数据做分库分表），MySQL 集群是32个实例，每个业务 SQL 都需要经过 Tomcat 拆分成 256 个 SQL 发送给 32 个MySQL（每个MySQL上有8个分库），这 256 条下发给 MySQL 的 SQL 不是完全串行，但也不是完全并行，有一定的并行性。</p>
<p>业务 SQL 如下是一个简单的select sum求和，这个 SQL在每个MySQL上都很快（有索引）</p>
<pre><code>SELECT SUM(emp_arr_amt) FROM table_c WHERE INSUTYPE=&apos;310&apos; AND Revs_Flag=&apos;Z&apos; AND accrym=&apos;201910&apos; AND emp_no=&apos;1050457&apos;;
</code></pre><h2 id="监控指标说明"><a href="#监控指标说明" class="headerlink" title="监控指标说明"></a>监控指标说明</h2><ul>
<li>后述或者截图中的逻辑RT/QPS是指 client 上看到的Tomcat的 RT 和 QPS； </li>
<li>RT ：response time 请求响应时间，判断性能瓶颈的唯一指标;</li>
<li>物理RT/QPS是指Tomcat看到的MySQL  RT 和QPS（这里的 RT 是指到达Tomcat节点网卡的 RT ，所以还包含了网络消耗）</li>
</ul>
<h2 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h2><p>通过client压一个Tomcat节点+32个MySQL，QPS大概是430，Tomcat节点CPU跑满，MySQL  RT 是0.5ms，增加一个Tomcat节点，QPS大概是700，Tomcat CPU接近跑满，MySQL  RT 是0.6ms，到这里性能基本随着扩容线性增加，是符合预期的。</p>
<p>继续增加Tomcat节点来横向扩容性能，通过client压三个Tomcat节点+32个MySQL，QPS还是700，Tomcat节点CPU跑不满，MySQL  RT 是0.8ms，这就严重不符合预期了。</p>
<p>性能压测原则：</p>
<blockquote>
<p>加并发QPS不再上升说明到了某个瓶颈，哪个环节RT增加最多瓶颈就在哪里</p>
</blockquote>
<p><img src="/images/951413iMgBlog/image-20221026145848312.png" alt="image-20221026145848312"></p>
<p><strong>到这里一切都还是符合我们的经验的，看起来就是 MySQL 有瓶颈（RT 增加明显）。</strong></p>
<h2 id="排查-MySQL"><a href="#排查-MySQL" class="headerlink" title="排查 MySQL"></a>排查 MySQL</h2><p>现场DBA通过监控看到MySQL CPU不到20%，没有慢查询，并且尝试用client越过所有中间环节直接压其中一个MySQL，可以将 MySQL CPU 跑满，这时的QPS大概是38000（对应上面的场景client QPS为700的时候，单个MySQL上的QPS才跑到6000) 所以排除了MySQL的嫌疑(这个推理不够严谨为后面排查埋下了大坑)。</p>
<p>那么接下来的嫌疑在网络、LVS 等中间环节上。</p>
<h2 id="LVS和网络的嫌疑"><a href="#LVS和网络的嫌疑" class="headerlink" title="LVS和网络的嫌疑"></a>LVS和网络的嫌疑</h2><p>首先通过大查询排除了带宽的问题，因为这里都是小包，pps到了72万，很自然想到了网关、LVS的限流之类的</p>
<p>pps监控，这台物理机有4个MySQL实例上，pps 9万左右，9*32/4=72万<br><img src="/images/oss/b84245c17e213de528f2ad8090d504f6.png" alt="image.png"></p>
<p>…………（省略巨长的分析、拉人、扯皮过程）</p>
<p>最终所有网络因素都被排除，核心证据是：做压测的时候反复从 Tomcat 上 ping 后面的MySQL，RT 跟没有压力的时候一样，也说明了网络没有问题(请思考这个 ping 的作用)。</p>
<h2 id="问题的确认"><a href="#问题的确认" class="headerlink" title="问题的确认"></a>问题的确认</h2><p>尝试在Tomcat上打开日志，并将慢 SQL 阈值设置为100ms，这个时候确实能从日志中看到大量MySQL上的慢查询，因为这个SQL需要在Tomcat上做拆分成256个SQL，同时下发，一旦有一个SQL返回慢，整个请求就因为这个短板被拖累了。平均 RT  0.8ms，但是经常有超过100ms的话对整体影响还是很大的。</p>
<p>将Tomcat记录下来的慢查询（Tomcat增加了一个唯一id下发给MySQL）到MySQL日志中查找，果然发现MySQL上确实慢了，所以到这里基本确认是MySQL的问题，终于不用再纠结是否是网络问题了。</p>
<p>同时在Tomcat进行抓包，对网卡上的 RT 进行统计分析：</p>
<p><img src="/images/oss/ffd66d9a6098979b555dfb00d3494255.png" alt="image.png"></p>
<p>上是Tomcat上抓到的每个sql的物理RT 平均值，上面是QPS 430的时候， RT  0.6ms，下面是3个server，QPS为700，但是 RT 上升到了0.9ms，基本跟Tomcat监控记录到的物理RT一致。如果MySQL上也有类似抓包计算 RT 时间的话可以快速排除网络问题。</p>
<p>网络抓包得到的 RT 数据更容易被所有人接受。尝试过在MySQL上抓包，但是因为LVS模块的原因，进出端口、ip都被修改过，所以没法分析一个流的响应时间。</p>
<h2 id="重心再次转向MySQL"><a href="#重心再次转向MySQL" class="headerlink" title="重心再次转向MySQL"></a>重心再次转向MySQL</h2><p>这个时候因为问题点基本确认，再去查看MySQL是否有问题的重心都不一样了，不再只是看看CPU和慢查询，这个问题明显更复杂一些。</p>
<blockquote>
<p>教训：CPU只是影响性能的一个因素，RT 才是结果，要追着 RT 跑，而不是只看 CPU</p>
</blockquote>
<p>通过监控发现MySQL CPU虽然一直不高，但是经常看到running thread飙到100多，很快又降下去了，看起来像是突发性的并发查询请求太多导致了排队等待，每个MySQL实例是8Core的CPU，尝试将MySQL实例扩容到16Core（只是为了验证这个问题），QPS确实可以上升到1000（没有到达理想的1400）。</p>
<p>这是Tomcat上监控到的MySQL状态：<br><img src="/images/oss/e73c1371a02106a52f8a13f89a9dd9ad.png" alt="image.png"></p>
<p>同时在MySQL机器上通过vmstat也可以看到这种飙升：<br><img src="/images/oss/4dbd9dff9deacec0e9911e3a7d025578.png" alt="image.png"></p>
<p>以上分析可以清晰看到虽然 MySQL 整体压力不大，但是似乎会偶尔来一波卡顿、running 任务飙升。</p>
<p>像这种短暂突发性的并发流量似乎监控都很难看到（基本都被平均掉了），只有一些实时性监控偶尔会采集到这种短暂突发性飙升，这也导致了一开始忽视了MySQL。</p>
<p>所以接下来的核心问题就是MySQL为什么会有这种飙升、这种飙升的影响到底是什么？</p>
<h2 id="perf-top"><a href="#perf-top" class="headerlink" title="perf top"></a>perf top</h2><p>直接用 perf 看下 MySQLD 进程，发现 ut_delay 高得不符合逻辑：</p>
<p><img src="/images/oss/cd145c494c074e01e9d2d1d5583a87a0.png" alt="image.png"></p>
<p>展开看一下，基本是在优化器中做索引命中行数的选择：</p>
<p><img src="/images/oss/46d5f5ee5c58d7090a71164e645ccf79.png" alt="image.png" style="zoom: 67%;"></p>
<p>跟直接在 MySQL 命令行中通过 show processlist看到的基本一致：</p>
<p><img src="/images/oss/89cccebe41a8b8461ea75586b61b929f.png" alt="image.png" style="zoom:50%;"></p>
<p>这是 MySQL 的优化器在对索引进行统计，统计的时候要加锁，thread running 抖动的时候通过 show processlist 看到很多 thread处于 statistics 状态。也就是高并发下加锁影响了 CPU 压不上去同时 RT 剧烈增加。</p>
<p>这里ut_delay 消耗了 28% 的 CPU 肯定太不正常了，于是将 innodb_spin_wait_delay 从 30 改成 6 后性能立即上去了，继续增加 Tomcat 节点，QPS也可以线性增加。</p>
<blockquote>
<p>耗CPU最高的调用函数栈是…<code>mutex_spin_wait</code>-&gt;<code>ut_delay</code>，属于锁等待的逻辑。InnoDB在这里用的是自旋锁，锁等待是通过调用 ut_delay 让 CPU做空循环在等锁的时候不释放CPU从而避免上下文切换，会消耗比较高的CPU。</p>
</blockquote>
<h2 id="最终的性能"><a href="#最终的性能" class="headerlink" title="最终的性能"></a>最终的性能</h2><p>调整参数 innodb_spin_wait_delay=6 后在4个Tomcat节点下，并发40时，QPS跑到了1700，物理RT：0.7，逻辑RT：19.6，cpu：90%，这个时候只需要继续扩容 Tomcat 节点的数量就可以增加QPS<br><img src="/images/oss/48c976f989747266f9892403794996c0.png" alt="image.png"></p>
<p>再跟调整前比较一下，innodb_spin_wait_delay=30，并发40时，QPS 500+，物理RT：2.6ms 逻辑RT：72.1ms cpu：37%<br><img src="/images/oss/fdb459972926cff371f5f5ab703790bb.png" alt="image.png"></p>
<p>再看看调整前压测的时候的vmstat和tsar –cpu，可以看到process running抖动明显<br><img src="/images/oss/4dbd9dff9deacec0e9911e3a7d025578.png" alt="image.png"></p>
<p>对比修改delay后的process running就很稳定了，即使QPS大了3倍<br><img src="/images/oss/ed46d35161ea28352acd4289a3e9ddad.png" alt="image.png"></p>
<h2 id="事后思考和分析"><a href="#事后思考和分析" class="headerlink" title="事后思考和分析"></a>事后思考和分析</h2><p>到这里问题得到了完美解决，但是不禁要问为什么？ut_delay 是怎么工作的？ 和 innodb_spin_wait_delay 以及自旋锁的关系？</p>
<h2 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h2><p>既然调整 innodb_spin_wait_delay 就能解决这个问题，那就要先分析一下 innodb_spin_wait_delay 的作用</p>
<h3 id="关于-innodb-spin-wait-delay"><a href="#关于-innodb-spin-wait-delay" class="headerlink" title="关于 innodb_spin_wait_delay"></a>关于 innodb_spin_wait_delay</h3><p>innodb通过大量的自旋锁(比如 <code>InnoDB</code> <a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_mutex" target="_blank" rel="external">mutexes</a> and <a href="https://dev.mysql.com/doc/refman/5.7/en/glossary.html#glos_rw_lock" target="_blank" rel="external">rw-locks</a>)来用高CPU消耗避免上下文切换，这是自旋锁的正确使用方式，在多核场景下，它们一起自旋抢同一个锁，容易造成<a href="https://stackoverflow.com/questions/30684974/are-cache-line-ping-pong-and-false-sharing-the-same" target="_blank" rel="external">cache ping-pong</a>，进而多个CPU核之间会互相使对方缓存部分无效。所以这里<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-performance-spin_lock_polling.html" target="_blank" rel="external">innodb通过增加 innodb_spin_wait_delay 和 Pause 配合来缓解cache ping-pong</a>，也就是本来通过CPU 高速自旋抢锁，换成了抢锁失败后 delay一下（Pause）但是不释放CPU，delay 时间到后继续抢锁，也就是把连续的自旋抢锁转换成了更稀疏的点状的抢锁（间隔的 delay是个随机数），这样不但避免了上下文切换也大大减少了cache ping-pong。</p>
<h3 id="自旋锁如何减少了cache-ping-pong"><a href="#自旋锁如何减少了cache-ping-pong" class="headerlink" title="自旋锁如何减少了cache ping-pong"></a>自旋锁如何减少了cache ping-pong</h3><p>多线程竞争锁的时候，加锁失败的线程会“忙等待”，直到它拿到锁。什么叫“忙等待”呢？它并不意味着一直执行 CAS 函数，而是会与 CPU 紧密配合 ，它通过 CPU 提供的 <code>PAUSE</code> 指令，减少循环等待时的cache ping-pong和耗电量；对于单核 CPU，忙等待并没有意义，此时它会主动把线程休眠。</p>
<h3 id="X86-PAUSE-指令"><a href="#X86-PAUSE-指令" class="headerlink" title="X86 PAUSE 指令"></a>X86 PAUSE 指令</h3><p>X86设计了Pause指令，也就是调用 Pause 指令的代码会抢着 CPU 不释放，但是CPU 会打个盹，比如 10个时钟周期，相对一次上下文切换是大几千个时钟周期。</p>
<p>这样应用一旦自旋抢锁失败可以先 Pause 一下，只是这个Pause 时间对于 MySQL 来说还不够久，所以需要增加参数 innodb_spin_wait_delay 来将休息时间放大一些。</p>
<p>在我们的这个场景下对每个 SQL的 RT 抖动非常敏感（放大256倍），所以过高的 delay 会导致部分SQL  RT  变高。</p>
<p>函数 ut_delay(ut_rnd_interval(0, srv_spin_wait_delay)) 用来执行这个delay：</p>
<pre><code>/***************************MySQL代码****************************/
/**Runs an idle loop on CPU. The argument gives the desired delay
in microseconds on 100 MHz Pentium + Visual C++.
@return dummy value */
UNIV_INTERN
ulint
ut_delay(ulint delay)  //delay 是[0,innodb_spin_wait_delay)之间的一个随机数
{
        ulint   i, j;
        UT_LOW_PRIORITY_CPU();
        j = 0;

        for (i = 0; i &lt; delay * 50; i++) {  //delay 放大50倍
                j += i;
                UT_RELAX_CPU();             //调用 CPU Pause
        }

        UT_RESUME_PRIORITY_CPU();
        return(j);
}
</code></pre><p>innodb_spin_wait_delay的默认值为6. spin 等待延迟是一个动态全局参数，可以在MySQL选项文件（my.cnf或my.ini）中指定该参数，或者在运行时使用SET GLOBAL 来修改。在我们的MySQL配置中默认改成了30，导致了这个问题。</p>
<h3 id="CPU-为什么要有Pause"><a href="#CPU-为什么要有Pause" class="headerlink" title="CPU 为什么要有Pause"></a>CPU 为什么要有Pause</h3><p>首先可以看到 Pause 指令的作用：</p>
<ul>
<li>避免上下文切换，应用层想要休息可能会用yield、sleep，这两操作对于CPU来说太重了(伴随上下文切换)</li>
<li>能给超线程腾出计算能力（HT共享核，但是有单独的寄存器等存储单元，CPU Pause的时候，对应的HT可以占用计算资源），比如同一个core上先跑多个Pause，同时再跑 nop 指令，这时 nop指令的 IPC基本不受Pause的影响</li>
<li>节能（CPU可以休息、但是不让出来），CPU Pause 的时候你从 top 能看到 CPU 100%，但是不耗能。</li>
</ul>
<p>所以有了 Pause 指令后能够提高超线程的利用率,节能，减少上下文切换提高自旋锁的效率。</p>
<blockquote>
<p><a href="https://www.reddit.com/r/intel/comments/hogk2n/research_on_the_impact_of_intel_Pause_instruction/" target="_blank" rel="external">The PAUSE instruction is first introduced</a> for Intel Pentium 4 processor to improve the performance of “spin-wait loop”. The PAUSE instruction is typically used with software threads executing on two logical processors located in the same processor core, waiting for a lock to be released. Such short wait loops tend to last between tens and a few hundreds of cycles. When the wait loop is expected to last for thousands of cycles or more, it is preferable to yield to the operating system by calling one of the OS synchronization API functions, such as WaitForSingleObject on Windows OS.</p>
<p>An Intel® processor suffers a severe performance penalty when exiting the loop because it detects a possible memory order violation. The PAUSE instruction provides a hint to the processor that the code sequence is a spin-wait loop. The processor uses this hint to avoid the memory order violation in most situations. The PAUSE instruction can improve the performance of the processors supporting Intel Hyper-Threading Technology when executing “spin-wait loops”. With Pause instruction, processors are able to avoid the memory order violation and pipeline flush, and reduce power consumption through pipeline stall.</p>
</blockquote>
<p><strong>从intel sdm手册以及实际测试验证来看，Pause 指令在执行过程中，基本不占用流水线执行资源。</strong></p>
<h3 id="Skylake-架构的8163-和-Broadwell架构-E5-2682-CPU型号的不同"><a href="#Skylake-架构的8163-和-Broadwell架构-E5-2682-CPU型号的不同" class="headerlink" title="Skylake 架构的8163 和 Broadwell架构 E5-2682 CPU型号的不同"></a>Skylake 架构的8163 和 Broadwell架构 E5-2682 CPU型号的不同</h3><p>为什么用得好好的 innodb_spin_wait_delay 参数这次就不行了呢？</p>
<p>这是因为以前业务一直使用的是 E5-2682 CPU，这次用的是新一代架构的 Skylake 8163，那这两款CPU在这里的核心差别是？</p>
<p>在Intel 64-ia-32-architectures-optimization-manual手册中提到：</p>
<blockquote>
<p>The latency of the PAUSE instruction in prior generation microarchitectures is about 10 cycles, whereas in Skylake microarchitecture it has been extended to as many as 140 cycles.</p>
<p><a href="https://xem.github.io/minix86/manual/intel-x86-and-64-manual-vol3/o_fe12b1e2a880e0ce-302.html" target="_blank" rel="external">The PAUSE instruction can improves the performance</a> of processors supporting Intel Hyper-Threading Technology when executing “spin-wait loops” and other routines where one thread is accessing a shared lock or semaphore in a tight polling loop. When executing a spin-wait loop, the processor can suffer a severe performance penalty when exiting the loop because it detects a possible memory order violation and flushes the core processor’s pipeline. The PAUSE instruction provides a hint to the processor that the code sequence is a spin-wait loop. The processor uses this hint to avoid the memory order violation and prevent the pipeline flush. In addition, the PAUSE instruction de-<br>pipelines the spin-wait loop to prevent it from consuming execution resources excessively and consume power needlessly. (See<a href="https://xem.github.io/minix86/manual/intel-x86-and-64-manual-vol3/o_fe12b1e2a880e0ce-305.html" target="_blank" rel="external"> Section 8.10.6.1, “Use the PAUSE Instruction in Spin-Wait Loops,” for more </a>information about using the PAUSE instruction with IA-32 processors supporting Intel Hyper-Threading Technology.)</p>
</blockquote>
<p>也就是<strong>Skylake架构的CPU的PAUSE指令从之前的10 cycles 改成了 140 cycles。</strong>这可是14倍的变化呀。</p>
<p>MySQL 使用 innodb_spin_wait_delay 控制 spin lock等待时间，等待时间时间从0*50个Pause到innodb_spin_wait_delay*50个Pause。<br>以前 innodb_spin_wait_delay 默认配置30，对于E5-2682 CPU，等待的最长时间为：<br>30 * 50 * 10=15000 cycles，对于2.5GHz的CPU，等待时间为6us。<br>对应计算 Skylake CPU的等待时间：30 *50 *140=210000 cycles，CPU主频也是2.5GHz，等待时间84us。</p>
<p>E5-2682 CPU型号在不同的delay参数和不同并发压力下的写入性能数据：</p>
<p><img src="/images/951413iMgBlog/image-20221026153750159.png" alt="image-20221026153750159"></p>
<p>Skylake 8163 CPU型号在不同的delay参数和不同并发压力下的写入性能数据：</p>
<p><img src="/images/951413iMgBlog/image-20221026153813774.png" alt="image-20221026153813774"></p>
<p>==因为8163的cycles从10改到了140，所以可以看到delay参数对性能的影响更加陡峻。==</p>
<h2 id="总结分析"><a href="#总结分析" class="headerlink" title="总结分析"></a>总结分析</h2><p>Intel CPU 架构不同使得 Pause 指令的CPU Cycles不同导致了 MySQL innodb_spin_wait_delay 在 spin lock 失败的时候（此时需要 Pause<em> innodb_spin_wait_delay</em>N）delay更久，使得调用方看到了MySQL更大的 RT ，进而导致 Tomcat Server上业务并发跑不起来，所以最终压力上不去。</p>
<p>在长链路的排查中，细化定位是哪个节点出了问题是最难的，要盯住 RT 而不是 CPU。</p>
<p>欲速则不达，做压测的时候还是要老老实实地从一个并发开始观察QPS、 RT ，然后一直增加压力到压不上去了，再看QPS、 RT 变化，然后确认瓶颈点。</p>
<h2 id="我想追加几个问题帮大家理解"><a href="#我想追加几个问题帮大家理解" class="headerlink" title="我想追加几个问题帮大家理解"></a>我想追加几个问题帮大家理解</h2><h3 id="为什么要有自旋锁-内核里面的spinlock"><a href="#为什么要有自旋锁-内核里面的spinlock" class="headerlink" title="为什么要有自旋锁(内核里面的spinlock)?"></a>为什么要有自旋锁(内核里面的spinlock)?</h3><p>等锁的时候可以释放CPU进入等待，这叫悲观锁，代价是释放CPU必然导致上下文切换，一次上下文切换至少需要几千个时钟周期，也就是CPU需要几千个时钟周期来完成上下文切换的工作(几千个时钟周期没有产出–真浪费)</p>
<p>或者等锁的时候不释放CPU，赌很快能等到锁，这叫乐观锁，Linux OS用的是spinlock（类似大家看到的CAS），也就是CPU不释放一直不停地检查能否拿到锁，一个时钟周期检查一次太快了(想想你去银行柜台办业务，每秒钟问一次柜员轮到你了没有！)，所以CPU工程师就在想能不能提供一条指令一直占着CPU很久，这条指令就是pause，每一个spinlock就会调用pause休息几十、几百个时钟周期后再去看看能否抢到所(柜台给你提供了沙发茶水，你坐一会再去问柜员)</p>
<h3 id="执行Pause指令的时候CPU真的休息了吗？"><a href="#执行Pause指令的时候CPU真的休息了吗？" class="headerlink" title="执行Pause指令的时候CPU真的休息了吗？"></a>执行Pause指令的时候CPU真的休息了吗？</h3><p>如果没有事情做的话，CPU是会停下来(省电)，如果开了超线程，如果一个核执行了Pause，那么对这个核的另一个超线程来说，白捡了100%的CPU，别人休息（Pause、stall）的时候正好给我用(这是超线程的本质)！</p>
<p>这也是为什么有些场景2个超线程能发挥2倍能力，有些场景2个超线程只能跑出1倍能力。</p>
<p>相对比上下文切换浪费的几千个时钟周期，Pause(spinlock)真是一点都没浪费。但如果你一直spinlock 几万、几十万个时钟周期都没等到锁还不释放也不对，这会导致其他线程调度不到CPU而饿死。一般自旋一段时间后都会放弃CPU转为上下文切换，所以MySQL 加了参数 innodb_spin_wait_delay 来控制spinlock的长短。</p>
<h3 id="并发高导致自旋锁效率低？"><a href="#并发高导致自旋锁效率低？" class="headerlink" title="并发高导致自旋锁效率低？"></a>并发高导致自旋锁效率低？</h3><p>如果并发高，都抢同一个锁，这里的效率会随着并发的增加而降低，不展开了，记住这个结论，类似太多人在柜台问轮到自己没有，留给柜员办业务的时间反而少了！</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://cloud.tencent.com/developer/article/1005284" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1005284</a></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-performance-spin_lock_polling.html" target="_blank" rel="external">mysql doc</a></p>
<p><a href="http://oliveryang.net/2018/01/cache-false-sharing-debug" target="_blank" rel="external">Cache Line 伪共享发现与优化</a></p>
<p><a href="https://en.wikichip.org/w/images/e/eb/intel-ref-248966-037.pdf" target="_blank" rel="external">intel spec</a></p>
<p><a href="https://mp.weixin.qq.com/s/dlKC13i9Z8wjDDiU2tig6Q" target="_blank" rel="external">Intel PAUSE指令变化影响到MySQL的性能，该如何解决？</a></p>
<p><a href="https://topic.atatech.org/articles/173194" target="_blank" rel="external">ARM软硬件协同设计：锁优化</a>, arm不同于x86，用的是yield来代替Pause</p>
<p><a href="http://cr.openjdk.java.net/~dchuyko/8186670/yield/spinwait.html" target="_blank" rel="external">http://cr.openjdk.java.net/~dchuyko/8186670/yield/spinwait.html</a></p>
<p><a href="https://aloiskraus.wordpress.com/2018/06/16/why-skylakex-cpus-are-sometimes-50-slower-how-intel-has-broken-existing-code/" target="_blank" rel="external">https://aloiskraus.wordpress.com/2018/06/16/why-skylakex-cpus-are-sometimes-50-slower-how-intel-has-broken-existing-code/</a> Windows+.NET 平台下的分析过程及修复方案</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/12/09/epoll的LT和ET/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/09/epoll的LT和ET/" itemprop="url">epoll的LT和ET</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-09T12:30:03+08:00">
                2019-12-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="epoll的LT和ET"><a href="#epoll的LT和ET" class="headerlink" title="epoll的LT和ET"></a>epoll的LT和ET</h1><ul>
<li><p>LT水平触发(翻译为 条件触发 更合适） </p>
<blockquote>
<p>如果事件来了，不管来了几个，只要仍然有未处理的事件，epoll都会通知你。比如来了，打印一行通知，但是不去处理事件，那么会不停滴打印通知。水平触发模式的 epoll 的扩展性很差。</p>
</blockquote>
</li>
<li><p>ET边沿触发<br>&gt;</p>
<blockquote>
<p>  如果事件来了，不管来了几个，你若不处理或者没有处理完，除非下一个事件到来，否则epoll将不会再通知你。 比如事件来了，打印一行通知，但是不去处理事件，那么不再通知，除非下个事件来了</p>
</blockquote>
</li>
</ul>
<p>LT比ET会多一次重新加入就绪队列的动作，也就是意味着一定有一次poll不到东西，效率是有影响但是队列长度有限所以基本可以不用考虑。但是LT编程方式上要简单多了，所以LT也是默认的。</p>
<p>举个 🌰：你有急事打电话找人，如果对方一直不接，那你只有一直打，直到他接电话为止，这就是 lt 模式；如果不急，电话打过去对方不接，那就等有空再打，这就是 et 模式。</p>
<h3 id="条件触发的问题：不必要的唤醒"><a href="#条件触发的问题：不必要的唤醒" class="headerlink" title="条件触发的问题：不必要的唤醒"></a>条件触发的问题：不必要的唤醒</h3><ol>
<li>内核：收到一个新建连接的请求</li>
<li>内核：由于 “惊群效应” ，唤醒两个正在 epoll_wait() 的线程 A 和线程 B</li>
<li>线程A：epoll_wait() 返回</li>
<li>线程B：epoll_wait() 返回</li>
<li>线程A：执行 accept() 并且成功</li>
<li>线程B：执行 accept() 失败，accept() 返回 EAGAIN</li>
</ol>
<h3 id="边缘触发的问题：不必要的唤醒以及饥饿"><a href="#边缘触发的问题：不必要的唤醒以及饥饿" class="headerlink" title="边缘触发的问题：不必要的唤醒以及饥饿"></a>边缘触发的问题：不必要的唤醒以及饥饿</h3><p>不必要的唤醒：</p>
<ol>
<li>内核：收到第一个连接请求。线程 A 和 线程 B 两个线程都在 epoll_wait() 上等待。由于采用边缘触发模式，所以只有一个线程会收到通知。这里假定线程 A 收到通知</li>
<li>线程A：epoll_wait() 返回</li>
<li>线程A：调用 accpet() 并且成功</li>
<li>内核：此时 accept queue 为空，所以将边缘触发的 socket 的状态从可读置成不可读</li>
<li>内核：收到第二个建连请求</li>
<li>内核：此时，由于线程 A 还在执行 accept() 处理，只剩下线程 B 在等待 epoll_wait()，于是唤醒线程 B</li>
<li>线程A：继续执行 accept() 直到返回 EAGAIN</li>
<li>线程B：执行 accept()，并返回 EAGAIN，此时线程 B 可能有点困惑(“明明通知我有事件，结果却返回 EAGAIN”)</li>
<li>线程A：再次执行 accept()，这次终于返回 EAGAIN</li>
</ol>
<p>饥饿：</p>
<ol>
<li>内核：接收到两个建连请求。线程 A 和 线程 B 两个线程都在等在 epoll_wait()。由于采用边缘触发模式，只有一个线程会被唤醒，我们这里假定线程 A 先被唤醒</li>
<li>线程A：epoll_wait() 返回</li>
<li>线程A：调用 accpet() 并且成功</li>
<li>内核：收到第三个建连请求。由于线程 A 还没有处理完(没有返回 EAGAIN)，当前 socket 还处于可读的状态，由于是边缘触发模式，所有不会产生新的事件</li>
<li>线程A：继续执行 accept() 希望返回 EAGAIN 再进入 epoll_wait() 等待，然而它又 accept() 成功并处理了一个新连接</li>
<li>内核：又收到了第四个建连请求</li>
<li>线程A：又继续执行 accept()，结果又返回成功</li>
</ol>
<p>ET的话会要求应用一直要把消息处理完毕，比如nginx用ET模式，来了一个上传大文件并压缩的任务，会造成这么一个循环：</p>
<blockquote>
<p>nginx读数据（未读完）-&gt;Gzip(需要时间，套接字又有数据过来)-&gt;读数据（未读完）-&gt;Gzip …..</p>
</blockquote>
<p>新的accpt进来，因为前一个nginx worker已经被唤醒并且还在read(这个时候内核因为accept queue为空所以已经将socket设置成不可读），所以即使其它worker 被唤醒，看到的也是一个不可读的socket，所以很快因为EAGAIN返回了。</p>
<p>这样就会造成nginx的这个worker假死了一样。如果上传速度慢，这个循环无法持续存在，也就是一旦读完nginx切走（再有数据进来等待下次触发），不会造成假死。</p>
<p>JDK中的NIO是条件触发（level-triggered），不支持ET。netty，nginx和redis默认是边缘触发（edge-triggered），netty因为JDK不支持ET，所以自己实现了Netty-native的抽象，不依赖JDK来提供ET。</p>
<p>边缘触发会比条件触发更高效一些，因为边缘触发不会让同一个文件描述符多次被处理,比如有些文件描述符已经不需要再读写了,但是在条件触发下每次都会返回,而边缘触发只会返回一次。</p>
<p>如果设置边缘触发,则必须将对应的文件描述符设置为非阻塞模式并且循环读取数据。否则会导致程序的效率大大下降或丢消息。</p>
<p>poll和epoll默认采用的都是条件触发,只是epoll可以修改成边缘触发。条件触发同时支持block和non-block，使用更简单一些。</p>
<h3 id="epoll-LT惊群的发生"><a href="#epoll-LT惊群的发生" class="headerlink" title="epoll LT惊群的发生"></a>epoll LT惊群的发生</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 否则会阻塞在IO系统调用，导致没有机会再epoll</span></div><div class="line">set_socket_nonblocking(sd);</div><div class="line">epfd = epoll_create(<span class="number">64</span>);</div><div class="line">event.data.fd = sd;</div><div class="line">epoll_ctl(epfd, EPOLL_CTL_ADD, sd, &amp;event);</div><div class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</div><div class="line">    epoll_wait(epfd, events, <span class="number">64</span>, xx);</div><div class="line">    ... <span class="comment">// 危险区域！如果有共享同一个epfd的进程/线程调用epoll_wait，它们也将会被唤醒！</span></div><div class="line">    <span class="comment">// 这个accept将会有多个进程/线程调用，如果并发请求数很少，那么将仅有几个进程会成功：</span></div><div class="line">    <span class="comment">// 1. 假设accept队列中有n个请求，则仅有n个进程能成功，其它将全部返回EAGAIN (Resource temporarily unavailable)</span></div><div class="line">    <span class="comment">// 2. 如果n很大(即增加请求负载)，虽然返回EAGAIN的比率会降低，但这些进程也并不一定取到了epoll_wait返回当下的那个预期的请求。</span></div><div class="line">    csd = accept(sd, &amp;in_addr, &amp;in_len); </div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>再看一遍LT的描述“如果事件来了，不管来了几个，只要仍然有未处理的事件，epoll都会通知你。”，显然，epoll_wait刚刚取到事件的时候的时候，不可能马上就调用accept去处理，事实上，逻辑在epoll_wait函数调用的ep_poll中还没返回的，这个时候，显然符合“仍然有未处理的事件”这个条件，显然这个时候为了实现这个语义，需要做的就是通知别的同样阻塞在同一个epoll句柄睡眠队列上的进程！在实现上，这个语义由两点来保证：</p>
<p>保证1：在LT模式下，“就绪链表”上取出的epi上报完事件后会重新加回“就绪链表”；<br>保证2：如果“就绪链表”不为空，且此时有进程阻塞在同一个epoll句柄的睡眠队列上，则唤醒它。</p>
<p>epoll LT模式下有进程被不必要唤醒，这一点并不是内核无意而为之的，内核肯定是知道这件事的，这个并不像之前accept惊群那样算是内核的一个缺陷。epoll LT模式只是提供了一种模式，误用这种模式将会造成类似惊群那样的效应。但是不管怎么说，为了讨论上的方便，后面我们姑且将这种效应称作epoll LT惊群吧。</p>
<h3 id="epoll-ET模式可以解决上面的问题，但是带来了新的麻烦"><a href="#epoll-ET模式可以解决上面的问题，但是带来了新的麻烦" class="headerlink" title="epoll ET模式可以解决上面的问题，但是带来了新的麻烦"></a>epoll ET模式可以解决上面的问题，但是带来了新的麻烦</h3><p>由于epi entry的callback即ep_poll_callback所做的事情仅仅是将该epi自身加入到epoll句柄的“就绪链表”，同时唤醒在epoll句柄睡眠队列上的task，所以这里并不对事件的细节进行计数，比如说，<strong>如果ep_poll_callback在将一个epi加入“就绪链表”之前发现它已经在“就绪链表”了，那么就不会再次添加，因此可以说，一个epi可能pending了多个事件，注意到这点非常重要！</strong></p>
<p>一个epi上pending多个事件，这个在LT模式下没有任何问题，因为获取事件的epi总是会被重新添加回“就绪链表”，那么如果还有事件，在下次check的时候总会取到。然而对于ET模式，仅仅将epi从“就绪链表”删除并将事件本身上报后就返回了，因此如果该epi里还有事件，则只能等待再次发生事件，进而调用ep_poll_callback时将该epi加入“就绪队列”。这意味着什么？</p>
<p>这意味着，应用程序，即epoll_wait的调用进程必须自己在获取事件后将其处理干净后方可再次调用epoll_wait，否则epoll_wait不会返回，而是必须等到下次产生事件的时候方可返回。这会导致事件堆积，所以一般会死循环一直拉取事件，直到拉取不到了再返回。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/157349" target="_blank" rel="external">Epoll is fundamentally broken</a> </p>
<p><a href="https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12" target="_blank" rel="external">Epoll is fundamentally broken 1/2</a> </p>
<p><a href="https://wenfh2020.com/2021/11/21/question-nginx-epoll-et/" target="_blank" rel="external">https://wenfh2020.com/2021/11/21/question-nginx-epoll-et/</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/12/09/如何在工作中学习-2019V2版/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/09/如何在工作中学习-2019V2版/" itemprop="url">如何在工作中学习-2019V2版</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-09T12:30:03+08:00">
                2019-12-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何在工作中学习-2019V2版"><a href="#如何在工作中学习-2019V2版" class="headerlink" title="如何在工作中学习-2019V2版"></a>如何在工作中学习-2019V2版</h1><p><a href="/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/">2018版简单一些，可以看这里</a>，2019版加入了一些新的理解和案例，目的是想要让文章中所说的不是空洞的口号，而是可以具体执行的命令，拉平对读者的要求。</p>
<blockquote>
<p>先说一件值得思考的事情：高考的时候大家都是一样的教科书，同一个教室，同样的老师辅导，时间精力基本差不多，可是最后别人考的是清华北大或者一本，而你的实力只能考个三本，为什么？ 当然这里主要是智商的影响，那么其他因素呢？智商解决的问题能不能后天用其他方式来补位一下？</p>
</blockquote>
<p>学习的闭环：先了解知识，再实战演练，然后总结复盘。很多时候只是停留在知识学习层面，没有实践，或者实践后没有思考复盘优化，都导致无法深入的理解掌握知识点(from: 瑾妤)</p>
<h2 id="关键问题点"><a href="#关键问题点" class="headerlink" title="关键问题点"></a>关键问题点</h2><h3 id="为什么你的知识积累不了？"><a href="#为什么你的知识积累不了？" class="headerlink" title="为什么你的知识积累不了？"></a>为什么你的知识积累不了？</h3><p>有些知识看过就忘、忘了再看，实际碰到问题还是联系不上这个知识，这其实是知识的积累出了问题，没有深入的理解自然就不能灵活运用，也就谈不上解决问题了。这跟大家一起看相同的高考教科书但是高考结果不一样是一个原因。问题出在了理解上，每个人的理解能力不一样（智商），绝大多数人对知识的理解要靠不断地实践（做题）来巩固。</p>
<h3 id="同样实践效果不一样？"><a href="#同样实践效果不一样？" class="headerlink" title="同样实践效果不一样？"></a>同样实践效果不一样？</h3><p>同样工作一年碰到了10个问题（或者说做了10套高考模拟试卷），但是结果不一样，那是因为在实践过程中方法不够好。或者说你对你为什么做对了、为什么做错了没有去复盘</p>
<p>假如碰到一个问题，身边的同事解决了，而我解决不了。那么我就去想这个问题他是怎么解决的，他看到这个问题后的逻辑和思考是怎么样的，有哪些知识指导了他这么逻辑推理，这些知识哪些我也知道但是我没有想到这么去运用推理（说明我对这个知识理解的不到位导致灵活运用缺乏）；这些知识中又有哪些是我不知道的（知识缺乏，没什么好说的快去Google什么学习下–有场景案例和目的加持，学习理解起来更快）。</p>
<p>等你把这个问题基本按照你同事掌握的知识和逻辑推理想明白后，需要再去琢磨一下他的逻辑推理解题思路中有没有不对的，有没有啰嗦的地方，有没有更直接的方式（对知识更好地运用）。</p>
<p>我相信每个问题都这么去实践的话就不应该再抱怨灵活运用、举一反三，同时知识也积累下来了，这种场景下积累到的知识是不会那么容易忘记的。</p>
<p>这就是向身边的牛人学习，同时很快超过他的办法。这就是为什么高考前你做了10套模拟题还不如其他人做一套的效果好</p>
<p><strong>知识+逻辑 基本等于你的能力</strong>，知识让你知道那个东西，逻辑让你把东西和问题联系起来</p>
<p><strong>这里的问题你可以理解成方案、架构、设计等</strong></p>
<h3 id="系统化的知识哪里来？"><a href="#系统化的知识哪里来？" class="headerlink" title="系统化的知识哪里来？"></a>系统化的知识哪里来？</h3><p>知识之间是可以联系起来的并且像一颗大树一样自我生长，但是当你都没理解透彻，自然没法产生联系，也就不能够自我生长了。</p>
<p>真正掌握好的知识点会慢慢生长连接最终组成一张大网</p>
<p>但是我们最容易陷入的就是掌握的深度、系统化（工作中碎片时间过多，学校里缺少时间）不够，所以一个知识点每次碰到花半个小时学习下来觉得掌握了，但是3个月后就又没印象了。总是感觉自己在懵懵懂懂中，或者一个领域学起来总是不得要领，根本的原因还是在于：宏观整体大图了解不够（缺乏体系，每次都是盲人摸象）；关键知识点深度不够，理解不透彻，这些关键点就是这个领域的骨架、支点、抓手。缺了抓手自然不能生长，缺了宏观大图容易误入歧途。</p>
<p>我们有时候发现自己在某个领域学起来特别快，但是换个领域就总是不得要领，问题出在了上面，即使花再多时间也是徒然。这也就是为什么学霸看两个小时的课本比你看两天效果还好，感受下来还觉得别人好聪明，是不是智商比我高啊。</p>
<p>所以新进入一个领域的时候要去找他的大图和抓手。</p>
<p>好的同事总是能很轻易地把这个大图交给你，再顺便给你几个抓手，你就基本入门了，这就是培训的魅力，这种情况肯定比自学效率高多了。但是目前绝大部分的培训都做不到这点</p>
<h3 id="好的逻辑又怎么来？"><a href="#好的逻辑又怎么来？" class="headerlink" title="好的逻辑又怎么来？"></a>好的逻辑又怎么来？</h3><p>实践、复盘</p>
<h2 id="讲个前同事的故事"><a href="#讲个前同事的故事" class="headerlink" title="讲个前同事的故事"></a>讲个前同事的故事</h2><p>有一个前同事是5Q过来的，负责技术（所有解决不了的问题都找他），这位同学从chinaren出道，跟着王兴一块创业5Q，5Q在学校靠鸡腿打下大片市场，最后被陈一舟的校内收购（据说被收购后5Q的好多技术都走了，最后王兴硬是呆在校内网把合约上的所有钱都拿到了）。这位同学让我最佩服的解决问题的能力，好多问题其实他也不一定就擅长，但是他就是有本事通过Help、Google不停地验证尝试就把一个不熟悉的问题给解决了，这是我最羡慕的能力，在后面的职业生涯中一直不停地往这个方面尝试。</p>
<h3 id="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"><a href="#应用刚启动连接到数据库的时候比较慢，但又不是慢查询" class="headerlink" title="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"></a>应用刚启动连接到数据库的时候比较慢，但又不是慢查询</h3><ol>
<li>这位同学的解决办法是通过tcpdump来分析网络包，看网络包的时间戳和网络包的内容，然后找到了具体卡在了哪里。</li>
<li>如果是专业的DBA可能会通过show processlist 看具体连接在做什么，比如看到这些连接状态是 <strong>authentication</strong> 状态，然后再通过Google或者对这个状态的理解知道创建连接的时候MySQL需要反查IP、域名这里比较耗时，通过配置参数 <strong>skip-name-resolve</strong> 跳过去就好了。</li>
<li><p>如果是MySQL的老司机，一上来就知道连接慢的话跟 <strong>skip-name-resolve</strong> 关系最大。</p>
<p> 在我眼里这三种方式都解决了问题，最后一种最快但是纯靠积累和经验，换个问题也许就不灵了；第一种方式是最牛逼和通用的，只需要最少的知识就把问题解决了。</p>
</li>
</ol>
<p>我当时跟着他从sudo、ls等linux命令开始学起。当然我不会轻易去打搅他问他，每次碰到问题我尽量让他在我的电脑上来操作，解决后我再自己复盘，通过history调出他的所有操作记录，看他在我的电脑上用Google搜啥了，然后一个个去学习分析他每个动作，去想他为什么搜这个关键字，复盘完还有不懂的再到他面前跟他面对面的讨论他为什么要这么做，指导他这么做的知识和逻辑又是什么。</p>
<h2 id="如何向身边的同学学习"><a href="#如何向身边的同学学习" class="headerlink" title="如何向身边的同学学习"></a>如何向身边的同学学习</h2><h3 id="微信提问的技巧"><a href="#微信提问的技巧" class="headerlink" title="微信提问的技巧"></a>微信提问的技巧</h3><p>我进现在的公司的时候是个网络小白，但是业务需要我去解决这些问题，于是我就经常在微信上找内部的专家来帮请教一些问题，首先要感谢他们的耐心，同时我觉得跟他们提问的时候的方法大家可以参考一下。</p>
<p>首先，没有客套直奔主题把问题描述清楚，微信消息本来就不是即时的，就不要问在不在、能不能问个问题、你好（因为这些问题会浪费他一次切换，真要客套把 你好 写在问题前面在一条消息中发出去）。</p>
<p>其次，我会截图把现象接下来，关键部分红框标明。如果是内部机器还会帮对方申请登陆账号，打通ssh登陆，然后把ssh登陆命令和触发截图现象命令的文字一起微信发过去。也就是对方收到我的消息，看到截图的问题后，他只要复制粘贴我发给他的文字信息就看到现象了。为什么要帮他申请账号，有时候账号要审批，要找人，对方不知道到哪里申请等等；这么复杂对方干脆就装作没看见你的消息好了。</p>
<p>为什么还要把ssh登陆命令、重现文字命令发给他呢，怕他敲错啊，敲错了还得来问你，一来一回时间都浪费了。你也许会说我截图上有重现命令啊，那么凭什么他帮你解决问题他还要瞪大眼睛看你的截图把你的命令抄下来？比如容器ID一长串，你是截图了，结果他把b抄成6了，重现不了，还得问你，又是几个来回……</p>
<p>提完问题后有三种情况：抱歉，我也不知道；这个问题你要问问谁，他应该知道；沉默</p>
<p>没关系微信的优势是复制粘贴方便，你就换个人再问，可能问到第三个人终于搞定了。那么我会回来把结果告诉前面我问过的同学，即使他是沉默的那个。因为我骚扰过人家，要回来填这个坑，另外也许他真的不知道，那么同步给他也可以帮到他。结果就是他觉得我很靠谱，信任度就建立好了，下次再有问题会更卖力地一起来解决。</p>
<h4 id="一些不好的"><a href="#一些不好的" class="headerlink" title="一些不好的"></a>一些不好的</h4><p>有个同学看了我的文章（晚上11点看的），马上发了微信消息过来问文章中用到的工具是什么。我还没睡觉但是躺床上看东西，有微信消息提醒，但没有切过去回复（不想中断我在看的东西）。5分钟后这个同学居然钉了我一下，我当时是很震惊的，这是你平时学习，不是我的产品出了故障，现在晚上11点。</p>
<p>提问题的时间要考虑对方大概率在电脑前，打字快。否则要紧的话就提选择题类型的问题</p>
<p>问题要尽量是封闭的，比如微信上不适合问的问题：</p>
<ul>
<li>为什么我们应用的TPS压不上去，即使CPU还有很多空闲（不好的原因：太开放，原因太多，对方要打字2000才能给你解释清楚各种可能的原因，你要不是他老板就不要这样问了）</li>
<li>用多条消息来描述一个问题，一次没把问题描述清楚，需要对方中断多次</li>
</ul>
<h2 id="场景式学习、体感的来源、面对问题学习"><a href="#场景式学习、体感的来源、面对问题学习" class="headerlink" title="场景式学习、体感的来源、面对问题学习"></a>场景式学习、体感的来源、面对问题学习</h2><p>前面提到的对知识的深入理解这有点空，如何才能做到深入理解？</p>
<h3 id="举个学习TCP三次握手例子"><a href="#举个学习TCP三次握手例子" class="headerlink" title="举个学习TCP三次握手例子"></a>举个学习TCP三次握手例子</h3><p>经历稍微丰富点的工程师都觉得TCP三次握手看过很多次、很多篇文章了，但是文章写得再好似乎当时理解了，但是总是过几个月就忘了或者一看就懂，过一阵子被人一问就模模糊糊了，或者两个为什么就答不上了，自己都觉得自己的回答是在猜或者不确定</p>
<p>为什么会这样呢？而学其它知识就好通畅多了，我觉得这里最主要的是我们对TCP缺乏体感，比如没有几个工程师去看过TCP握手的代码，也没法想象真正的TCP握手是如何在电脑里运作的（打电话能给你一些类似的体感，但是细节覆盖面不够）。</p>
<p>如果这个时候你一边学习的时候一边再用wireshark抓包看看三次握手具体在干什么，比抽象的描述实在多了，你能看到具体握手的一来一回，并且看到一来一回带了哪些内容，这些内容又是用来做什么、为什么要带，这个时候你再去看别人讲解的理论顿时会觉得好理解多了，以后也很难忘记。</p>
<p>但是这里很多人执行能力不强，想去抓包，但是觉得要下载安装wireshark，要学习wireshark就放弃了。只看不动手当然是最舒适的，但是这个最舒适给了你在学习的假象，没有结果。</p>
<p>这是不是跟你要解决一个难题非常像，这个难题需要你去做很多事，比如下载源代码（翻不了墙，放弃）；比如要编译（还要去学习那些编译参数，放弃）；比如要搭建环境（太琐屑，放弃）。你看这中间九九八十一难你放弃了一难都取不了真经。这也是为什么同样学习、同样的问题，他能学会，他能解决，你不可以。</p>
<h3 id="再来看一个解决问题的例子"><a href="#再来看一个解决问题的例子" class="headerlink" title="再来看一个解决问题的例子"></a>再来看一个解决问题的例子</h3><p><a href="https://www.atatech.org/articles/73174" target="_blank" rel="external">会员系统双11优化这个问题</a>对我来说，我是个外来者，完全不懂这里面的部署架构、业务逻辑。但是在问题的关键地方（会员认为自己没问题–压力测试正常的；淘宝API更是认为自己没问题，alimonitor监控显示正常），结果就是会员的同学说我们没有问题，淘宝API肯定有问题，然后就不去思考自己这边可能出问题的环节了。思想上已经甩包了，那么即使再去review流程、环节也就不会那么仔细，自然更是发现不了问题了。</p>
<p>但是我的经验告诉我要有证据地甩包，或者说拿着证据优雅地甩包，这迫使我去找更多的细节证据（证据要给力哦，不能让人家拍回来）。如果我是这么说的，这个问题在淘宝API这里，你看理由是…………，我做了这些实验，看到了这些东东。那么淘宝API那边想要证明我的理由错了就会更积极地去找一些数据。</p>
<p>事实上我就是做这些实验找证据过程中发现了会员的问题，这就是态度、执行力、知识、逻辑能力综合下来拿到的一个结果。我最不喜欢的一句话就是我的程序没问题，因为我的逻辑是这样的，不会错的。你当然不会写你知道的错误逻辑，程序之所以有错误都是在你的逻辑、意料之外的东西。有很多次一堆人电话会议中扯皮的时候，我一般把电话静音了，直接上去人肉一个个过对方的逻辑，一般来说电话会议还没有结束我就给出来对方逻辑之外的东西。</p>
<h3 id="场景式学习"><a href="#场景式学习" class="headerlink" title="场景式学习"></a>场景式学习</h3><p>我带2岁的小朋友看刷牙的画本的时候，小朋友理解不了喝口水含在嘴里咕噜咕噜不要咽下去，然后刷牙的时候就都喝下去了。我讲到这里的时候立马放下书把小朋友带到洗手间，先开始我自己刷牙了，示范一下什么是咕噜咕噜（放心，他还是理解不了的，但是至少有点感觉了，水在口里会响，然后水会吐出来）。示范完然后辅导他刷牙，喝水的时候我和他一起直接低着头，喝水然后立马水吐出来了，让他理解了到嘴里的东西不全是吞下去的。然后喝水晃脑袋，有点声音了（离咕噜咕噜不远了）。训练几次后小朋友就理解了咕噜咕噜，也学会了咕噜咕噜。这就是场景式学习的魅力。</p>
<p>很多年前我有一次等电梯，边上还有一个老太太，一个年轻的妈妈带着一个4、5岁的娃。应该是刚从外面玩了回来，妈妈在教育娃娃刚刚在外面哪里做错了，那个小朋友也是气嘟嘟地。进了电梯后都不说话，小朋友就开始踢电梯。这个时候那个年轻的妈妈又想开始教育小朋友了。这时老太太教育这个妈妈说，这是小朋友不高兴，做出的反抗，就是想要用这个方式抗议刚刚的教育或者挑逗起妈妈的注意。这个时候要忽视他，不要去在意，他踢几下后（虽然没有公德这么小懂不了这么多）脚也疼还没人搭理他这个动作，就觉得真没劲，可能后面他都不踢电梯了，觉得这是一个非常无聊还挨疼的事情。那么我在这个场景下立马反应过来，这就是很多以前我对一些小朋友的行为不理解的原因啊，这比书上看到的深刻多了。就是他们生气了在那里做妖挑逗你骂他、打他或者激怒你来吸引大人的注意力。</p>
<h2 id="钉子式学习方法和系统性学习方法"><a href="#钉子式学习方法和系统性学习方法" class="headerlink" title="钉子式学习方法和系统性学习方法"></a>钉子式学习方法和系统性学习方法</h2><p>系统性就是想掌握MySQL，那么搞几本MySQL专著和MySQL 官方DOC看下来，一般课程设计的好的话还是比较容易普遍性地掌握下来，绝大部分时候都是这种学习方法，可是问题在于在种方式下学完后当时看着似乎理解了，但是很容易忘记，一片一片地系统性的忘记。还是一般人对知识的理解没那么容易真正理解。</p>
<p>钉子式的学习方式，就是在一大片知识中打入几个桩，反复演练将这个桩不停地夯实，夯温，做到在这个知识点上用通俗的语言跟小白都能讲明白，然后在这几个桩中间发散像星星之火燎原一样把整个一片知识都掌握下来。这种学习方法的缺点就是很难找到一片知识点的这个点，然后没有很好整合的话知识过于零散。</p>
<p>我们常说的一个人很聪明，就是指系统性的看看书就都理解了，是真的理解那种，还能灵活运用，但是大多数普通人就不是这样的，看完书似乎理解了，实际几周后基本都忘记了，真正实践需要用的时候还是用不好。</p>
<p>实际这两种学习方法要互相结合，对普通人来讲钉子式学习方法更好一些，掌握几个钉子后再系统性地学习也容易多了；对非常聪明的人来说系统性地学习效率更高一些。</p>
<h3 id="举个Open-SSH的例子"><a href="#举个Open-SSH的例子" class="headerlink" title="举个Open-SSH的例子"></a>举个Open-SSH的例子</h3><p>为了做通 SSH 的免密登陆，大家都需要用到 ssh-keygen/ssh-copy-id， 如果我们把这两个命令当一个小的钉子的话，会去了解ssh-keygen做了啥（生成了密钥对），或者ssh-copy-id 的时候报错了（原来是需要秘钥对），然后将 ssh-keygen 生成的pub key复制到server的~/.ssh/authorized_keys 中。</p>
<p>然后你应该会对这个原理要有一些理解（更大的钉子），于是理解了密钥对，和ssh验证的流程，顺便学会怎么看ssh debug信息，那么接下来网络上各种ssh攻略、各种ssh卡顿的解决都是很简单的事情了。</p>
<p>比如你通过SSH可以解决这些问题：</p>
<ul>
<li>免密登陆</li>
<li>ssh卡顿</li>
<li>怎么去掉ssh的时候需要手工多输入yes</li>
<li>怎么样一次登录，多次复用</li>
<li>我的ssh怎么很快就断掉了</li>
<li>我怎么样才能一次通过跳板机ssh到目标机器</li>
<li>我怎么样通过ssh科学上网</li>
<li>我的ansible（底层批量命令都是基于ssh）怎么这么多问题，到底是为什么</li>
<li>我的git怎么报网络错误了</li>
<li>xshell我怎么配置不好</li>
<li>https为什么需要随机数加密，还需要签名</li>
<li>…………</li>
</ul>
<p>这些问题都是一步步在扩大ssh的外延，让这个钉子变成一个巨大的桩。</p>
<p>然后就会学习到一些<a href="/2018/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82SSH--SSH%E8%8A%B1%E5%BC%8F%E7%8E%A9%E6%B3%95/">高级一些的ssh配置</a>，比如干掉ssh的时候经常要yes一下(StrictHostKeyChecking=no), 或者怎么配置一下ssh就不会断线了（ServerAliveInterval=15），或者将 ssh跳板机-&gt;ssh server的过程做成 ssh server一步就可以了(ProxyCommand)，进而发现用 ssh的ProxyCommand很容易科学上网了，或者git有问题的时候轻而易举地把ssh debug打开，对git进行debug了……</p>
<p>这基本都还是ssh的本质范围，像ansible、git在底层都是依赖ssh来通讯的，你会发现学、调试xshell、ansible和git简直太容易了。</p>
<p>另外理解了ssh的秘钥对，也就理解了非对称加密，同时也很容易理解https流程（SSL），同时知道对称和非对称加密各自的优缺点，SSL为什么需要用到这两种加密算法了。</p>
<p>你看一个简单日常的知识我们只要沿着它用钉子精神，深挖细挖你就会发现知识之间的连接，这个小小的知识点成为你知识体系的一根结实的柱子。</p>
<p>我见过太多年老的工程师、年轻的工程师，天天在那里ssh 密码，ssh 跳板机，ssh 目标机，一小会ssh断了，重来一遍；或者ssh后卡住了，等吧……</p>
<p>在这个问题上表现得没有求知欲、没有探索精神、没有一次把问题搞定的魄力，所以就习惯了，然后年轻的工程师也变成了年老的工程师 :)</p>
<h2 id="空洞的口号"><a href="#空洞的口号" class="headerlink" title="空洞的口号"></a>空洞的口号</h2><p>很多文章都会教大家：举一反三、灵活运用、活学活用、多做多练。但是只有这些口号是没法落地的，落地的基本原则就是前面提到的，却总是被忽视了。</p>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举一反三，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>肯定知识效率最牛逼，但是拥有这种技能的人毕竟非常少（天生的高智商吧）。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快的掌握一个新知识，非常气人。剩下的绝大部分只能拼时间+方法+总结等也能掌握一些知识</p>
<p>非常遗憾我就是工程效率型，只能羡慕那些知识效率型的学霸。但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，两者之间当然没有明显的界限，知识积累多了逻辑训练好了在别人看来你的智商就高了</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>高质量学习+反思和输出，才能建立自己的知识脉络和体系。</p>
<p>①学习力-学习总结能力；</p>
<p>②输出力-逻辑思维和沟通表达能力；</p>
<p>③反思力-自省和修正能力；</p>
<p>某个领域的知识体系犹如一棵大树的根、干、枝、叶。</p>
<p>①损之又损得其道，为其根</p>
<p>②基本的方法论，为其干</p>
<p>③领域的通用知识，为其枝</p>
<p>④与业务绑定的特定知识，为其叶</p>
<p>在这个之上，你所取得的成就，为其花和果</p>
<p>学习能力+思考能力+行动能力=结果</p>
<p>最好的学习方法：带着问题、场景学习（目标明确）</p>
<h2 id="为什么看电影注意力特别好，做正事注意力集中不了"><a href="#为什么看电影注意力特别好，做正事注意力集中不了" class="headerlink" title="为什么看电影注意力特别好，做正事注意力集中不了"></a>为什么看电影注意力特别好，做正事注意力集中不了</h2><p>首先接受这个现实，医学上把这叫作注意力缺失症，基本所有人都有这种毛病，因为做正事比较枯燥、困难，让人不舒服，集中不了注意力，逃避很正常！</p>
<p>改善方法：做笔记、收集素材、写作</p>
<h2 id="极易被手机、微博、朋友圈干扰"><a href="#极易被手机、微博、朋友圈干扰" class="headerlink" title="极易被手机、微博、朋友圈干扰"></a>极易被手机、微博、朋友圈干扰</h2><p>意志力—还没有好办法</p>
<h2 id="改变条件反射，多逻辑思考"><a href="#改变条件反射，多逻辑思考" class="headerlink" title="改变条件反射，多逻辑思考"></a>改变条件反射，多逻辑思考</h2><p>有科学家通过研究，发现一个人一天的行为中，5%是非习惯性的，用思考脑的逻辑驱动，95%是习惯性的，用反射脑的直觉驱动，决定我们一生的，永远是95%的反射脑（习惯），而不是5%的思考脑（逻辑）</p>
<p>互联网+手机时代：浏览信息的时间多了，自己思考和琢磨的时间少了，专注在无效事情上的时间多了，专注在自我成长上的时间少了。</p>
<h2 id="容易忘记"><a href="#容易忘记" class="headerlink" title="容易忘记"></a>容易忘记</h2><p>学东西当时感觉很好，但是过几周基本都忘记了</p>
<p>这很正常，主要还是理解不够，理解不够也正常，这就是普通人的智商和理解能力。</p>
<p>改善：做笔记，利用碎片时间回顾</p>
<p>总结成系统性的文章，知识体系化，不会再忘记了。</p>
<h2 id="执行力和自律"><a href="#执行力和自律" class="headerlink" title="执行力和自律"></a>执行力和自律</h2><p>执行力和自律在我们的工作和生活中出现的频率非常高，因为这是我们成长或做成事时必须要有的2个关键词，但是很长一段时间里，对于提升执行力，疑惑很大。同时在工作场景中可能会被老板多次要求提升执行力，抽象又具体，但往往只有提升执行力的要求没有如何提升的方法和认知，这是普遍点到即止的现象，普遍提升不了执行力的现象。</p>
<p>“要有执行力”就是一句空话，谁都想，但是臣妾做不到。</p>
<p>人生由成长（学习）和享受（比如看电影、刷朋友圈）构成，成长太艰难，享受就很自然符合人性</p>
<p>怎么办？</p>
<pre><code>划重点：执行力就是想明白，然后一步一步做下去。
</code></pre><h2 id="跳出舒适区"><a href="#跳出舒适区" class="headerlink" title="跳出舒适区"></a>跳出舒适区</h2><p>重复、没有进步的时候就是舒适区，人性就是喜欢适合自己、符合自己技能的环境，解决问题容易；对陌生区域有恐惧感。</p>
<p>有时候是缺机会和场景驱动自我去学习，要找到从舒适到陌生区域的交融点，慢慢跨出去。<br>比如从自己熟悉的知识体系中入手，从已有的抓手和桩开始突击不清楚的问题，也就是横向、纵向多深挖，自然恐惧区就越来越小了，舒适区慢慢在扩张</p>
<h2 id="养成写文章的习惯非常重要"><a href="#养成写文章的习惯非常重要" class="headerlink" title="养成写文章的习惯非常重要"></a>养成写文章的习惯非常重要</h2><p>对自我碎片知识的总结、加深理解的良机，将知识体系化、结构化，形成知识体系中的抓手、桩。</p>
<p>缺的不是鸡汤，而是勺子，勺子就是具体的步骤，可以复制，对人、人性要求很低的动作。</p>
<p>生活本质是生产（工作学习成长）和消费(娱乐、刷朋友圈等)，消费总是符合人性的，当是对自己的适当奖励，不要把自己搞成机器人</p>
<p>可以做的是生产的时候效率更高</p>
<h2 id="知识分两种"><a href="#知识分两种" class="headerlink" title="知识分两种"></a>知识分两种</h2><p>一种是通用知识（不是说对所有人通用，而是说在一个专业领域去到哪个公司都能通用）；另外一种是跟业务公司绑定的特定知识</p>
<p>通用知识没有任何疑问碰到后要非常饥渴地扑上去掌握他们（受益终生，这还有什么疑问吗？）。对于特定知识就要看你对业务需要掌握的深度了，肯定也是需要掌握一些的，特定知识掌握好的一般在公司里混的也会比较好</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/11/24/到底是谁reset了你的连接/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/24/到底是谁reset了你的连接/" itemprop="url">到底是谁reset了你的连接</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-24T17:30:03+08:00">
                2019-11-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="到底是谁reset了你的连接"><a href="#到底是谁reset了你的连接" class="headerlink" title="到底是谁reset了你的连接"></a>到底是谁reset了你的连接</h1><p>通过一个案例展示TCP连接是如何被reset的，以及identification、ttl都可以帮我们干点啥。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>用户经常连不上服务，经过抓包发现是链路上连接被reset了，需要定位到是哪个设备发起的reset</p>
<p>比如：</p>
<ol>
<li>用户用navicat从自己访问云上的MySQL的时候，点开数据库总是报错（不是稳定报错，有一定的概率报错）</li>
<li>某家居客户通过专线访问云上MySQL，总是被reset( 内网ip地址重复–都是192.168.*， 导致连接被reset)</li>
</ol>
<blockquote>
<p><strong>进程被kill、异常退出时，针对它打开的连接，内核就会发送 RST 报文来关闭</strong>。RST 的全称是 Reset 复位的意思，它可以不走四次挥手强制关闭连接，但当报文延迟或者重复传输时，这种方式会导致数据错乱，所以这是不得已而为之的关闭连接方案。当然还有其它场景也会触发reset</p>
</blockquote>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>在 Navicat 机器上抓包如下：</p>
<p><img src="/images/oss/83b07725d92b9e4d3eb4a504cf83cc09.png" alt="image.png"></p>
<p>从抓包可以清楚看到 Navicat 发送 Use Database后收到了 MySQL（来自3306端口）的Reset重接连接命令，所以连接强行中断，然后 Navicat报错了。注意图中红框中的 Identification 两次都是13052，先留下不表，这是个线索。</p>
<p><img src="/images/oss/53b5dc8e0a90ed9ad641caf38399141b.png" alt="image.png"></p>
<h2 id="MySQL-Server上抓包"><a href="#MySQL-Server上抓包" class="headerlink" title="MySQL Server上抓包"></a>MySQL Server上抓包</h2><p>特别说明下，MySQL上抓到的不是跟Navicat上抓到的同一次报错，所以报错的端口等会不一样</p>
<p><img src="/images/oss/70287488290b38cd4753d9fce0bee945.png" alt="image.png"></p>
<p>从这个图中可以清楚看到reset是从 Navicat 客户端发过来的，并且 Use Database被拦截了，没有发到MySQL上。</p>
<p>从这里基本可以判断是客户的防火墙之类的中间设备监控到了关键字之类的触发了防火墙向两边发送了reset，导致了 Navicat 报错。</p>
<h3 id="如果连接已经断开"><a href="#如果连接已经断开" class="headerlink" title="如果连接已经断开"></a>如果连接已经断开</h3><p>如果连接已经断开后还收到Client的请求包，因为连接在Server上是不存在的，这个时候Server收到这个包后也会发一个reset回去，这个reset的特点是identification是0.</p>
<h2 id="到底是谁动了这个连接呢？"><a href="#到底是谁动了这个连接呢？" class="headerlink" title="到底是谁动了这个连接呢？"></a>到底是谁动了这个连接呢？</h2><h3 id="得帮客户解决问题"><a href="#得帮客户解决问题" class="headerlink" title="得帮客户解决问题"></a>得帮客户解决问题</h3><p>虽然原因很清楚，但是客户说连本地 MySQL就没这个问题，连你的云上MySQL就这样，你让我们怎么用？你们得帮我们找到是哪个设备。</p>
<p>这不废话么，连本地没经过这么多防火墙、网关当然没事了。但是客户第一，不能这么说，得找到问题所在。</p>
<h2 id="Identification-和-TTL"><a href="#Identification-和-TTL" class="headerlink" title="Identification 和 TTL"></a>Identification 和 TTL</h2><h3 id="线索一-Identification"><a href="#线索一-Identification" class="headerlink" title="线索一 Identification"></a>线索一 Identification</h3><p>还记得第一个截图中的两个相同的identification 13052吧，让我们来看看基础知识：</p>
<p><img src="/images/oss/eed9ba1f9ba492ed8954ae7f39e72803.png" alt="image.png"></p>
<p>（摘自 TCP卷一），简单来说这个 identification 用来标识一个连接中的每个包，这个序号按包的个数依次递增，通信双方是两个不同的序列。<strong>主要是用于ip packet的reassemble</strong>。</p>
<p>所以如果这个reset是MySQL发出来的话，因为MySQL发出的前一个包的 identification 是23403，所以这个必须是23404，实际上居然是13502（而且还和Navicat发出的 Use Database包是同一个 identification），这是非常不对的。</p>
<p>所以可以大胆猜测，这里有个中间设备收到 Use Database后触发了不放行的逻辑，于是冒充 Navicat给 MySQL Server发了reset包，src ip/src port/seq等都直接用Navicat的，identification也用Navicat的，所以 MySQL Server收到的 Reset看起来很正常（啥都是对的，没留下一点冒充的痕迹）。</p>
<p>但是这个中间设备还要冒充MySQL Server给 Navicat 也发个reset，有点难为中间设备了，这个时候中间设备手里只有 Navicat 发出来的包， src ip/src port/seq 都比较好反过来，但是 identification 就不好糊弄了，手里只有 Navicat的，因为 Navicat和MySQL Server是两个序列的 identification，这下中间设备搞不出来MySQL Server的identification，怎么办？ 只能糊弄了，就随手用 Navicat 自己的 identification填回去了（所以看到这么个奇怪的 identification）</p>
<p><strong>identification不对不影响实际连接被reset，也就是验证包的时候不会判断identification的正确性。</strong></p>
<h3 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h3><p>identification基本撇清了MySQL的嫌疑，还得进一步找到是哪个机器，我们先来看一个基础知识 TTL(Time-to-Live):</p>
<p><img src="/images/oss/ed8c624b704b0c94da2ca76a37b39916.png" alt="image.png"></p>
<p>然后我们再看看 Navicat收到的这个reset包的ttl是63，而正常的MySQL Server回过来的包是47，而发出的第一个包初始ttl是64，所以这里可以很清楚地看到在Navicat 下一跳发出的这个reset</p>
<p><img src="/images/oss/b288a740f9f10007485e37fd339051f8.png" alt="image.png"></p>
<p>既然是下一跳干的直接拿这个包的src mac地址，然后到内网中找这个内网设备就可以了，最终找到是一个锐捷的防火墙。</p>
<p>如果不是下一跳可以通过 traceroute/mtr 来找到这个设备的ip</p>
<h2 id="某家居的reset"><a href="#某家居的reset" class="headerlink" title="某家居的reset"></a>某家居的reset</h2><p><img src="/images/oss/1573793438383-3a05c4da-1443-4fcf-8b59-b93bc2a246de.png" alt="undefined"> </p>
<p>从图中可以清楚看到都是3306收到ttl为62的reset，正常ttl是61，所以推定reset来自client的下一跳上。</p>
<h2 id="某ISV-vpn环境reset"><a href="#某ISV-vpn环境reset" class="headerlink" title="某ISV vpn环境reset"></a>某ISV vpn环境reset</h2><p>client通过公网到server有几十跳，偶尔会出现连接被reset。反复重现发现只要是： select <em> from table1 ; 就一定reset，但是select </em> from table1 limit 1 之有极低的概率会被reset，reset的概率跟查询结果的大小比较相关。</p>
<p>于是在server和client上同时抓到了一次完整的reset</p>
<p>如下图红框 Server正常发出了一个大小为761的response包，id 51101，注意seq号，另外通过上下文知道server client之间的rt是15ms左右（15ms后 server收到了一个reset id为0）</p>
<p><img src="/images/oss/89f584899a5e5e00ba5c2b16707ed24a.png" alt="image.png"></p>
<p>下图是client收到的 id 51101号包，seq也正常，只是原来的response内容被替换成了reset，可以推断是中间环节检测到id 51101号包触发了某个条件，然后向server、client同时发出了reset，server收到的reset包是id 是0（伪造出来的），client收到的reset包还是51101，可以判断出是51101号包触发的reset，中间环节披着51101号包的外衣将response替换成了reset，这种双向reset基本是同时发出，从server和client的接收时间来看，这个中间环节挨着client，同时server收到的reset 的id是0，结合ttl等综合判断client侧的防火墙发出了这个reset</p>
<p><img src="/images/oss/ec1f04befe56823668b4d1f831bd3ea4.png" alt=""></p>
<p>最终排查后client端</p>
<blockquote>
<p>公司部分网络设置了一些拦截措施，然后现在把这次项目中涉及到的服务器添加到了白名单中，现在运行正常了</p>
</blockquote>
<h3 id="扩展一下"><a href="#扩展一下" class="headerlink" title="扩展一下"></a>扩展一下</h3><p>假如这里不是下一跳，而是隔了几跳发过来的reset，那么这个src mac地址就不是发reset设备的mac了，那该怎么办呢？</p>
<p>可以根据中间的跳数(TTL)，再配合 traceroute 来找到这个设备的ip</p>
<h2 id="SLB-reset"><a href="#SLB-reset" class="headerlink" title="SLB reset"></a>SLB reset</h2><p>如果连接闲置15分钟(900秒)后，SLB会给两端发送reset，设置的ttl为102（102年，下图经过3跳后到达RS 节点所以看到的是99），identification 为31415（π）</p>
<p><img src="/images/951413iMgBlog/image-20220722161729776.png" alt="image-20220722161729776"></p>
<h2 id="被忽略的reset"><a href="#被忽略的reset" class="headerlink" title="被忽略的reset"></a><a href="https://mp.weixin.qq.com/s/YWzuKBK3TMclejeN2ziAvQ" target="_blank" rel="external">被忽略的reset</a></h2><p>不是收到reset就一定释放连接，OS还是会验证一下这个reset 包的有效性，主要是通过reset包的seq是否落在接收窗口内来验证，当然五元组一定要对。</p>
<p><img src="/images/951413iMgBlog/640-20220224102640374.png" alt="Image"></p>
<p>但是对于SLB来说，收到reset就会clean 连接的session（SLB没做合法性验证），一般等session失效后（10秒）</p>
<h2 id="SLB主动reset的话"><a href="#SLB主动reset的话" class="headerlink" title="SLB主动reset的话"></a>SLB主动reset的话</h2><p>ttl是102, identification是31415，探活reset不是这样的。</p>
<p>如下图就是SLB发出来的reset packet</p>
<p><img src="/images/oss/9de70216-188c-4ca4-898f-0fa88e853c18.png" alt="img"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>基础知识很重要，但是知道ttl、identification到会用ttl、identification是两个不同的层次。只是看书的话未必会有很深的印象，实际也不一定会灵活使用。</p>
<p>平时不要看那么多书，会用才是关键。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mp.weixin.qq.com/s/YWzuKBK3TMclejeN2ziAvQ" target="_blank" rel="external">TCP中并不是所有的RST都有效</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="twitter @plantegg" />
          <p class="site-author-name" itemprop="name">twitter @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">171</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">22</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">265</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
